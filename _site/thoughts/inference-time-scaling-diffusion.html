<!DOCTYPE html>
<!--
  Original Design: Spectral by HTML5 UP
    html5up.net | @n33co
    Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
  Jekyll build mod and further hacks by @arkadianriver, MIT license
-->
<html>
  <head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Beyond denoising - rethinking inference-time scaling in diffusion models</title>

  <meta name="google-site-verification" content="WrNs4kb-PL779UWOhOTLegwiql-42uVzYDfCoJxQRPs" />
  <meta name="description" content="" />

  <!-- Open Graph Meta Tags -->
  <meta property="og:title" content="Beyond denoising - rethinking inference-time scaling in diffusion models" />
  <meta property="og:description" content="" />
  <meta property="og:type" content="article" />
  <meta property="og:url" content="http://localhost:4000/thoughts/inference-time-scaling-diffusion" />
  <meta property="og:image" content="http://localhost:4000/images/ula3.jpg" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />

  <!-- Twitter Card Meta Tags -->
  <meta name="twitter:card" content="summary_large_image" />
  <meta name="twitter:title" content="Beyond denoising - rethinking inference-time scaling in diffusion models" />
  <meta name="twitter:description" content="" />
  <meta name="twitter:image" content="http://localhost:4000/images/ula3.jpg" />

  <!-- Your existing meta tags -->
  <link rel="canonical" href="http://localhost:4000/thoughts/inference-time-scaling-diffusion" />
  <link rel="shortcut icon" href="/favicon.ico">
  <link rel="stylesheet" href="/css/main.css" />

  
  <!-- Add JSON-LD for Article and Person Schema -->
  <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "Article",
      "headline": "Beyond denoising - rethinking inference-time scaling in diffusion models",
      "description": "",
      "datePublished": "2025-03-05T00:00:00+01:00",
      "dateModified": "2025-03-05T00:00:00+01:00",
      "keywords": "",
      "author": {
        "@type": "Person",
        "name": "Urszula Czerwinska",
        "description": "Data Scientist & Deep Learning Engineer",
        "url": "https://www.linkedin.com/in/urszula-czerwinska/"
      },
      "image": {
        "@type": "ImageObject",
        "url": "http://localhost:4000/images/writing.jpeg",
        "width": 1200,
        "height": 630
      },
      "publisher": {
        "@type": "Organization",
        "name": "Urszula Czerwinska",
        "logo": {
          "@type": "ImageObject",
          "url": "http://localhost:4000/images/logo.png",
          "width": 60,
          "height": 60
        }
      },
      "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "http://localhost:4000/thoughts/inference-time-scaling-diffusion"
      }
    }
  </script>

  <!-- Your existing conditional styles and scripts -->

  <link rel="alternate" type="application/rss+xml" title="Urszula Czerwinska | Python, TensorFlow Expert | Data Scientist & Deep Learning Engineer" href="http://localhost:4000/feed.xml">
</head>

  <!-- Semantic Schema generated by InLinks -->


  
  <body>


    <!-- Page Wrapper -->
    <div id="page-wrapper">

      <!-- Header -->
<header id="header">
  <h1><a href="/index.html"> <span><img src="/favicon.ico" alt="Logo" style="height: 40px; vertical-align: middle; margin-right: 5px;"> Urszula Czerwinska </span></a></h1>
  <nav id="nav">
    <ul>
      <li class="special">
        <a href="#menu" class="menuToggle">  <span>
          Menu
        </span></a>
        <div id="menu">
          <ul>
            <li><a href="/index.html">Home</a></li>
            <li><a href="/about/">About</a></li>
            <li><a href="/works/">Works</a></li>
            <li><a href="/thoughts/">Thoughts</a></li>
            <li><a href="/feed.xml"
                   class="icon fa-feed"> RSS Feed</a></li>
          </ul>
        </div>
      </li>
    </ul>
  </nav>
</header>


      <article id="standard">

          <header>
    <h2>Beyond denoising - rethinking inference-time scaling in diffusion models</h2>
    <p><p><span class="image fit"></span></p>
</p>
  </header>
  <ul class="breadcrumb">
  <li><a href="/index.html">Home</a></li>
  <li>Beyond denoising - rethinking inference-time scaling in diffusion models</li>
</ul>



          <section class="wrapper style5">
    <div class="inner">
      <span id="post-date">5 March 2025</span><hr
        style="margin-top:3px;" />

      <p><span class="image fit"></span></p>

<p><img src="https://cdn-images-1.medium.com/max/800/1*p3GtIq_UgFgPA7ObBMaITw.png" alt="" />
&lt;/span&gt;</p>

<p>Generative AI is experiencing a paradigm shift. While early breakthroughs in diffusion models emphasized <strong>training-time scaling</strong> — bigger models, more data, and longer training — recent research suggests that <strong>inference-time scaling</strong> might be an equally powerful lever for improving generation quality. Unlike static architectures, diffusion models offer the unique ability to allocate compute dynamically during sampling, yet standard techniques (like increasing denoising steps) hit diminishing returns.</p>

<p><a href="https://arxiv.org/abs/2501.09732">Maq et al. (2025)</a> challenge this bottleneck, proposing that inference-time compute should be viewed not as a simple iteration counter but as a <strong>search problem</strong> — one where strategically refining injected noise can unlock better outputs. This perspective fundamentally reframes how we think about sample generation, drawing intriguing parallels with <strong>search-driven optimizations in Large Language Models (LLMs)</strong>, such as tree-of-thoughts prompting or best-of-n decoding.</p>

<p>Are we witnessing the emergence of an <strong>active generation paradigm</strong>, where models don’t just passively map noise to images but dynamically explore pathways to the best output?</p>

<h3 id="why-scaling-matters-in-generative-ai">Why scaling matters in Generative AI</h3>

<p>Let me introduce some context first.</p>

<p>The success of <strong>large generative models</strong> — whether in text (LLMs like GPT-4), images (Stable Diffusion, DALL·E), or even video and audio — rests on a simple but powerful idea:</p>

<blockquote>
  <p><strong><em>More compute → Better results.</em></strong></p>
</blockquote>

<p>This principle, known as <strong>scaling laws</strong>, tells us that as we increase <strong>model size, training data, and compute power</strong>, generative models tend to improve in a predictable way.</p>

<p><span class="image fit"></span></p>

<p><img src="https://cdn-images-1.medium.com/max/800/1*dvU8hFuvO6NUGWwF89bEhg.png" alt="" />
&lt;/span&gt;</p>

<p>However, until recently, research on scaling has focused <strong>almost exclusively on training</strong>. The assumption has been:</p>

<ul>
  <li>Train a bigger model on more data → get better outputs.</li>
  <li>Once trained, the model is <strong>fixed</strong> — inference simply runs it as-is.</li>
</ul>

<p>But what if we could make models <strong>smarter at inference time</strong>, allowing them to refine their outputs dynamically?</p>

<h3 id="scaling-inllms">Scaling in LLMs</h3>

<p>In the world of <strong>Large Language Models (LLMs)</strong>, researchers have recently found that performance can improve <strong>even after training is complete</strong>, simply by <strong>allocating more compute at inference time</strong>.</p>

<p>This is done through:</p>

<ul>
  <li><strong>Search-based decoding</strong> (e.g., “best-of-n” sampling, tree-of-thoughts, reranking).</li>
  <li><strong>Iterative refinement</strong> (models generating multiple responses and selecting the best).</li>
  <li><strong>Verifier-guided outputs</strong> (using reward models like RLHF to optimize responses).</li>
</ul>

<p>These techniques allow LLMs to generate <strong>higher-quality, more accurate, and more contextually relevant</strong> outputs without retraining.</p>

<p><span class="image fit"></span></p>

<p><img src="https://cdn-images-1.medium.com/max/800/1*rVRrc9bx_YNxwm3p6QJdpQ.png" alt="" />
&lt;/span&gt;</p>

<p>There are numerous great ressources on scaling laws for LLM, I recommend you check these ones out:</p>

<p><a href="https://cameronrwolfe.substack.com/p/llm-scaling-laws" title="https://cameronrwolfe.substack.com/p/llm-scaling-laws"><strong>Scaling Laws for LLMs: From GPT-3 to o3</strong>
_Understanding the current state of LLM scaling and the future of AI research…_cameronrwolfe.substack.com</a><a href="https://cameronrwolfe.substack.com/p/llm-scaling-laws"></a></p>

<p><a href="https://lifearchitect.ai/chinchilla/" title="https://lifearchitect.ai/chinchilla/"><strong>Chinchilla data-optimal scaling laws: In plain English</strong>
_Important: This page summarizes data scaling only, using tokens to parameters as a ratio, and as derived from large…_lifearchitect.ai</a><a href="https://lifearchitect.ai/chinchilla/"></a></p>

<p><a href="https://www.interconnects.ai/p/openai-strawberry-and-inference-scaling-laws" title="https://www.interconnects.ai/p/openai-strawberry-and-inference-scaling-laws"><strong>OpenAI’s Strawberry and inference scaling laws</strong>
_OpenAI’s Strawberry, LM self-talk, inference scaling laws, and spending more on inference. Coming waves in LLMs._www.interconnects.ai</a><a href="https://www.interconnects.ai/p/openai-strawberry-and-inference-scaling-laws"></a></p>

<h3 id="what-about-diffusion-models">What about Diffusion Models?</h3>

<p>Diffusion models — used in <strong>AI-generated art, photorealistic synthesis, and video generation</strong> — have a built-in way to allocate compute at inference:</p>

<ul>
  <li>Instead of producing an image in one step (like a GAN), they <strong>start from random noise</strong> and progressively refine it over multiple steps.</li>
  <li>The number of <strong>denoising steps</strong> (or <strong>NFEs, Number of Function Evaluations</strong>) determines <strong>output quality vs. compute cost</strong>.</li>
</ul>

<p><span class="image fit"></span></p>

<p><img src="https://cdn-images-1.medium.com/max/800/1*cyZ4Ak47TauJ4pT5uC-ugw.png" alt="" />
&lt;/span&gt;</p>

<p>If you need to better understand diffusion models and denoising, here are some great ressources:</p>

<p><a href="https://learn.deeplearning.ai/courses/diffusion-models/" title="https://learn.deeplearning.ai/courses/diffusion-models/"><strong>How Diffusion Models Work - DeepLearning.AI</strong>
_Learn and build diffusion models from the ground up, understanding each step. Learn about diffusion models in use today…_learn.deeplearning.ai</a><a href="https://learn.deeplearning.ai/courses/diffusion-models/"></a></p>

<p><a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/" title="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/"><strong>What are Diffusion Models?</strong>
_Updated on 2021-09-19: Highly recommend this blog post on score-based generative modeling by Yang Song (author of…_lilianweng.github.io</a><a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/"></a></p>

<p><strong>==&gt; But here’s the problem</strong>: Simply increasing denoising steps <strong>only helps up to a point</strong>. Beyond a certain threshold, <strong>performance gains flatten out</strong> — more steps <strong>don’t</strong> improve quality much.</p>

<p>Maq et al. (2025) propose a new way forward:</p>

<blockquote>
  <p><strong><em>Instead of blindly adding denoising steps, we should actively search for better noise to start with.</em></strong></p>
</blockquote>

<p>This idea reframes inference-time scaling <strong>not as a matter of iteration count</strong>, but as a <strong>search problem</strong> — one that could make diffusion models <strong>far more powerful and efficient</strong>.</p>

<h3 id="the-limits-of-traditional-inference-scaling">The limits of traditional inference scaling</h3>

<p>Scaling inference-time compute in generative models isn’t new, but its limitations have been stark. Most diffusion models operate under a straightforward assumption: <strong>more denoising steps = better samples</strong>. While this holds in early iterations, studies have shown that performance gains flatten quickly beyond a certain number of denoising function evaluations (NFEs).</p>

<p>Maq et al. (2025) provide empirical evidence of this plateau, reinforcing what past work (e.g., Karras et al., 2022) has hinted at — more iterations introduce <strong>approximation errors</strong> and <strong>discretization artifacts</strong>, limiting the benefits of brute-force step scaling. This raises a critical question:</p>

<p><span class="image fit"></span></p>

<p><img src="https://cdn-images-1.medium.com/max/800/1*7Qb7lwpubNreFoceYT7ZMw.png" alt="" />
&lt;/span&gt;</p>

<blockquote>
  <p><em>If adding more denoising steps isn’t the answer, where should we invest our compute budget instead?</em></p>
</blockquote>

<p>Their proposed solution: <strong>optimize the starting noise itself</strong>. Not all noise samples are equal — some lead to better generations than others. By reframing sample generation as a <strong>search over noise candidates</strong>, the model can actively seek higher-quality results rather than blindly committing to a single path.</p>

<p>This shift from <strong>passive denoising to active search</strong> echoes trends in LLMs, where inference-time optimizations (e.g., re-ranking outputs, iterative reasoning) have led to significant performance boosts without retraining. Could similar techniques unlock new frontiers for diffusion models?</p>

<h3 id="the-searchparadigm">The Search Paradigm</h3>

<p>At the heart of Maq et al.’s framework is a <strong>two-axis search strategy</strong>:</p>

<p><strong>Verifier Functions</strong>: Instead of passively denoising noise into images, a <strong>verifier</strong> scores generated samples, helping guide search toward higher-quality outputs. The paper explores three types:</p>

<ul>
  <li><strong>Oracle Verifiers</strong> (ideal but impractical, using ground-truth FID or IS scores)</li>
  <li><strong>Supervised Verifiers</strong> (pretrained classifiers like CLIP)</li>
  <li><strong>Self-Supervised Verifiers</strong> (consistency-based scoring without external labels)</li>
</ul>

<p><span class="image fit"></span></p>

<p><img src="https://cdn-images-1.medium.com/max/800/1*Bx9m3OO-j0crc_hCK4TD-A.png" alt="" />
&lt;/span&gt;</p>

<p><strong>Search Algorithms</strong>: Instead of iterating endlessly on a single noise sample, the model actively <strong>evaluates multiple noise candidates</strong> and refines the search:</p>

<ul>
  <li><strong>Random Search:</strong> Generate multiple samples and pick the best (best-of-n selection).</li>
  <li><strong>Zero-Order Search:</strong> Iteratively refine noise candidates using verifier feedback.</li>
  <li><strong>Search Over Paths:</strong> Adjust noise at intermediate steps, treating generation as a <strong>trajectory exploration</strong> rather than a fixed path.</li>
</ul>

<p><span class="image fit"></span></p>

<p><img src="https://cdn-images-1.medium.com/max/800/1*iTmMX0SLA0u883oPQmGDjg.png" alt="" />
&lt;/span&gt;</p>

<p><strong>Inference-time scaling is no longer just about running a fixed pipeline more times — it’s about making smarter choices along the way.</strong></p>

<h3 id="my-thoughts">My thoughts</h3>

<p>Maq et al.’s approach is an exciting step forward, but it also raises <strong>critical challenges</strong>:</p>

<h4 id="bias-who-decides-whats-better">Bias: Who decides what’s “better”?</h4>

<p>While the paper demonstrates impressive gains, the choice of verifier is <strong>non-trivial</strong>. Verifier models (like CLIP or ImageReward) come with <strong>built-in biases</strong> — favoring certain aesthetics, object distributions, or stylistic preferences. This leads to potential <strong>mode collapse</strong>, where optimizing too aggressively for a specific verifier reduces overall diversity.</p>

<p>A striking analogy exists in LLMs: when models are fine-tuned on <strong>reward functions like RLHF</strong>, they can start <strong>overfitting to human preferences</strong> rather than maintaining broad generalization. This is a serious concern for diffusion models — will optimizing noise for a verifier ultimately <strong>reduce creative variation</strong>?</p>

<h4 id="compute-cost-is-this-scalable">Compute cost: Is this scalable?</h4>

<p>While this approach sidesteps retraining, it <strong>dramatically increases inference costs</strong>.</p>

<ul>
  <li><strong>Random search scales linearly</strong> with the number of candidate samples.</li>
  <li><strong>Zero-order search requires iterative evaluations</strong>, compounding compute needs.</li>
  <li><strong>Path-based search introduces additional denoising passes.</strong></li>
</ul>

<p>For <strong>real-time applications</strong> like interactive art generation, these search-based methods might be impractical. Future research should explore ways to <strong>compress the search process</strong> or <strong>pretrain lightweight noise-ranking networks</strong> to reduce computational overhead.</p>

<h4 id="does-search-capture-complexprompts">Does search capture complex prompts?</h4>

<p>A major limitation of diffusion models today is their struggle with <strong>compositional prompts</strong> (e.g., “a cat wearing sunglasses sitting on a skateboard in front of the Eiffel Tower”). The search-based approach optimizes local noise variations — but does it help with complex scene composition?</p>

<p>If not, we might need <strong>hierarchical search strategies</strong> — coarse-to-fine optimizations that first establish global structure before refining details.</p>

<h3 id="the-future-of-adaptive-generation">The future of adaptive generation</h3>

<p>Maq et al. (2025) mark an important shift in how we think about <strong>inference-time resource allocation</strong>. Their work suggests that diffusion models can dynamically improve themselves at generation time, similar to how LLMs refine responses through <strong>iterative reasoning and ranking</strong>.</p>

<p>This brings us to a bigger question:</p>

<blockquote>
  <p><em>Could generative models move beyond static sampling pipelines into</em> <strong><em>adaptive generation loops</em></strong><em>?</em></p>
</blockquote>

<p>Imagine diffusion models that:</p>

<ul>
  <li>Dynamically adjust noise injection based on <strong>scene complexity</strong></li>
  <li>Leverage multi-step verifier feedback to iteratively <strong>improve object coherence</strong></li>
  <li>Incorporate <strong>user-guided search</strong>, allowing interactive control over generation</li>
</ul>

<p>Rather than a fixed “noise-to-image” pipeline, we might be moving toward an era where <strong>generative models actively explore multiple solutions before committing to a final output</strong> — more akin to <strong>an artist refining a painting rather than a one-shot rendering process</strong>.</p>

<h3 id="conclusions">Conclusions</h3>

<p>The insights from Maq et al. (2025) suggest that inference-time compute can be used <strong>far more intelligently</strong> than previously assumed. By treating sample generation as <strong>an optimization problem over noise</strong>, rather than just a denoising pipeline, they demonstrate a scalable approach to boosting performance without retraining.</p>

<p>However, open questions remain — how do we mitigate verifier bias? Can we make this efficient enough for real-time applications? And most importantly, <strong>how do we ensure that search strategies don’t sacrifice creativity for optimization?</strong></p>

<p>Regardless, one thing is clear: <strong>inference-time search marks the beginning of a new, more dynamic era for diffusion models.</strong></p>

<footer>
  <p>Exported from <a href="https://medium.com">Medium</a> in March 2025.</p>
  <p><a href="https://medium.com/data-science-collective/beyond-denoising-rethinking-inference-time-scaling-in-diffusion-models-55603337e44a">View
      the original</a></p>
</footer>
<script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-584ec4ce89deed84"></script>

<div class="addthis_inline_share_toolbox"></div>


<hr style="margin-bottom:12px;" />
<div class="author">
<!-- style="margin:8px 28px 12px 0;position:relative;float:left;"> -->
  <!-- style="position: relative; float: left;margin:0;padding:0;" -->
  <div style="display:inline-block;border-radius:7px;overflow:hidden;height:100px;width:100px;background:url(/images/ula.jpg);background-size:100px;"></div>
  <div style="display:inline-block;padding-left:12px;vertical-align:top;"><b>by:<br />Urszula Czerwinska</b><br />(<a href="mailto:urszula.czerwinska@cri-paris.org">urszula.czerwinska@cri-paris.org</a>)<br
    /><i><a href="http://urszulaczerwinska.github.io" target="_blank">http://urszulaczerwinska.github.io</a></i>
  </div>
  <div class="auth-desc"><p> Senior Data Scientist / Deep Learning Engineer </p> PhD in Bio-Mathematics, Data Science & Machine Learning
</div>
</div>
<hr style="margin-top:9px;" />

  
    </div>
  </section>


      </article>

      <!-- Footer -->
<footer id="footer">
  <ul class="icons">
    <li><a target="_blank" href="https://twitter.com/ulalaparis" class="icon fa-twitter"
           ><span class="label">twitter</span></a></li>
    <li><a target="_blank" href="https://github.com/urszulaczerwinska" class="icon fa-github"
           ><span class="label">github</span></a></li>
    <li><a target="_blank" href="https://linkedin.com/in/urszulaczerwinska" class="icon fa-linkedin-square"
           ><span class="label">linkedin-square</span></a></li>
    <li><a target="_blank" href="mailto:ulcia.liberte@gmail.com" class="icon fa-envelope"
           ><span class="label">E-mail</span></a></li>
  </ul>
  <ul class="copyright">
    <li>&copy; 2016,
    2025
      Urszula Czerwinska</li>
    <li><a href="/credits/">Credits</a></li>
  </ul>
</footer>


      <!-- Scripts -->
<script src="/js/jquery.min.js"></script>
<script src="/js/jquery.scrollex.min.js"></script>
<script src="/js/jquery.scrolly.min.js"></script>
<script src="/js/skel.min.js"></script>
<script src="/js/util.js"></script>
<!--[if lte IE 8]><script src="/js/ie/respond.min.js"></script><![endif]-->
<script src="/js/main.js"></script>

    </div>

  </body>



</html>