<!DOCTYPE html>
<!--
  Original Design: Spectral by HTML5 UP
    html5up.net | @n33co
    Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
  Jekyll build mod and further hacks by @arkadianriver, MIT license
-->
<html>
  <head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>How to choose the best image embeddings for your e-commerce business ?</title>

  <meta name="google-site-verification" content="WrNs4kb-PL779UWOhOTLegwiql-42uVzYDfCoJxQRPs" />
  <meta name="description" content="Discover how to select the right image embeddings strategy ‚Äî pre-trained, fine-tuned, or top-tuned ‚Äî to boost visual search, product tagging, and recommendat..." />

  <!-- Open Graph Meta Tags -->
  <meta property="og:title" content="How to choose the best image embeddings for your e-commerce business ?" />
  <meta property="og:description" content="Discover how to select the right image embeddings strategy ‚Äî pre-trained, fine-tuned, or top-tuned ‚Äî to boost visual search, product tagging, and recommendat..." />
  <meta property="og:type" content="article" />
  <meta property="og:url" content="http://urszulaczerwinska.github.io/works/foundation-models-image-embeddings" />
  <meta property="og:image" content="http://urszulaczerwinska.github.io/images/ula3.jpg" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />

  <!-- Twitter Card Meta Tags -->
  <meta name="twitter:card" content="summary_large_image" />
  <meta name="twitter:title" content="How to choose the best image embeddings for your e-commerce business ?" />
  <meta name="twitter:description" content="Discover how to select the right image embeddings strategy ‚Äî pre-trained, fine-tuned, or top-tuned ‚Äî to boost visual search, product tagging, and recommendat..." />
  <meta name="twitter:image" content="http://urszulaczerwinska.github.io/images/ula3.jpg" />

  <!-- Your existing meta tags -->
  <link rel="canonical" href="http://urszulaczerwinska.github.io/works/foundation-models-image-embeddings" />
  <link rel="shortcut icon" href="/favicon.ico">
  <link rel="stylesheet" href="/css/main.css" />

  
  <!-- Add JSON-LD for Article and Person Schema -->
  <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "Article",
      "headline": "How to choose the best image embeddings for your e-commerce business ?",
      "description": "Discover how image embeddings power AI-driven visual search, product recommendations, and auto-tagging in e-commerce. Learn how Leboncoin uses foundation mod...",
      "datePublished": "2025-04-13T00:00:00+02:00",
      "dateModified": "2025-04-13T00:00:00+02:00",
      "keywords": "FoundationModels, ComputerVision, DeepLearning, AI, E-commerce, ImageSearch",
      "author": {
        "@type": "Person",
        "name": "Urszula Czerwinska",
        "description": "Data Scientist & Deep Learning Engineer",
        "url": "https://www.linkedin.com/in/urszula-czerwinska/"
      },
      "image": {
        "@type": "ImageObject",
        "url": "http://urszulaczerwinska.github.io/images/writing.jpeg",
        "width": 1200,
        "height": 630
      },
      "publisher": {
        "@type": "Organization",
        "name": "Urszula Czerwinska",
        "logo": {
          "@type": "ImageObject",
          "url": "http://urszulaczerwinska.github.io/images/logo.png",
          "width": 60,
          "height": 60
        }
      },
      "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "http://urszulaczerwinska.github.io/works/foundation-models-image-embeddings"
      }
    }
  </script>

  <!-- Your existing conditional styles and scripts -->

  <link rel="alternate" type="application/rss+xml" title="Urszula Czerwinska | Python, TensorFlow Expert | Data Scientist & Deep Learning Engineer" href="http://urszulaczerwinska.github.io/feed.xml">
</head>

  <!-- Semantic Schema generated by InLinks -->


  
  <body>


    <!-- Page Wrapper -->
    <div id="page-wrapper">

      <!-- Header -->
<header id="header">
  <h1><a href="/index.html"> <span><img src="/favicon.ico" alt="Logo" style="height: 40px; vertical-align: middle; margin-right: 5px;"> Urszula Czerwinska </span></a></h1>
  <nav id="nav">
    <ul>
      <li class="special">
        <a href="#menu" class="menuToggle">  <span>
          Menu
        </span></a>
        <div id="menu">
          <ul>
            <li><a href="/index.html">Home</a></li>
            <li><a href="/about/">About</a></li>
            <li><a href="/works/">Works</a></li>
            <li><a href="/thoughts/">Thoughts</a></li>
            <li><a href="/feed.xml"
                   class="icon fa-feed"> RSS Feed</a></li>
          </ul>
        </div>
      </li>
    </ul>
  </nav>
</header>


      <article id="main">

          <header>
    <h2>How to choose the best image embeddings for your e-commerce business ?</h2>
    <p>Discover how to select the right image embeddings strategy ‚Äî pre-trained, fine-tuned, or top-tuned ‚Äî to boost visual search, product tagging, and recommendations in your e-commerce platform.</p>
  </header>
  <ul class="breadcrumb">
  <li><a href="/index.html">Home</a></li>
  <li>How to choose the best image embeddings for your e-commerce business ?</li>
</ul>



          <section class="wrapper style5">
    <div class="inner">
      <span id="post-date">13 April 2025</span><hr
        style="margin-top:3px;" />
      <h4>Skills</h4>
  <ul class="techlist">
<li><span class="tech">FoundationModels</span></li>
<li><span class="tech">ComputerVision</span></li>
<li><span class="tech">DeepLearning</span></li>
<li><span class="tech">AI</span></li>
<li><span class="tech">E-commerce</span></li>
<li><span class="tech">ImageSearch</span></li>
</ul>

  

      <p><span class="image fit"></span></p>

<p><img src="https://cdn-images-1.medium.com/max/800/0*eSwizQN9nC4jaMvw" alt="Discover how to select the right image embeddings strategy ‚Äî pre-trained, fine-tuned, or top-tuned ‚Äî to boost visual search, product tagging, and recommendations in your e-commerce platform." /></p>

<p>Imagine uploading a picture of a designer lamp and instantly finding similar items on <em>Leboncoin</em>. You don‚Äôt need imagine any longer, you can go to <em>Leboncoin</em> app and try it out by youself¬†!¬†</p>

<p><span class="image fit"></span></p>

<p><img src="https://cdn-images-1.medium.com/max/800/1*O_hLOiIUpaZTmRxGTUluuQ.gif" alt="Designer lamp used in visual search example on Leboncoin" /></p>

<p>This magic happens because AI converts images into <strong>embeddings</strong>‚Ää‚Äî‚Ääcompact numerical representations that capture the essential features of an image. These embeddings can be used for:</p>

<ul>
  <li><strong>Image Search:</strong> Finding visually similar products</li>
  <li><strong>Product Categorization:</strong> Automatically tagging and sorting listings</li>
  <li><strong>Image classification:</strong> Detect catgory, weight range, color‚Ä¶</li>
  <li><strong>Recommendations:</strong> Suggesting related products to users</li>
</ul>

<p>But not all embeddings are created equal. F<strong>oundation models aren‚Äôt optimized for e-commerce out of the box</strong>. You need to decide whether to fine-tune them, use them as-is, or apply a lightweight tuning approach. <strong>So, which strategy is best?</strong> That‚Äôs exactly what we set out to benchmark.</p>

<p>You can find the full article accepted for ‚Äú<a href="https://saiconference.com/FTC">Future Technologies Conference (FTC) 2025</a>‚Äù that will happen in Munich, Germany 6‚Äì7 November 2025 on arxiv at <a href="https://arxiv.org/abs/2504.07567">this link</a>.¬†</p>

<p>Come to say hi if you are attending, <a href="https://www.linkedin.com/in/jeremychamoux/">Jeremy Chamoux</a> will present our work there.</p>

<p>FTC is the world‚Äôs pre-eminent forum for reporting research breakthroughs in Artificial Intelligence, Robotics, Data Science, Computing, Ambient Intelligence and related fields. It has emerged as the foremost world-wide gathering of academic researchers, Ph.D. &amp; graduate students, top research think tanks &amp; industry technology developers.</p>

<h3 id="the-challenge-finding-the-best-ai-model-for-thejob">The challenge: Finding the best AI model for the¬†job</h3>

<p>There‚Äôs no shortage of AI models that can generate image embeddings. Some are designed for <strong>general-purpose vision tasks</strong>, while others specialize in <strong>e-commerce or multimodal learning (combining images and text).</strong></p>

<p><span class="image fit"></span></p>

<p><img src="https://cdn-images-1.medium.com/max/800/1*YnrSr_AmE29Eg7jWLYD5-A.png" alt="Comparison of supervised, self-supervised, and contrastive learning models" /></p>

<p>&lt;/span&gt;</p>

<p>To determine the best approach for <em>Leboncoin‚Äôs</em> marketplace, we benchmarked three main types of AI models:</p>

<p><strong>Supervised Learning Models:</strong> Trained on labeled data (e.g., images explicitly tagged with categories).</p>

<ul>
  <li>ConvNext</li>
  <li>ResNet</li>
  <li>ViT</li>
</ul>

<p><strong>Self-Supervised Learning (SSL) Models:</strong> Learn patterns without labeled data, making them more flexible.</p>

<ul>
  <li>DINO</li>
  <li>DINOv2</li>
  <li>Maws</li>
  <li>MAE</li>
</ul>

<p><strong>Contrastive Learning Models (Text-Image):</strong> Models like CLIP, which link images to text descriptions for richer representations.</p>

<ul>
  <li>CLIP (different versions with pretraining)</li>
  <li>SigLIP</li>
</ul>

<p>We also tested different <strong>supervised</strong> <strong>fine-tuning strategies</strong> to see whether adapting these models to e-commerce data would improve results.</p>

<p>Selecting these models families we also had in mind the possible adaption work we planned to conduct as a next step.¬†</p>

<p>Adapting different models family requires different input data, for supervised learning we would need images and labels on a task making model learn information generic enough to reuse for different applications, for contrastive learning we would need image caption(s) pairs, while for self-supervised learning just images would be enough (which would allow us to leverage Tb of images).</p>

<p><span class="image fit"></span></p>

<p><img src="https://cdn-images-1.medium.com/max/1200/1*0lI1KyCIzWdHSfbrehN9AA.png" alt="Benchmark pipeline architecture for evaluating AI models on e-commerce tasks" /></p>

<p>&lt;/span&gt;</p>

<h3 id="how-we-tested-ai-models-on-e-commerce-data">How we tested AI models on e-commerce data</h3>

<p>To ensure a fair comparison, we evaluated AI models on <strong>six real-world e-commerce datasets</strong> covering different product categories:</p>

<p><span class="image fit"></span></p>

<p><img src="https://cdn-images-1.medium.com/max/1200/1*lJg5nMhonkm4tdkWVz47QQ.png" alt="Product category distribution across the six real-world e-commerce datasets used" /></p>

<p>&lt;/span&gt;</p>

<h3 id="key-experiments-weran">Key experiments we¬†ran</h3>

<p>Each AI model was tested on two core tasks:</p>

<ol>
  <li><strong>Product Classification:</strong> Can the AI correctly categorize images into predefined product types?</li>
  <li><strong>Image Retrieval:</strong> Given a query image, can the AI find the most visually similar products (where similarity is defined as from the same category)?</li>
</ol>

<p>For retrieval, we used <strong>vector search</strong> (storing embeddings in a database and finding nearest neighbors) with Millvus and evaluated the models using industry-standard ranking metrics like <strong>Mean Average Precision (MAP)</strong> and <strong>Recall@K</strong> (how often the correct product appears in the top results).</p>

<h3 id="understanding-image-embeddings-the-three-main-approaches">Understanding image embeddings: The three main approaches</h3>

<p>When working with image-based AI in e-commerce, you have <strong>three primary choices</strong> for generating embeddings:</p>

<h3 id="1-pre-trained-embeddings-off-the-shelf-models">1. Pre-trained embeddings (Off-the-shelf models)</h3>

<p>These embeddings come from large-scale models trained on general-purpose datasets like <strong>ImageNet</strong>. Examples include:</p>

<p><strong>Pros:</strong>
- No training required‚Ää‚Äî‚Ääjust use them as-is
- Works well for general image understanding
- Ideal for companies with limited ML expertise</p>

<p><strong>Cons:</strong>
- May not capture e-commerce-specific nuances (e.g., distinguishing a luxury bag from a knockoff)
- Suboptimal performance in fine-grained product categorization</p>

<h3 id="2-fully-fine-tuned-embeddings-training-on-your-owndata">2. Fully fine-tuned embeddings (Training on your own¬†data)</h3>

<p>Fine-tuning means <strong>adapting a pre-trained model to your specific e-commerce dataset</strong> by updating all of its parameters. This makes the embeddings <strong>highly specialized</strong> for tasks like product classification or search.</p>

<p><strong>Pros:</strong>
- Best accuracy for domain-specific tasks
- Captures subtle differences in product categories</p>

<p><strong>Cons:</strong>
- Computationally expensive (requires GPUs, storage, and time)
- Risk of overfitting, especially with small datasets
- Requires ML expertise to tune hyperparameters effectively</p>

<h3 id="3-top-tuned-embeddings-the-sweet-spot-for-e-commerce">3. Top-tuned embeddings (The sweet spot for e-commerce)</h3>

<p>Top-tuning is a middle ground: instead of fine-tuning the entire model, we <strong>freeze the base layers and only train a lightweight classifier on top</strong>. This is much cheaper than full fine-tuning but can still provide a performance boost.</p>

<p><strong>Pros:</strong>
- Significant improvement over pre-trained models
- Requires far fewer resources than full fine-tuning
- Faster to deploy in production</p>

<p><strong>Cons:</strong>
- Not always as strong as fully fine-tuned models for classification tasks
- Still requires some labeled data for training</p>

<h3 id="key-results">Key results</h3>

<p><span class="image fit"></span></p>

<p><img src="https://cdn-images-1.medium.com/max/1200/1*qbKsOd8pSZsMzj8qmm2P6Q.png" alt="Bar graph showing performance results across model types and tasks" />
&lt;/span&gt;</p>

<p><span class="image fit"></span></p>

<p><img src="https://cdn-images-1.medium.com/max/800/0*erow3Ys7hQrfzkGZ" alt="Visual comparison of retrieval results using different AI models" />
&lt;/span&gt;</p>

<h4 id="1-different-model-types-excel-in-different-scenarios">1. Different Model Types Excel in Different Scenarios</h4>

<ul>
  <li><strong>Supervised fine-tuned models</strong> (e.g., ConvNeXt-Base, ViT-B) achieve <strong>the highest accuracy</strong> but require <strong>significant computational resources</strong>.</li>
  <li><strong>Self-supervised models (SSL)</strong> (e.g., DINO-ViT, MAWS-ViT) show <strong>high variance</strong>, making them <strong>less stable</strong> but useful with <strong>top-tuning</strong>.</li>
  <li><strong>Contrastive text-image models</strong> (e.g., SigLIP, Marqo, Apple CLIP) <strong>excel in retrieval tasks</strong> with minimal adaptation.</li>
  <li><strong>Top-tuned models</strong> add lightweight layers to pre-trained models, often <strong>matching full fine-tuning</strong> while reducing compute costs.</li>
  <li><strong>Cross-tuned models</strong>, trained on one dataset and used on another, show <strong>mixed results</strong>, performing well only when datasets share characteristics.</li>
</ul>

<h4 id="2-convnext-base-leads-in-supervised-fine-tuning">2. ConvNeXt-Base Leads in Supervised Fine-Tuning</h4>

<ul>
  <li><strong>ConvNeXt-Base achieved 93% accuracy</strong>, outperforming <strong>ViT-B and DINO-ResNet50 by 3.6%</strong> in classification tasks.</li>
  <li>It also dominated retrieval performance on <strong>Cars196, SOP, and Fashion</strong>, but struggled on <strong>Product-10k</strong>, where <strong>ViT-B excelled</strong>.</li>
</ul>

<h4 id="3-vit-models-offer-strong-generalization">3. ViT Models Offer Strong Generalization</h4>

<ul>
  <li><strong>ViT-base performed consistently well across datasets</strong>, balancing accuracy and efficiency.</li>
  <li><strong>ViT-large, despite being 4.6√ó more expensive to train than ViT-B, underperformed</strong>, suggesting dataset size impacts ViT-L‚Äôs effectiveness.</li>
</ul>

<h4 id="4-self-supervised-learning-ssl-shows-highvariance">4. Self-Supervised Learning (SSL) Shows High¬†Variance</h4>

<ul>
  <li><strong>DINO-ViT-B and MAWS-ViT-B showed competitive performance</strong>, but SSL models exhibited <strong>10√ó higher variance</strong> than supervised models.</li>
  <li><strong>DINOv2 was the best-performing SSL model</strong>, but overall, SSL models were less stable without additional fine-tuning.</li>
</ul>

<p><span class="image fit"></span></p>

<p><img src="https://cdn-images-1.medium.com/max/1200/1*a4akPXoge0Vcgjhw6M2xoQ.png" alt="Detailed classification and retrieval performance of top models" /></p>

<p>&lt;/span&gt;</p>

<h4 id="5-contrastive-text-image-models-excel-in-retrieval">5. Contrastive Text-Image Models Excel in Retrieval</h4>

<ul>
  <li><strong>SigLIP achieved state-of-the-art retrieval performance</strong> across five of six datasets, proving its versatility.</li>
  <li><strong>Marqo performed strongly, particularly in Product-10k, SOP, and Fashion datasets</strong>, highlighting its effectiveness for fine-grained retrieval but in image-image retrieval it did not beat SigLip (while <a href="https://www.marqo.ai/blog/introducing-marqos-ecommerce-embedding-models">according to authors</a> it beats SigLip on text-image benchmark).</li>
  <li><strong>Apple CLIP ranked among the top models</strong>, excelling in <strong>RP2K and retrieval-heavy tasks</strong>.</li>
</ul>

<p><span class="image fit"></span></p>

<p><img src="https://cdn-images-1.medium.com/max/1200/1*vYtN_0ZUPq2DGUU6KGGrxQ.png" alt="Chart showing retrieval performance of contrastive models across datasets" /></p>

<p>&lt;/span&gt;</p>

<h4 id="6-fine-tuning-is-crucial-but-not-always-necessary">6. Fine-Tuning is Crucial, But Not Always Necessary</h4>

<ul>
  <li><strong>Fine-tuning significantly improves retrieval accuracy</strong>, often <strong>matching or surpassing previous state-of-the-art (SOTA) benchmarks</strong>.</li>
  <li><strong>Top-Tuning</strong> (adding lightweight layers) <strong>boosts self-supervised and text-image models</strong>, often matching full fine-tuning at <strong>a fraction of the computational cost</strong>.</li>
</ul>

<p><span class="image fit"></span></p>

<p><img src="https://cdn-images-1.medium.com/max/1200/1*Pq7CdxIxckhlUexbRGOvjw.png" alt="Performance comparison between fine-tuning and top-tuning strategies" /></p>

<p>&lt;/span&gt;</p>

<h4 id="7-top-tuning-shows-the-biggest-gains-for-sslmodels">7. Top-Tuning Shows the Biggest Gains for SSL¬†Models</h4>

<ul>
  <li><strong>Self-supervised models saw a 5% average performance boost</strong> with top-tuning, sometimes <strong>matching fully fine-tuned models</strong>.</li>
  <li>However, <strong>DINO-ResNet50 and MAE models saw performance drops</strong> with top-tuning, showing that its effectiveness depends on architecture.</li>
</ul>

<p><span class="image fit"></span></p>

<p><img src="https://cdn-images-1.medium.com/max/800/1*JnxLfvfeZjsV35Zauq6IgA.png" alt="SSL model performance improvement with top-tuning across datasets" /></p>

<p>&lt;/span&gt;</p>

<h4 id="8-cross-tuning-is-dataset-dependent">8. Cross-Tuning is Dataset-Dependent</h4>

<ul>
  <li><strong>Cross-tuned models struggled on dissimilar datasets</strong>, with performance dropping <strong>up to -0.5 mMP@5</strong>.</li>
  <li>However, <strong>Cars196 fine-tuned models transferred well to RP2K</strong>, suggesting that dataset <strong>specialization plays a key role in cross-tuning success</strong>.</li>
</ul>

<h4 id="9-training-time-trade-offs-matter">9. Training Time Trade-Offs Matter</h4>

<ul>
  <li><strong>ConvNeXt and ViT-Large require the longest training times</strong>, making them less ideal for rapid deployment.</li>
  <li><strong>ViT-B and ResNet50 offer strong performance with much faster training</strong>, making them efficient choices.</li>
</ul>

<h3 id="practical-recommendations-for-e-commerce-ai">Practical recommendations for e-commerce AI</h3>

<p>After humndreds of tests, here‚Äôs what we recommend for online marketplaces:</p>

<h3 id="for-visual-search--recommendations">For Visual Search &amp; Recommendations</h3>

<ul>
  <li><strong>Use top-tuned contrastive models like SigLIP</strong></li>
</ul>

<p>=&gt; They perform best in image retrieval, even without labeled data.</p>

<p><strong>Pretrained contrastive models (like CLIP) perform surprisingly well retrieval.</strong>
These models, originally trained for multimodal tasks, excel at <strong>image-to-image retrieval</strong> without requiring domain-specific fine-tuning. This makes them ideal for visual search applications.</p>

<h3 id="for-auto-tagging--categorization">For auto-tagging &amp; categorization</h3>

<ul>
  <li><strong>Use fully fine-tuned supervised models (ViT, ConvNeXt) when accuracy is critical</strong></li>
</ul>

<p>=&gt; This is ideal for structured catalogs where precision matters.</p>

<p><strong>Fine-tuning is powerful but costly.</strong>
Fully fine-tuned models perform the best but require <strong>significant computational resources</strong>. If you have a dedicated ML team and the budget, this is the best approach for classification tasks.</p>

<h3 id="for-fast--cost-effective-ai-deployment">For fast &amp; cost-effective AI deployment</h3>

<ul>
  <li><strong>Use pre-trained SSL models with top-tuning</strong></li>
</ul>

<p>=&gt; They provide flexibility while keeping computational costs low.</p>

<p>Applying a lightweight classifier on frozen embeddings provides a <strong>3.9%‚Äì5.0% improvement</strong> over pre-trained models while using a fraction of the compute required for full fine-tuning.</p>

<p><strong>Takeaway:</strong> If you need a quick performance boost without massive compute costs, <strong>top-tuning is the way to go.</strong></p>

<h3 id="for-cross-domain-adaptation-eg-fashion--generalretail">For cross-domain adaptation (e.g., Fashion ‚Üí General¬†Retail)</h3>

<ul>
  <li><strong>Use cross-tuned models (trained on one dataset, applied to another)</strong></li>
</ul>

<p>=&gt; This works well when datasets share similar characteristics.</p>

<p>Adapting embeddings from one dataset to another only works well when the datasets have <strong>similar characteristics</strong>. Otherwise, performance can degrade significantly.</p>

<h3 id="what-this-means-for-leboncoin">What this means for Leboncoin</h3>

<p>At <em>Leboncoin</em>, this research helps us make <strong>data-driven decisions on AI adoption</strong> for improving product search, recommendations, and categorization. By choosing the right AI models, we can:</p>

<ul>
  <li><strong>Deliver better models</strong> exploiting users images.</li>
  <li><strong>Optimize AI costs</strong> by using efficient training strategies.</li>
  <li><strong>Guide ML teams</strong> towards most effective strategy for the embeddings choice</li>
</ul>

<p>This study confirms that <strong>choosing the right image embedding strategy can significantly impact e-commerce performance &amp; ml time to market</strong>.</p>

<p>üí° <strong>If your business relies on image-based search, classification, or recommendations, investing in contrastive embeddings and lightweight tuning is a no-brainer.</strong> It delivers high accuracy while keeping computational costs in check.</p>

<p>If you‚Äôre working on <strong>image search, retrieval, or classification in e-commerce</strong>, what‚Äôs been your experience? Let‚Äôs discuss in the comments</p>

<footer>
  <p>Exported from <a href="https://medium.com">Medium</a> on April 13,
    2025.</p>
  <p><a href="https://medium.com/leboncoin-tech-blog/how-to-choose-the-best-image-embeddings-for-your-e-commerce-business-8006f17b495a">View
      the original. This article was orignally co-authored by Cognition team members and published by LeboncoinTech</a></p>
</footer>
<script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-584ec4ce89deed84"></script>

<div class="addthis_inline_share_toolbox"></div>


      <div class="backnext btnbottom">
                <div class="back">
        </div>
        <div class="next">
        <div class="npointer"><a href="/works/genai-workshop-implementation"
         class="button small" title="Next">Next &gt;</a></div>
         <!-- class="icon fa-forward" title="Next"></a></div> -->
        <div class="ntitle"><a href="/works/genai-workshop-implementation">GenAI Workshop - Insights on implementing RAG in production environments</a></div>
        </div>

      </div>
      <script>
      document.body.onkeyup = function(e){
        if (e.keyCode == '37') { window.location = ''; }
        if (e.keyCode == '39') { window.location = '/works/genai-workshop-implementation'; }
      };
      </script>
<hr style="margin-bottom:12px;" />
<div class="author">
<!-- style="margin:8px 28px 12px 0;position:relative;float:left;"> -->
  <!-- style="position: relative; float: left;margin:0;padding:0;" -->
  <div style="display:inline-block;border-radius:7px;overflow:hidden;height:100px;width:100px;background:url(/images/ula.jpg);background-size:100px;"></div>
  <div style="display:inline-block;padding-left:12px;vertical-align:top;"><b>by:<br />Urszula Czerwinska</b><br />(<a href="mailto:urszula.czerwinska@cri-paris.org">urszula.czerwinska@cri-paris.org</a>)<br
    /><i><a href="http://urszulaczerwinska.github.io" target="_blank">http://urszulaczerwinska.github.io</a></i>
  </div>
  <div class="auth-desc"><p> Senior Data Scientist / Deep Learning Engineer </p> PhD in Bio-Mathematics, Data Science & Machine Learning
</div>
</div>
<hr style="margin-top:9px;" />

  
    </div>
  </section>


      </article>

      <!-- Footer -->
<footer id="footer">
  <ul class="icons">
    <li><a target="_blank" href="https://twitter.com/ulalaparis" class="icon fa-twitter"
           ><span class="label">twitter</span></a></li>
    <li><a target="_blank" href="https://github.com/urszulaczerwinska" class="icon fa-github"
           ><span class="label">github</span></a></li>
    <li><a target="_blank" href="https://linkedin.com/in/urszulaczerwinska" class="icon fa-linkedin-square"
           ><span class="label">linkedin-square</span></a></li>
    <li><a target="_blank" href="mailto:ulcia.liberte@gmail.com" class="icon fa-envelope"
           ><span class="label">E-mail</span></a></li>
  </ul>
  <ul class="copyright">
    <li>&copy; 2016,
    2025
      Urszula Czerwinska</li>
    <li><a href="/credits/">Credits</a></li>
  </ul>
</footer>


      <!-- Scripts -->
<script src="/js/jquery.min.js"></script>
<script src="/js/jquery.scrollex.min.js"></script>
<script src="/js/jquery.scrolly.min.js"></script>
<script src="/js/skel.min.js"></script>
<script src="/js/util.js"></script>
<!--[if lte IE 8]><script src="/js/ie/respond.min.js"></script><![endif]-->
<script src="/js/main.js"></script>

    </div>

  </body>



</html>