<!DOCTYPE html>
<!--
  Original Design: Spectral by HTML5 UP
    html5up.net | @n33co
    Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
  Jekyll build mod and further hacks by @arkadianriver, MIT license
-->
<html>
  <head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Deep Dive in PaddleOCR inference</title>
  <meta name="google-site-verification" content="WrNs4kb-PL779UWOhOTLegwiql-42uVzYDfCoJxQRPs" />
  <meta name="description" content="Discover the complexities of using PaddleOCR as a Text in Image service and how the Cognition team overcame the challenges to improve user experience">
  <!--[if lte IE 8]><script src="/js/ie/html5shiv.js"></script><![endif]-->
  <link rel="canonical" href="http://localhost:4000/works/deep-dive-in-paddleocr-inference">
  <link rel="shortcut icon" href="/favicon.ico">

  <link rel="stylesheet" href="/css/main.css" />


  
  <!-- Add JSON-LD for Article and Person Schema -->
  <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "Article",
      "headline": "Deep Dive in PaddleOCR inference"
      "description": "A comprehensive exploration of the complexities of using PaddleOCR as a Text in Image service and the strategies employed by the Cognition team to enhance it...",
      "datePublished": "2023-03-06T00:00:00+01:00",
      "dateModified": "2023-03-06T00:00:00+01:00",
      "keywords": "data science, OCR, deep learning, computer vision, machine learning",
      "author": {
        "@type": "Person",
        "name": "Urszula Czerwinska",
        "description": "Data Scientist & Deep Learning Engineer",
        "url": "https://www.linkedin.com/in/urszula-czerwinska/"
      },
      "image": {
        "@type": "ImageObject",
        "url": "http://localhost:4000/images/writing.jpeg",
        "width": 1200,
        "height": 630
      },
      "publisher": {
        "@type": "Organization",
        "name": "Urszula Czerwinska",
        "logo": {
          "@type": "ImageObject",
          "url": "http://localhost:4000/images/logo.png",
          "width": 60,
          "height": 60
        }
      },
      "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "http://localhost:4000/works/deep-dive-in-paddleocr-inference"
      }
    }
</script>


  <!--[if lte IE 8]><link rel="stylesheet" href="/css/ie8.css" /><![endif]-->
  <!--[if lte IE 9]><link rel="stylesheet" href="/css/ie9.css" /><![endif]-->
  <style>
  #main > header {
    background-image: -moz-linear-gradient(top, rgba(0,0,0,0.5), rgba(0,0,0,0.5)), url("/images/text-in-image.jpg");
    background-image: -webkit-linear-gradient(top, rgba(0,0,0,0.5), rgba(0,0,0,0.5)), url("/images/text-in-image.jpg");
    background-image: -ms-linear-gradient(top, rgba(0,0,0,0.5), rgba(0,0,0,0.5)), url("/images/text-in-image.jpg");
    background-image: linear-gradient(top, rgba(0,0,0,0.5), rgba(0,0,0,0.5)), url("/images/text-in-image.jpg");
  }
  </style>
  <!--[if lte IE 9]>
  <style>
  #main > header {
    background-image: url("/images/text-in-image.jpg");
  }
  </style>
  -->
  <link rel="alternate" type="application/rss+xml" title="Urszula Czerwinska | Data Scientist & Deep Learning Engineer" href="http://localhost:4000/feed.xml">
</head>

  <!-- Semantic Schema generated by InLinks -->


  
  <body>


    <!-- Page Wrapper -->
    <div id="page-wrapper">

      <!-- Header -->
<header id="header">
  <h1><a href="/index.html"> <span><img src="/favicon.ico" alt="Logo" style="height: 40px; vertical-align: middle; margin-right: 5px;"> Urszula Czerwinska </span></a></h1>
  <nav id="nav">
    <ul>
      <li class="special">
        <a href="#menu" class="menuToggle">  <span>
          Menu
        </span></a>
        <div id="menu">
          <ul>
            <li><a href="/index.html">Home</a></li>
            <li><a href="/about/">About</a></li>
            <li><a href="/works/">Works</a></li>
            <li><a href="/thoughts/">Thoughts</a></li>
            <li><a href="/feed.xml"
                   class="icon fa-feed"> RSS Feed</a></li>
          </ul>
        </div>
      </li>
    </ul>
  </nav>
</header>


      <article id="main">

          <header>
    <h2>Deep Dive in PaddleOCR inference</h2>
    <p>Discover the complexities of using PaddleOCR as a Text in Image service and how the Cognition team overcame the challenges to improve user experience</p>
  </header>
  <ul class="breadcrumb">
  <li><a href="/index.html">Home</a></li>
  <li>Deep Dive in PaddleOCR inference</li>
</ul>



          <section class="wrapper style5">
    <div class="inner">
      <span id="post-date">6 March 2023</span><hr
        style="margin-top:3px;" />
      <h4>Skills</h4>
  <ul class="techlist">
<li><span class="tech">data science</span></li>
<li><span class="tech">OCR</span></li>
<li><span class="tech">deep learning</span></li>
<li><span class="tech">computer vision</span></li>
<li><span class="tech">machine learning</span></li>
</ul>

  

      <head>
  <meta name="description" content="A deep dive into the complexities of using PaddleOCR for text extraction from images and how the Cognition team improved the service. Learn about the challenges and solutions that enhanced user experience in OCR services." />
</head>

<p>This article is a deep dive into part of our work as described in <a href="/works/text-in-image-2-0-improving-ocr-service-with-paddleocr"><strong>Article 1: Text in Image 2.0: improving OCR service with PaddleOCR</strong></a><strong>.</strong></p>

<p>We are Cognition, an <a href="https://www.adevinta.com/">Adevinta</a> Computer Vision Machine Learning (ML) team working on solutions for our marketplaces. Adevinta is a global classifieds specialist with market-leading positions in key European markets that aims to find perfect matches between its users and the platforms’ goods. As a Global Team, our team, Cognition, provides image processing APIs to all of our marketplaces.</p>

<p>In the process of improving our OCR API for text extraction from images, we updated our existing Text in Image service to the <a href="https://github.com/PaddlePaddle/PaddleOCR">PaddleOCR</a> framework, which was the winner of our benchmarks. In order to test if this framework was the most suitable solution, we carried out a deeper analysis of their code base. This article shares the challenges we encountered and how we overcame them.</p>

<p>We believe our code version is easier to work with, given the use case of text extraction from images. The different steps and pre-processing and post-processing parts are clearly separated so they can be called independently, which should make further community extensions easier to add. It also makes putting into production easier, as the simplified, modular code combines well with the structure of <em>inference.py</em> for serving SageMaker endpoints. Our proposed code version does not alter predictions (compared to the 2.6 release) for images.</p>

<h2 id="understanding-the-paddleocr-framework">Understanding the PaddleOCR framework</h2>

<p><a href="https://github.com/PaddlePaddle/Paddle">PaddlePaddle</a> (short for Parallel Distributed Deep Learning) is an open source deep learning platform developed by Baidu Research. It is written in C++ and Python, and is designed to be easy to use and efficient for large-scale machine learning tasks.</p>

<p>PaddlePaddle provides a range of tools and libraries for building and training deep learning models, including support for convolutional neural networks (CNNs), recurrent neural networks (RNNs) and long short-term memory (LSTM) networks.</p>

<p><a href="https://github.com/PaddlePaddle/PaddleOCR">PaddleOCR</a> builds on PaddlePaddle, an unfamiliar framework that our team had not used before. To make things even more challenging, PaddleOCR is not just one algorithm, it includes a range of pre-trained models and tools for recognising text in images and documents, as well as for training custom OCR models.</p>

<p>PaddleOCR is divided into two main sections:</p>

<ul>
  <li><strong>PP-OCR</strong>, an OCR system used for text extraction from images</li>
  <li><strong>PP-Structure</strong>, a document analysis system which aims to perform layout analysis and table recognition</li>
</ul>

<p>PP-OCR exists in three different versions (V1, V2 and V3). In these different releases, major improvements were brought to the models’ architecture.</p>

<p>For our Text in Image service update, we focused on the most recent and most performant PP-OCRv3 release.</p>

<h3 id="the-paddleocrv3-models-architecture">The PaddleOCRv3 models architecture</h3>

<p><span class="image fit">
<img src="https://cdn-images-1.medium.com/max/800/0*1mI3YTIjAut_QMrl" alt="PaddleOCRv3 Architecture" />
</span></p>

<p>PP-OCRv3 is composed of three parts: detection, classification and recognition, all of which can be used independently. Each part has its own model trained with the PaddlePaddle framework. For those interested, model details can be found in this dedicated research article PP-OCRv3: <a href="https://arxiv.org/abs/2206.03001v2">More Attempts for the Improvement of Ultra Lightweight OCR System (Yanjun et al., 2022)</a>.</p>

<p>PP-OCRv3 text detection is made with the Differentiable Binarization algorithm (<a href="https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.6/doc/doc_en/algorithm_det_db_en.md">DB</a>) trained using distillation strategy. The PP-OCRv3 recogniser is optimised based on the text recognition algorithm, Scene Text Recognition with a Single Visual Model (<a href="https://arxiv.org/abs/2205.00159">SVTR, Du et al. 2022)</a>.</p>

<p>PP-OCRv3 adopts the text recognition network SVTR_LCNet, and uses <a href="https://arxiv.org/abs/2002.01276">the guided training of Connectionist Temporal Classification (CTC</a>, Z<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lin%2C+Z">hiping</a> et al., 2020) by the attention, data augmentation strategy, TextConAug, Unified Deep Mutual Learning and Unlabelled Images Mining (first introduced in <a href="https://arxiv.org/abs/2109.03144">PaddleOCRv2, Yanjun et al. 2021</a>). The Text classifier is a simple binary classifier with classes 0 and 180°.</p>

<h3 id="paddleocr-inference-in-practice">PaddleOCR inference in practice</h3>

<p>While testing on our benchmarks, we used the PaddleOCR code for inference with default parameters and “latin” as a language (see their <a href="https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.6/doc/doc_en/quickstart_en.md">QuickStart page</a>).</p>

<p>Reading the documentation and looking into the class parameters, we saw lots of model combinations to test and therefore more opportunities to potentially improve our score.</p>

<p>For instance, the <a href="https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.6/doc/doc_en/detection_en.md">documentation</a> suggests there is a choice between “DB” and “EAST” algorithms for detection, but it’s only the main inference <a href="https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.6/paddleocr.py">script</a> where the algorithm has to be “DB” — the <a href="https://github.com/PaddlePaddle/PaddleOCR/blob/23e034c40ecd5755af48d7b14dcc1bf6c5cf1128/tools/infer/predict_det.py#L62">script</a> of detection inference goes through a long list of algorithms. A similar situation occurs with text recognition where the pre-trained algorithm for Latin is “SVTR_LCNet”, but in <a href="https://github.com/PaddlePaddle/PaddleOCR/blob/23e034c40ecd5755af48d7b14dcc1bf6c5cf1128/paddleocr.py#L51">theory</a>, the accepted values are “‘CRNN’ and ‘SVTR_LCNet’ with the general <a href="https://github.com/PaddlePaddle/PaddleOCR/blob/18ddb6d5f9bdc2c1b0aa7f6e399ec0f76119dc87/doc/doc_en/inference_args_en.md">documentation</a> mentioning a plethora of models.</p>

<p>Pre-trained English models are available in “‘CRNN’ and ‘SVTR_LCNet’ architectures. However, to find the information, the user would need to look into the pretrained model <a href="https://github.com/PaddlePaddle/PaddleOCR/blob/18ddb6d5f9bdc2c1b0aa7f6e399ec0f76119dc87/configs/rec/PP-OCRv3/en_PP-OCRv3_rec.yml#L39">config</a>. If the user does not specify the “rec_algorithm”, the default value, “SVTR_LCNet”, would be used, even if it isn’t correct. This doesn’t actually make any difference to the inference <a href="https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.6/tools/infer/predict_rec.py">code</a> as none of the “if” applies to ‘CRNN’ or ‘SVTR_LCNet’.</p>

<p>In order to test a different architecture, we would need to train it ourselves and chain dedicated scripts.</p>

<h2 id="clarifying-paddleocr-inference">Clarifying PaddleOCR inference</h2>

<p>From digging into the code, we discovered several complexities, unnecessary for our use case. Firstly, the code seemed to grow organically, where the inference version is a limited choice entry to the multi-option code. This leaves us with numerous “factory patterns” and “if .. elses”, where the user has no choice at all. The English documentation was confusing and referenced different usage cases. We struggled to follow the logic as it neither explained parameters, nor clearly defined the limitations of the inference code.</p>

<p>Despite these complexities, we managed to clarify the general way of working, calling the PaddleOCR.ocr() method from the ‘master’ file, <em>paddleocr.py</em>.</p>

<p><span class="image fit">
<img src="https://cdn-images-1.medium.com/max/800/0*zwImfJ-4pOxDvrEI" alt="PaddleOCR.ocr() Method" />
</span></p>

<p>The input image and parameters are entered into the PaddleOCR.ocr() method which calls TextSystem class in order: TextDetector, TextClassifier and TextRecogniser, with a selection of helper functions, including one that formats the outputs of TextDetector into a list of cropped images being input to TextClassifier and TextRecogniser.</p>

<p>The PaddleOCR.ocr() method is parsing params, including the language, version, type of OCR (or structure), downloads inference models and imports actual image (with check_image).</p>

<p>If we want our image to go through a full OCR process, the TextSystem class will sequentially call classes responsible for detection, classification and recognition.</p>

<p><span class="image fit">
<img src="https://cdn-images-1.medium.com/max/800/0*B-7pY0A4Xv7eNTcr" alt="TextSystem Class Flow" />
</span></p>

<p>Each of the main classes has an <em>__init__</em> method that initialises pre- &amp; post- processing classes and loads the model (create_predictor), and <em>__call__</em> method that executes (pre- &amp;) post-processing on the image and performs the model inference for the input image(s).</p>

<p>Most of the scripts used for inference can be found under ‘tools/infer/’. The pre-processing scripts are under “ppocr/data/imaug/operators.py”. The post-processing classes are under ‘ppocr/postprocess/’.</p>

<p>This schema enables us to reduce the essential inference code to just a couple of files and better understand exactly how the code works. To make it easier to maintain, we decided to reformat the code, keeping only the essential parts for our use case.</p>

<h2 id="paddleocr-inference-code-caveats-andfixes">PaddleOCR inference code caveats and fixes</h2>

<p>Let’s walk you through the PaddleOCR features we didn’t like and suggestions on how they could be improved.</p>

<h3 id="spaghetti-code">Spaghetti code</h3>

<p>Overall, most of the code is in object oriented programming style where classes are not modular and most things happen in very long <em>__init__</em> and <em>__call__</em> methods. We have noticed (fig. 2 and fig. 3) that generally, three parts can be extracted: pre-processing, inference and post-processing. We have removed ‘create_operators’ and ‘build_post_process’ intermediate functions and called directly the class performing the task such as “DBPostprocess” and “NormalizeImage”. To make things more straightforward, we transformed them into simple functions, performing what their <em>__call__</em> method was doing before. This leaves us with more modular code and direct logic that fits our needs.</p>

<h3 id="parameter-parsing"><em>Parameter parsing</em></h3>

<p>We found it problematic that the inference class requires 105 parameters, of which more than 70 were ignored.</p>

<p><span class="image fit">
<img src="https://cdn-images-1.medium.com/max/800/1*jPMJx-wOF-R5DsmqJFs5BA.png" alt="PaddleOCR inference parameters are not all used" />
</span></p>

<p><a href="https://github.com/PaddlePaddle/PaddleOCR/blob/18ddb6d5f9bdc2c1b0aa7f6e399ec0f76119dc87/doc/doc_en/inference_args_en.md">English documentation</a> lists the parameters and gives a succinct definition of them. In the code, they are defined in at least three different places: <a href="https://github.com/PaddlePaddle/PaddleOCR/blob/23e034c40ecd5755af48d7b14dcc1bf6c5cf1128/paddleocr.py#L307">paddleocr.py</a>, <a href="https://github.com/PaddlePaddle/PaddleOCR/blob/23e034c40ecd5755af48d7b14dcc1bf6c5cf1128/ppstructure/utility.py#L21">utility.py</a> and different <a href="https://github.com/PaddlePaddle/PaddleOCR/blob/34b9569800a38af41a27ed893b12567757ef6c89/tools/infer/utility.py#L34">utility.py</a>.</p>

<p>However while executing the code, we found that only 20 parameters were useful in our refactored code:</p>

<p>When rewriting the code, we cleaned the parameter list, leaving only the relevant parameters.</p>

<h3 id="parameter-impact-on-prediction"><em>Parameter impact on prediction</em></h3>

<p>Some of the parameter definitions and effect they would have when changed from default, were not clear to us. We built a <a href="https://streamlit.io/">Streamlit app</a> to visualise the changes in params on the predictions. For instance, “unclip ratio” would impact the size of the box, and “threshold” would detect two bounding boxes instead of one. We advise you to play with your own data and model to see how different parameters affect the detection. Overall, we were not able to see a major improvement from changing defaults.</p>

<p><span class="image fit">
<img src="https://cdn-images-1.medium.com/max/800/0*B4uqn-7vcxfu5aPz" alt="The illustration of PaddleOCR parameters impact on the machine learning model prediction" />
</span></p>

<h3 id="language-choice"><em>Language choice</em></h3>

<p>Normally in our role, we work with “‘PP-OCRv3”, the most recent version of the framework. As we are dealing with European languages, we would choose “fr”, “en”, “es” as the “lang” param, thinking that this means different models are being called. However, while looking into the paddleocr.py, we saw how the languages are interpreted:</p>

<p>The first definition serves to define the recognition model name/path. But if we typed “fr” or “es”, it becomes lang = “latin”, yet “en” remains “en”. Then another simplification happens for the detection model.</p>

<blockquote>
  <p>if lang in [“en”, “latin”]:</p>
</blockquote>

<blockquote>
  <p>det_lang = “en”</p>
</blockquote>

<p>We are left with an English detection model and a Latin recognition model for any European language written with Latin characters except English, which has its own recognition model.</p>

<h3 id="downloading-models"><em>Downloading models</em></h3>

<p>Based on the language parameter and framework version, the first time we call the PaddleOCR class with those parameters, the model will be downloaded from the url encoded in paddleocr.py.</p>

<p>Firstly, this could cause some issues when running the code in secure or offline environments.</p>

<p>Secondly, we found inconsistencies between the model urls in the paddleocr.py and the models provided in the dedicated <a href="https://github.com/PaddlePaddle/PaddleOCR/blob/18ddb6d5f9bdc2c1b0aa7f6e399ec0f76119dc87/doc/doc_en/models_list_en.md">documentation page</a>. For instance, “en_PP-OCRv3_det_slim” is not an option when models are downloaded by the paddleocr.py script. In order to use some of the models from Model Zoo, a database of pre-trained models and code, you would need to download the model and provide the path to it manually.</p>

<p>In order to remove this ambiguity and use the specific model we needed, we decided to pre-download the chosen model, then provide the path directly. In the original code, it is possible to provide det_model_dir, cls_model_dir and rec_model_dir. The language param will then be ignored and any pre-trained model with the accepted backbones can be used. After this process, we removed the model download functionality from our code.</p>

<h3 id="using-onnxmodels"><em>Using ONNX models</em></h3>

<p>PaddleOCR provides a <a href="https://github.com/PaddlePaddle/PaddleOCR/blob/18ddb6d5f9bdc2c1b0aa7f6e399ec0f76119dc87/deploy/paddle2onnx/readme.md">handy way</a> to export models to the <a href="https://onnx.ai/">ONNX framework</a> that can serve or integrate in different pipelines. We exported the pre-trained models using PaddleOCR instructions. In the PaddleOCR class, there is a parameter “use_onnx”. If one sets “use_onnx” and provides a direct path to the ONNX models to PaddleOCR(), the model would use the ONNX model for prediction. However, there is a small bug that occurs while running ONNX with GPUs, described further in this <a href="https://github.com/PaddlePaddle/PaddleOCR/issues/8688">issue</a>.</p>

<p>We applied the modification suggested and tested the code with ONNX models, obtaining satisfactory results on both CPU and GPU (even though we noticed small numerical differences between the Paddle and ONNX model versions).</p>

<h3 id="documentation"><em>Documentation</em></h3>

<p>If you look at the <a href="https://github.com/PaddlePaddle/PaddleOCR/tree/18ddb6d5f9bdc2c1b0aa7f6e399ec0f76119dc87/doc">documentation pages</a>, you will find a lot of resources in both English and Chinese. However, when looking at <a href="https://github.com/PaddlePaddle/PaddleOCR/issues">Issues</a>, you will find most of them are in Chinese, Japanese or Korean. The same applies to blog posts and community resources online. We also found that some documentation is only partially translated to English and the Chinese version contains much more detail.</p>

<p>We did not find a solution for this. We made sure to always check both the English and Chinese documentation (translated to English by an automatic translator) to ensure that we have all the possible information.</p>

<h3 id="tests--pylint-typing"><em>Tests &amp;</em> <a href="https://pylint.pycqa.org/en/latest/"><em>pylint</em></a> <em>&amp;</em> <a href="https://docs.python.org/3/library/typing.html"><em>typing</em></a></h3>

<p>In general, as the original code is not modular, it was not tested according to the standards of our team. Once we cleaned and simplified the code, we worked on linting and variable typing. Our next step will be to write meaningful unit tests to secure the code base.</p>

<h2 id="summary">Summary</h2>

<p>PaddleOCR is a powerful and optimised library for the extraction of text from images. However, we found that the code doesn’t fit the standards of our team as it is too complex to maintain and understand. In this article, we pointed out some of the pain points for us that other PaddleOCR users may experience when working with this framework. The fixes we proposed made our lives easier and the code more transparent for any team member and the wider community, without compromising the speed or the original model accuracy.</p>

<footer>
  <p>Exported from <a href="https://medium.com">Medium</a> on June 06,
    2023.</p>
  <p><a href="https://medium.com/adevinta-tech-blog/deep-dive-in-paddleocr-inference-e86f618a0937">View
      the original. This article was orignally co-authored by Cognition team members, special credits to Joaquin Cabezas</a></p>
</footer>
<script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-584ec4ce89deed84"></script>

<div class="addthis_inline_share_toolbox"></div>


      <div class="backnext btnbottom">
                <div class="back">
        <div class="bpointer"><a href="/works/text-in-image-2-0-improving-ocr-service-with-paddleocr"
         class="button small" title="Back">&lt; Back</a></div>
         <!-- class="icon fa-backward" title="Back"></a></div> -->
        <div class="btitle"><a href="/works/text-in-image-2-0-improving-ocr-service-with-paddleocr">Text in Image 2.0 - improving OCR service with PaddleOCR</a></div>
        </div>
        <div class="next">
        <div class="npointer"><a href="/works/ner_cc"
         class="button small" title="Next">Next &gt;</a></div>
         <!-- class="icon fa-forward" title="Next"></a></div> -->
        <div class="ntitle"><a href="/works/ner_cc">Named Entity Recognition Tool by Cour de Cassation</a></div>
        </div>

      </div>
      <script>
      document.body.onkeyup = function(e){
        if (e.keyCode == '37') { window.location = '/works/text-in-image-2-0-improving-ocr-service-with-paddleocr'; }
        if (e.keyCode == '39') { window.location = '/works/ner_cc'; }
      };
      </script>
<hr style="margin-bottom:12px;" />
<div class="author">
<!-- style="margin:8px 28px 12px 0;position:relative;float:left;"> -->
  <!-- style="position: relative; float: left;margin:0;padding:0;" -->
  <div style="display:inline-block;border-radius:7px;overflow:hidden;height:100px;width:100px;background:url(/images/ula.jpg);background-size:100px;"></div>
  <div style="display:inline-block;padding-left:12px;vertical-align:top;"><b>by:<br />Urszula Czerwinska</b><br />(<a href="mailto:urszula.czerwinska@cri-paris.org">urszula.czerwinska@cri-paris.org</a>)<br
    /><i><a href="http://urszulaczerwinska.github.io" target="_blank">http://urszulaczerwinska.github.io</a></i>
  </div>
  <div class="auth-desc"><p> Senior Data Scientist / Deep Learning Engineer </p> PhD in Bio-Mathematics, Data Science & Machine Learning
</div>
</div>
<hr style="margin-top:9px;" />

  
    </div>
  </section>


      </article>

      <!-- Footer -->
<footer id="footer">
  <ul class="icons">
    <li><a target="_blank" href="https://twitter.com/ulalaparis" class="icon fa-twitter"
           ><span class="label">twitter</span></a></li>
    <li><a target="_blank" href="https://github.com/urszulaczerwinska" class="icon fa-github"
           ><span class="label">github</span></a></li>
    <li><a target="_blank" href="https://linkedin.com/in/urszulaczerwinska" class="icon fa-linkedin-square"
           ><span class="label">linkedin-square</span></a></li>
    <li><a target="_blank" href="mailto:ulcia.liberte@gmail.com" class="icon fa-envelope"
           ><span class="label">E-mail</span></a></li>
  </ul>
  <ul class="copyright">
    <li>&copy; 2016,
    2024
      Urszula Czerwinska</li>
    <li><a href="/credits/">Credits</a></li>
  </ul>
</footer>


      <!-- Scripts -->
<script src="/js/jquery.min.js"></script>
<script src="/js/jquery.scrollex.min.js"></script>
<script src="/js/jquery.scrolly.min.js"></script>
<script src="/js/skel.min.js"></script>
<script src="/js/util.js"></script>
<!--[if lte IE 8]><script src="/js/ie/respond.min.js"></script><![endif]-->
<script src="/js/main.js"></script>

    </div>

  </body>



</html>