<!DOCTYPE html>
<!--
  Original Design: Spectral by HTML5 UP
    html5up.net | @n33co
    Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
  Jekyll build mod and further hacks by @arkadianriver, MIT license
-->
<html>
  <head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Push the limits of machine learning explainability</title>
  <meta name="google-site-verification" content="WrNs4kb-PL779UWOhOTLegwiql-42uVzYDfCoJxQRPs" />
  <meta name="description" content="an ultimate guide to SHAP library">
  <!--[if lte IE 8]><script src="/js/ie/html5shiv.js"></script><![endif]-->
  <link rel="canonical" href="http://urszulaczerwinska.github.io/works/interpretability-shap">
  <link rel="shortcut icon" href="/favicon.ico">

  <link rel="stylesheet" href="/css/main.css" />


  
  <!-- Add JSON-LD for Article and Person Schema -->
  <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "Article",
      "headline": "Push the limits of machine learning explainability"
      "description": "Discover how to push the boundaries of explainability in data science and machine learning. This comprehensive guide to SHAP interpretability (SHapley Additi...",
      "datePublished": "2020-03-01T00:00:00+01:00",
      "dateModified": "2020-03-01T00:00:00+01:00",
      "keywords": "data science, interpretability, XAI, machine learning, explainable AI",
      "author": {
        "@type": "Person",
        "name": "Urszula Czerwinska",
        "description": "Data Scientist & Deep Learning Engineer",
        "url": "https://www.linkedin.com/in/urszula-czerwinska/"
      },
      "image": {
        "@type": "ImageObject",
        "url": "http://urszulaczerwinska.github.io/images/writing.jpeg",
        "width": 1200,
        "height": 630
      },
      "publisher": {
        "@type": "Organization",
        "name": "Urszula Czerwinska",
        "logo": {
          "@type": "ImageObject",
          "url": "http://urszulaczerwinska.github.io/images/logo.png",
          "width": 60,
          "height": 60
        }
      },
      "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "http://urszulaczerwinska.github.io/works/interpretability-shap"
      }
    }
</script>


  <!--[if lte IE 8]><link rel="stylesheet" href="/css/ie8.css" /><![endif]-->
  <!--[if lte IE 9]><link rel="stylesheet" href="/css/ie9.css" /><![endif]-->
  <style>
  #main > header {
    background-image: -moz-linear-gradient(top, rgba(0,0,0,0.5), rgba(0,0,0,0.5)), url("/images/engineering.jpg");
    background-image: -webkit-linear-gradient(top, rgba(0,0,0,0.5), rgba(0,0,0,0.5)), url("/images/engineering.jpg");
    background-image: -ms-linear-gradient(top, rgba(0,0,0,0.5), rgba(0,0,0,0.5)), url("/images/engineering.jpg");
    background-image: linear-gradient(top, rgba(0,0,0,0.5), rgba(0,0,0,0.5)), url("/images/engineering.jpg");
  }
  </style>
  <!--[if lte IE 9]>
  <style>
  #main > header {
    background-image: url("/images/engineering.jpg");
  }
  </style>
  -->
  <link rel="alternate" type="application/rss+xml" title="Urszula Czerwinska | Data Scientist & Deep Learning Engineer" href="http://urszulaczerwinska.github.io/feed.xml">
</head>

  <!-- Semantic Schema generated by InLinks -->


  
  <body>


    <!-- Page Wrapper -->
    <div id="page-wrapper">

      <!-- Header -->
<header id="header">
  <h1><a href="/index.html"> <span><img src="/favicon.ico" alt="Logo" style="height: 40px; vertical-align: middle; margin-right: 5px;"> Urszula Czerwinska </span></a></h1>
  <nav id="nav">
    <ul>
      <li class="special">
        <a href="#menu" class="menuToggle">  <span>
          Menu
        </span></a>
        <div id="menu">
          <ul>
            <li><a href="/index.html">Home</a></li>
            <li><a href="/about/">About</a></li>
            <li><a href="/works/">Works</a></li>
            <li><a href="/thoughts/">Thoughts</a></li>
            <li><a href="/feed.xml"
                   class="icon fa-feed"> RSS Feed</a></li>
          </ul>
        </div>
      </li>
    </ul>
  </nav>
</header>


      <article id="main">

          <header>
    <h2>Push the limits of machine learning explainability</h2>
    <p>an ultimate guide to SHAP library</p>
  </header>
  <ul class="breadcrumb">
  <li><a href="/index.html">Home</a></li>
  <li>Push the limits of machine learning explainability</li>
</ul>



          <section class="wrapper style5">
    <div class="inner">
      <span id="post-date">1 March 2020</span><hr
        style="margin-top:3px;" />
      <h4>Skills</h4>
  <ul class="techlist">
<li><span class="tech">data science</span></li>
<li><span class="tech">interpretability</span></li>
<li><span class="tech">XAI</span></li>
<li><span class="tech">machine learning</span></li>
<li><span class="tech">explainable AI</span></li>
</ul>

  

      <head>
  <meta
    name="description"
    content="Explore the power of SHAP in enhancing model interpretability in data science and machine learning. This guide provides detailed insights and practical examples for data professionals."
  />
</head>

<section>
  <h1>
    Summary - A Comprehensive Guide to SHAP: Enhancing Machine Learning
    Interpretability
  </h1>
  <p>
    This article is a guide to the advanced and lesser-known features of the
    python SHAP library. It is based on an example of tabular data
    classification.
  </p>
  <p>
    But first, let’s talk about the motivation and interest in explainability at
    Saegus that motivated and financed my explorations.
  </p>

  <span class="image fit"
    ><img src="/images/engineering.jpg" alt="" width="70%"
  /></span>

  <h3>The Theory Behind Explainability in AI and Machine Learning</h3>
  <p>
    The explainability of algorithms is taking more and more place in the
    discussions about Data Science. We know that algorithms are powerful, we
    know that they can assist us in many tasks: price prediction, document
    classification, video recommendation.
  </p>
  <p>
    From now on, more and more questions are being asked about this
    prediction:<br />- Is it ethical?<br />- Is it affected by bias?<br />- Is
    it used for the right reasons?
  </p>
  <p>
    In many domains such as medicine, banking or insurance, algorithms can be
    used if, and only if, it is possible to trace and explain (or better,
    interpret) the decisions of these algorithms.
  </p>
  <h4>Key Terminology in Machine Learning Interpretability and SHAP</h4>
  <p>In this article we would like to distinguish the terms:</p>
  <p>
    <strong>Explainability</strong>: possibility to explain from a technical
    point of view the prediction of an algorithm.
  </p>
  <p>
    <strong>Interpretability</strong>: the ability to explain or provide meaning
    in terms that are understandable by a human being.
  </p>
  <p>
    <strong>Transparency</strong>: a model is considered transparent if it is
    understandable on its own.
  </p>
  <h4>Why Explainability Matters in Data Science ?</h4>
  <p>
    Interpretability helps to ensure impartiality in decision-making, i.e. to
    detect and therefore correct biases in the training data set. In addition,
    it facilitates robustness by highlighting potential adverse disturbances
    that could change the prediction. It can also act as an assurance that only
    significant features infer the outcome.
  </p>
  <p>
    Sometimes, it would be more advisable to abandon the machine learning
    approach, and use deterministic algorithms based on rules justified by
    industry knowledge or legislation [1].
  </p>
  <p>
    Nevertheless, it is too tempting to access the capabilities of machine
    learning algorithms that can offer high accuracy. We can talk about the
    trade-off between accuracy and explainability. This trade-off consists in
    discarding more complex models such as neural networks for simpler
    algorithms that can be explained.
  </p>
  <span class="image fit"
    ><img src="/images/model-interpret.png" alt="" /><em
      >As described in [2] relation between interpretability and accuracy of the
      model. For some models improvements can be made towards a more
      interpretable or more relevant model.</em
    ></span
  >
  <p>
    To achieve these goals, a new field has emerged: XAI (Explainable Artificial
    Intelligence), which aims to produce algorithms that are both powerful and
    explainable.
  </p>
  <p>
    Many frameworks have been proposed to help explain non-transparent
    algorithms. A very good presentation of these methods can be found in the
    Cloudera white paper [3].
  </p>
  <p>
    In this article we will deal with one of the most used frameworks: SHAP.
  </p>
  <h4>Exploring the Audience for Explainable AI in Data Science</h4>
  <p>
    Different profiles interested in expainability or interpretability have been
    identified:
  </p>
  <ul>
    <li>
      Business expert/model user — in order to trust the model, understand the
      causality of the prediction
    </li>
    <li>
      Regulatory bodies to certify compliance with the legislation, auditing
    </li>
    <li>
      Managers and executive board to assess regulatory compliance, understand
      enterprise AI applications
    </li>
    <li>
      Users impacted by model decisions in order to understand the situation,
      verify decisions
    </li>
    <li>
      Data scientist, developer, PO to ensure/improve product performance, find
      new features, explain functioning/predictions to superiors
    </li>
  </ul>
  <p>
    In order to make explainability accessible to people with low technical
    skills, first of all, the creator: a data scientist/developer must be
    comfortable with the tools of explainability.
  </p>
  <p>
    The data scientist will use them above all to understand and improve his
    model and then to communicate with his superiors and regulatory bodies.<br />Recently,
    explainability tools have become more and more accessible.
  </p>
  <p>
    For example,
    <a
      href="https://www.dataiku.com/"
      data-href="https://www.dataiku.com/"
      class="markup--anchor markup--p-anchor"
      rel="noopener"
      target="_blank"
      >Dataiku </a
    >— ML’s platform — has added in its latest version 7.0 published on March 2,
    2020 explainability tools: Shapley values and “The Individual Conditional
    Expectation” (ICE).
  </p>
  <span class="image fit"
    ><img src="/images/dataiku.png" alt="" /><em
      >Dataiku prediction studio</em
    ></span
  >
  <p>
    <a href="https://azure.microsoft.com/en-us/services/machine-learning/"
      >Azure ML</a
    >
    proposes its own version of Shap and alternative tools adding interactive
    <a
      href="https://docs.microsoft.com/en-us/azure/machine-learning/how-to-machine-learning-interpretability-aml"
      >dashboards</a
    >.
  </p>

  <span class="image fit"
    ><img src="/images/azure.png" alt="" /><em
      >Azure ML interpretability dashboard</em
    ></span
  >

  <p>
    There are also open-source webapps such as this one described in the medium
    article [4] that facilitate the exploration of the SHAP library.
  </p>
  <div>
    <a
      href="https://towardsdatascience.com/understand-the-machine-learning-blackbox-with-ml-interpreter-7b0f9a2d8e9f"
      ><strong
        >Understand the machine learning Blackbox with ML interpreter</strong
      ><br /><em class="markup--em markup--mixtapeEmbed-em"
        >There are dangers in having models running the world and making
        decisions from hiring to criminal justice</em
      >towardsdatascience.com</a
    ><a
      href="https://towardsdatascience.com/understand-the-machine-learning-blackbox-with-ml-interpreter-7b0f9a2d8e9f"
    ></a>
  </div>
  <div><br /><br /></div>
  <p>
    These tools, very interesting to get a quick overview of interpretation, do
    not necessarily give an understanding of the full potential of the SHAP
    library. Few allow to explore interaction values or to use different
    background or display sets.
  </p>
  <p>
    I investigated the SHAP framework and I present you my remarks and the usage
    of less known features, available in the official version of the library in
    open source. I also propose some interactive visualizations easy to
    integrate in your projects.
  </p>
  <h3>Step-by-Step Guide: Using SHAP for Machine Learning Models</h3>
  <p>
    Most data scientists have already heard of the SHAP framework.<br />In this
    post, we won’t explain in detail how the calculations behind the library are
    done. Many resources are available online such as the SHAP documentation
    [5], publications by authors of the library [6,7], the great book
    “Interpretable Machine Learning” [8] and multiple medium articles [9,10,11].
  </p>
  <p>
    In summary, Shapley’s values calculate the importance of a feature by
    comparing what a model predicts with and without this feature. However,
    since the order in which a model sees the features can affect its
    predictions, this is done in all possible ways, so that the features are
    compared fairly. This approach is inspired by game theory.
  </p>
  <p>
    Having worked with many clients, for example in the banking and insurance
    sectors, one can see that their data scientists are struggling to exploit
    the full potential of SHAP. They don’t know how this tool could really be
    useful for understanding a model and how to use it to go beyond simply
    extracting the importance of features.
  </p>
  <blockquote>The devil is in the detail</blockquote>
  <p>
    SHAP comes with a set of visualizations that are quite complex and not
    always intuitive, even for a data scientist.
  </p>
  <p>
    On top of that, there are several technical nuances to be able to use SHAP
    with your data. Francesco Porchetti’s blog article [12] expresses some of
    these frustrations by exploring the SHAP,
    <a href="https://github.com/marcotcr/lime">LIME</a>,
    <a
      href="https://github.com/SauceCat/PDPbox"
      data-href="https://github.com/SauceCat/PDPbox"
      >PDPbox </a
    >(PDP and ICE) and
    <a
      href="https://eli5.readthedocs.io/en/latest/autodocs/sklearn.html#module-eli5.sklearn.permutation_importance"
      >ELI5 </a
    >libraries.
  </p>
  <p>
    <strong
      ><em
        >At Saegus, I worked on a course which aims to give more clarity to the
        SHAP framework and to facilitate the use of this tool.</em
      ></strong
    >
  </p>
  <p>
    In this post I would like to share with you some observations collected
    during that process.
  </p>
  <p>
    SHAP is used to explain an existing model. Taking a binary classification
    case built with a sklearn model. We train, tune and test our model. Then we
    can use our data and the model to create an additional SHAP model that
    explains our classification model.
  </p>
  <span class="image fit"
    ><img
      src="/images/shap.png"
      alt="SHAP plot demonstrating deep learning AI model interpretability in data science."
    /><em>Image source: SHAP github</em></span
  >
  <h4>Vocabulary</h4>
  <p>
    It is important to understand all the bricks that make up a SHAP
    explanation.
  </p>
  <p>
    Often, by using default values for parameters, the complexity of the choices
    we make remains obscure.
  </p>
  <p>
    <strong>global explanations<br /></strong>explanations of how the model
    works from a general point of view
  </p>
  <p>
    <strong>local explanations</strong><br />explanations of the model for a
    sample (a data point)
  </p>
  <p>
    <strong>explainer </strong>(shap.explainer_type(params))<br />type of
    explainability algorithm to be chosen according to the model used.
  </p>
  <p>
    The parameters are different for each type of model. Usually, the model and
    training data must be provided, at a minimum.
  </p>
  <p>
    <strong>base value</strong> (explainer.expected_value)<br /><em
      >E(y_hat)</em
    >
    is “the value that would be predicted if we didn’t know any features of the
    current output” is the <em>mean(y_hat)</em> prediction for the training data
    set or the background set. We can call it “reference value”, it’s a scalar
    (<em>n</em>).
  </p>
  <p>
    It’s important to choose your background set carefully — if we have the
    unbalanced training set this will result in a base value placed among the
    majority of samples. This can also be a desired effect: for example if for a
    bank loan we want to answer the question: “how is the customer in question
    different from customers who have been approved for the loan” or “how is my
    false positive different from the true positives”.
  </p>
  <figure class="highlight"><pre><code class="language-python" data-lang="python"> <span class="c1"># equilibrated case background = X.sample(1000) #X is
</span>  <span class="n">equilibrated</span> <span class="c1"># background used in explainer defines base value explainer =
</span>  <span class="n">shap</span><span class="p">.</span><span class="nc">TreeExplainer</span><span class="p">(</span><span class="n">xgb_model</span><span class="p">,</span><span class="n">background</span><span class="p">,</span><span class="n">model_output</span><span class="o">=</span><span class="sh">"</span><span class="s">raw</span><span class="sh">"</span> <span class="p">)</span> <span class="n">shap_values</span> <span class="o">=</span>
  <span class="n">explainer</span><span class="p">.</span><span class="nf">shap_values</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="c1"># background used in the plot, the points that are
</span>  <span class="n">visible</span> <span class="n">on</span> <span class="n">the</span> <span class="n">plot</span> <span class="n">shap</span><span class="p">.</span><span class="nf">summary_plot</span><span class="p">(</span><span class="n">shap_values</span><span class="p">,</span><span class="n">background</span><span class="p">,</span>
  <span class="n">feature_names</span><span class="o">=</span><span class="n">background</span><span class="p">.</span><span class="n">columns</span><span class="p">)</span> </code></pre></figure> <figure class="highlight"><pre><code class="language-python" data-lang="python"> <span class="c1">#
</span>  <span class="n">base</span> <span class="n">value</span> <span class="n">shifted</span> <span class="n">class1</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">class1</span><span class="p">,:]</span> <span class="c1">#X is equilibrated # background
</span>  <span class="k">from</span> <span class="k">class</span> <span class="err">1 </span><span class="nc">is</span> <span class="n">used</span> <span class="ow">in</span> <span class="n">explainer</span> <span class="n">defines</span> <span class="n">base</span> <span class="n">value</span> <span class="n">explainer</span> <span class="o">=</span>
  <span class="n">shap</span><span class="p">.</span><span class="nc">TreeExplainer</span><span class="p">(</span><span class="n">xgb_model</span><span class="p">,</span><span class="n">class1</span><span class="p">,</span><span class="n">model_output</span><span class="o">=</span><span class="sh">"</span><span class="s">raw</span><span class="sh">"</span> <span class="p">)</span> <span class="n">shap_values</span> <span class="o">=</span>
  <span class="n">explainer</span><span class="p">.</span><span class="nf">shap_values</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="c1"># points from class 0 is used in the plot, the points
</span>  <span class="n">that</span> <span class="n">are</span> <span class="n">visible</span> <span class="n">on</span> <span class="n">the</span> <span class="n">plot</span> <span class="n">shap</span><span class="p">.</span><span class="nf">summary_plot</span><span class="p">(</span><span class="n">shap_values</span><span class="p">,</span><span class="n">X</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">class0</span><span class="p">,:],</span>
  <span class="n">feature_names</span><span class="o">=</span><span class="n">X</span><span class="p">.</span><span class="n">columns</span><span class="p">)</span> </code></pre></figure>

  <span class="image fit"
    ><img src="/images/background-shap.png" alt="" /><em
      >Selecting the background dataset changes the question answered by
      shap values.</em
    ></span
  >
  <p>
    <strong>SHAPley values</strong> (explainer.shap_values(x))<br />the average
    contribution of each feature to each prediction for each sample based on all
    possible features. It is a (<em>n,m</em>) <em>n </em>— samples,
    <em>m </em>— features matrix that represents the contribution of each
    feature to each sample.
  </p>
  <p>
    <strong>output value</strong> (for a sample)<br />the value predicted by the
    algorithm (the probability, logit or raw output values of the model)
  </p>
  <p>
    <strong>display features</strong> (<em>n </em>x <em>m</em>)<br />a matrix of
    original values — before transformation/encoding/engineering of features
    etc. — that can be provided to some graphs to improve interpretation. Often
    overlooked and essential for interpretation.
  </p>
  <p>____</p>
  <p><strong>SHAPley values</strong></p>
  <p>
    Shapley values remain the central element. Once we realize that this is
    simply a matrix with the same dimensions as our input data and that we can
    analyze it in different ways to explain the model and not only. We can
    reduce its dimensions, we can cluster it, we can use it to create new
    features. An interesting exploration described in the article [12] aims at
    improving anomaly detection using auto encoders and SHAP. The SHAP library
    proposes a rich but not exchaustive exploration through visualizations.
  </p>
  <h4>Visualizing SHAP: Enhancing Interpretability in Deep Learning Models</h4>
  <p>
    The SHAP library offers different visualizations. A good explanation on how
    to read the colors of the summary plot can be found in this medium article
    [14].
  </p>
  <span class="image fit"
    ><img src="/images/shap-global.png" alt="" /><em
      >A summary of graphical visualizations to analyze global explanations</em
    ></span
  >
  <p>
    <strong>The summary plot</strong> shows the most important features and the
    magnitude of their impact on the model. It can take several graphical forms
    and for the models explained by TreeExplainer we can also observe the<strong
      ><em> interaction values </em></strong
    >using the “compact dot” with shap_interaction_values in input.
  </p>
  <p>
    <strong>The dependency plot</strong> allows to analyze the features two by
    two by suggesting a possibility to observe the interactions. The scatter
    plot represents a dependency between a feature(x) and the shapley values (y)
    colored by a second feature(hue).
  </p>
  <p>
    On a personal note, I find that an observation of a three-factor
    relationship at the same time is not intuitive for the human brain (at least
    mine). I also doubt that an observation of dependency by observing colours
    can be scientifically accurate. Shap can give us an interaction relationship
    that is calculated as a correlation between the shapley values of the first
    feature and the values of the second feature. If possible (for
    TreeExplainer) it makes more sense to use the shapley interaction values to
    observe interactions.
  </p>
  <figure>
    <script src="https://gist.github.com/UrszulaCzerwinska/b7853f5f0b209f2f89e2a3590dd6f329.js"></script>
    <figcaption class="imageCaption">
      Snippet code to reproduce my dependence plot variant.
    </figcaption>
  </figure>
  <p>
    I propose an interactive variant of dependency plot that allows to observe
    the relationship between a feature(x), the shapley values (y) and the
    prediction (histogram colors). What seems important to me in this version is
    the possibility to display on the graph the original values (Income in k
    USD) instead of the normalized space used by the model.
  </p>
  <span class="image fit"
    ><img
      src="/images/my-dep-plot.png"
      alt="Histogram visualizing SHAP interaction values in a data science model analysis."
    /><em>My variant of dependence plot</em></span
  >
  <span class="image fit"
    ><img
      src="/images/shap-local.png"
      alt="Histogram visualizing SHAP interaction values in a data science model analysis."
    /><em
      >a summary of graphical visualizations to analyze local explanations</em
    ></span
  >
  <p>
    There are three alternatives for the visualization of explanations of a
    sample: force plot, decision plot and waterfall plot.
  </p>
  <p>
    For a sample, these three representations are redundant, they represent the
    information in a very similar way. At the same time, some elements of these
    graphs are complementary. By putting the three side by side, I have the
    impression to understand the result in a more intuitive way. The force plot
    is good to see where the “output value” fits in relation to the “base
    value”. We also see which features have a positive (red) or negative (blue)
    impact on the prediction and the magnitude of this impact. The water plot
    also allows us to see the amplitude and the nature of the impact of a
    feature with its quantification. It also allows to see the order of
    importance of the features and the values taken by each feature for the
    studied sample. The Decision plot makes it possible to observe the amplitude
    of each change, “a trajectory” taken by a samplefor the values of the
    displayed features.
  </p>
  <p>
    By using force plot and decision plot we can represent several samples at
    the same time.
  </p>
  <p>
    The force plot for a set of samples can be compared to the last level of a
    dendrogram. The samples are grouped by similarity or by selected feature. In
    my opinion, this graph is difficult to read for a random sample. It is much
    more meaningful if we represent the contrasting cases or with a hypothesis
    behind.
  </p>
  <p>
    The decision plot, for a set of samples, quickly becomes cumbersome if we
    select too many samples. It is very useful to observe a ‘trajectory
    deviation’ or ‘diverging/converging trajectories’ of a limited group of
    samples.
  </p>
  <h4>
    Explainers in SHAP: Understanding Different Model Interpretability
    Approaches
  </h4>
  <p>
    Explainers are the models used to calculate shapley values. The diagram
    below shows different types of Explainers.
  </p>
  <p>
    The choice of Explainers depends mainly on the selected learning model. For
    linear models, the “Linear Explainer” is used, for decision trees and “set”
    type models — “TreeExplainer”. “Kernel Explainer” is slower than the above
    mentioned explainers.
  </p>
  <p>
    In addition the “Tree Explainer” allows to display the interaction values
    (see next section). It also allows to transform the model output into
    probabilities or logloss, which is useful for a better understanding of the
    model or to compare several models.
  </p>
  <p>
    The Kernel Explainer creates a model that substitutes the closest to our
    model. Kernel Explainer can be used to explain neural networks. For deep
    learning models, there are the Deep Explainer and the Grandient Explainer.
    For this paper we have not investigated the explainability of neural
    networks.
  </p>
  <span class="image fit"
    ><img
      src="/images/explainer.png"
      alt="Force plot visualization of SHAP values enhancing transparency in AI model predictions; Waterfall plot depicting SHAP value contributions to machine learning predictions."
    /><em>a summary of Explainer types in the SHAP library</em></span
  >
  <h4>Shapley values of interactions</h4>
  <p>
    One of the properties that allows to go further in the analysis of a model
    that can be explained with the “Tree Explainer” is the calculation of
    shapley values of interactions.
  </p>
  <p>
    These values make it possible to quantify the impact of an interaction
    between two features on the prediction for each sample. As the matrix of
    shapley values has two dimensions (samples x features), the interactions are
    a tensor with three dimensions (samples x features x features).
  </p>
  <span class="image fit"
    ><img
      src="/images/interactions-1.png"
      alt="Interactive SHAP dependence plot showing feature impact in deep learning models."
    /><em
      >a summary of graphical visualizations to analyze local explanations</em
    ></span
  >
  <span class="image fit"
    ><img
      src="/images/interactions-2.png"
      alt="Interactive SHAP dependence plot showing feature impact in deep learning models."
    /><em
      >a summary of graphical visualizations to analyze local explanations</em
    ></span
  >
  <span class="image fit"
    ><img
      src="/images/interactions-3.png"
      alt="Interactive SHAP dependence plot showing feature impact in deep learning models."
    /><em
      >a summary of graphical visualizations to analyze local explanations</em
    ></span
  >

  <p>
    <strong
      >Here’s how interaction values help interpret a binary classification
      model.</strong
    >
  </p>
  <p>
    I used a Kaggle [15] dataset that represents a client base and the binary
    dependent feature: did the client accept the personal loan? NO/YES (0/1).
  </p>
  <span class="image fit"
    ><img
      src="/images/table.png"
      alt="A summary of SHAP graphical
      visualizations to analyze local explanations of machine learning model"
    /><em
      >a summary of graphical visualizations to analyze local explanations</em
    ></span
  >

  <p>
    I’ve trained several models, including an xgboost model that we treated with
    the Tree Explainer.
  </p>
  <span class="image fit"
    ><img
      src="/images/sklearn-model.png"
      alt="A summary of SHAP graphical
        visualizations to analyze local explanations of machine learning model"
    /><em
      >a summary of graphical visualizations to analyze local explanations</em
    ></span
  >

  <p>The background dataset was balanced and represented 40% of the dataset.</p>
  <figure class="highlight"><pre><code class="language-python" data-lang="python"> <span class="c1"># xgb - traned model # X_background - background
</span>  <span class="n">dataset</span> <span class="n">explainer_raw</span> <span class="o">=</span> <span class="n">shap</span><span class="p">.</span><span class="nc">TreeExplainer</span><span class="p">(</span><span class="n">xgb</span><span class="p">,</span><span class="n">X_background</span><span class="p">,</span>
  <span class="n">model_output</span><span class="o">=</span><span class="sh">"</span><span class="s">raw</span><span class="sh">"</span><span class="p">,</span> <span class="n">feature_perturbation</span><span class="o">=</span><span class="sh">"</span><span class="s">tree_path_dependent</span><span class="sh">"</span> <span class="p">)</span> <span class="c1"># project
</span>  <span class="n">data</span> <span class="n">point</span> <span class="n">of</span> <span class="n">background</span> <span class="n">datasetshap_values</span> <span class="o">=</span>
  <span class="n">explainer_raw</span><span class="p">.</span><span class="nf">shap_values</span><span class="p">(</span><span class="n">X_background</span><span class="p">)</span> <span class="c1"># obtain interaction values
</span>  <span class="n">shap_interaction_values</span> <span class="o">=</span> <span class="n">explainer_raw</span><span class="p">.</span><span class="nf">shap_interaction_values</span><span class="p">(</span><span class="n">X_background</span><span class="p">)</span>
  <span class="c1"># dimensions shap_values.shape &gt;&gt;&gt;(2543, 16) shap_interaction_values.shape
</span>  <span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="mi">2543</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span> <span class="n">shap</span><span class="p">.</span><span class="nf">summary_plot</span><span class="p">(</span><span class="n">shap_interaction_values</span><span class="p">,</span> <span class="n">X_background</span><span class="p">,</span>
  <span class="n">plot_type</span><span class="o">=</span><span class="sh">"</span><span class="s">compact_dot</span><span class="sh">"</span><span class="p">)</span> </code></pre></figure>

  <span class="image fit"
    ><img
      src="/images/summary-plot.png"
      alt="Summary plot showcasing SHAP values and feature interactions in machine learning models."
    /><em>Summary plot with interactions</em></span
  >

  <p>To better explore interactions, a heatmap can be very useful.</p>
  <span class="image fit"
    ><img
      src="/images/histogram.png"
      alt="Heatmap illustrating SHAP values and feature interactions in machine learning models."
    /><em>Histogram of interaction values</em></span
  >
  <p>
    In the Summary_plot one can observe the importance of features and the
    importance the interactions. The interactions appear in double which
    confuses a little the reading.
  </p>
  <p>
    In the histogram, we observe directly the interactions. The strongests of
    them of being: Income-Education, Income — Family, Income — CCAvg and
    Family-Education, Income-Age.
  </p>
  <p>
    Then I investigated the interactions two by two.<br />To understand the
    difference between a dependency_plot and a dependency_plot of interactions
    here are the two:
  </p>

  <figure class="highlight"><pre><code class="language-python" data-lang="python"> <span class="c1"># dependence_plot classique shap.dependence_plot("Age",
</span>  <span class="n">shap_values</span><span class="p">,</span> <span class="n">features</span><span class="o">=</span> <span class="n">X_background</span><span class="p">,</span> <span class="n">display_features</span><span class="o">=</span><span class="n">X_background_display</span><span class="p">,</span>
  <span class="n">interaction_index</span><span class="o">=</span><span class="sh">"</span><span class="s">Income</span><span class="sh">"</span><span class="p">)</span> </code></pre></figure>

  <span class="image fit"
    ><img
      src="/images/dep-plot.png"
      alt="SHAP dependency plot illustrating interaction effects in machine learning algorithm"
  /></span>
  <figure class="highlight"><pre><code class="language-python" data-lang="python"> <span class="c1"># dependence_plot des interactions
</span>  <span class="n">shap</span><span class="p">.</span><span class="nf">dependence_plot</span><span class="p">((</span><span class="sh">"</span><span class="s">Age</span><span class="sh">"</span><span class="p">,</span><span class="sh">"</span><span class="s">Income</span><span class="sh">"</span><span class="p">),</span> <span class="n">shap_interaction_values</span><span class="p">,</span> <span class="n">features</span><span class="o">=</span>
  <span class="n">X_background</span><span class="p">,</span><span class="n">display_features</span><span class="o">=</span><span class="n">X_background_display</span><span class="p">)</span> </code></pre></figure>
  <span class="image fit"
    ><img
      src="/images/interaction-dep-plot.png"
      alt="SHAP dependency plot illustrating interaction effects in machine learning algorithm"
  /></span>

  <p>
    Even when using the ‘display_features’ parameter, the Age and Income values
    are displayed in the transformed space.
  </p>
  <p>
    For this reasons I offer an interactive version, which displays the
    non-transformed values.
  </p>
  <span class="image fit"
    ><img
      src="/images/interactive-dep-plot.png"
      alt="SHAP dependency plot illustrating interaction effects in machine learning algorithm"
  /></span>

  <p>And here is the code to reproduce this plot:</p>
  <figure>
    <script src="https://gist.github.com/UrszulaCzerwinska/b78e85c6e1795d949321209cbe41587a.js"></script>
  </figure>
  <p>Here we have the strongest interactions:</p>
  <p>Income — Education</p>
  <span class="image fit"
    ><img
      src="/images/income-education.png"
      alt="Shapley interaction plot for better visualization of features interaction in machine learning model"
  /></span>

  <p>
    In this graph, we notice that with an Education level 1 (undergrad), low
    income (under 100 k USD) is an encouraging factor to take a credit, and high
    income (over 120 k USD) is an inhibiting interaction.<br />For individuals
    with Education 2 &amp; 3 (graduated &amp; advanced/professional), the
    interaction effect is slightly lower and opposite to that for Education ==
    1.
  </p>
  <span class="image fit"
    ><img
      src="/images/income-family.png"
      alt="Shapley interaction plot for better visualization of features interaction in machine learning model"
  /></span>

  <p>
    For the features “Family” and “number of people” in the household, the
    interaction is positive when income is low (below USD 100k) and the family
    has 1–2 members. For higher incomes (&gt; 120 k USD), for family with 1–2
    members has a negative effect. The opposite is true for families of 3–4
    people.
  </p>
  <span class="image fit"
    ><img
      src="/images/income-ccavg.png"
      alt="Shapley interaction plot for better visualization of features interaction in machine learning model"
  /></span>

  <p>
    The interaction between income and credit card average spending is more
    complex. For low income (&lt;100 k USD) and low CCAvg (&lt;4 k USD) the
    interaction has a negative effect, for income between 50 and 110 k USD and
    CCAvg 2–6 k USD the effect is strongly positive, this could define a
    potential target for credit canvassing along these two axes. For high
    incomes (&gt; 120 k USD), the low CCAvg has a positive impact on the
    prediction of class 1, high CCAvg has a small negative effect on the
    predictions, the medium CCAvg has a stronger negative impact.
  </p>
  <span class="image fit"
    ><img
      src="/images/family-education.png"
      alt="Shapley interaction plot for better visualization of features interaction in machine learning model"
  /></span>
  <p>
    The interaction between two features is a little less readible. For a family
    of 1 and 2 members with “undergrad” education, the interaction has a
    negative impact. For a family of 3–4 members the effect is the opposite.
  </p>
  <span class="image fit"
    ><img
      src="/images/income-age.png"
      alt="Shapley interaction plot for better visualization of features interaction in machine learning model"
  /></span>

  <p>
    For low incomes (&lt; 70k USD), impact changes linearly with age, the higher
    the age, the more the impact varies positively. For high incomes (&gt;120k
    USD), the interaction impact is lower, at middle age (~40 years) the impact
    is slightly positive, at low age the impact is negative and for age &gt;45
    the impact is neutral.
  </p>
  <p>
    These findings would be more complicated to interpret if the values of the
    features had not corresponded to original values. For instance, speaking of
    age or income in negative in units. Therefore, representing explanations in
    an understandable dimension facilitates interpretation.
  </p>
  <h4>Comparing Models: How SHAP Improves Machine Learning Interpretability</h4>
  <p>
    In some situations, we may want to compare the predictions of different
    models for the same samples. Understand why one model classifies the sample
    correctly and the other one does not.<br />To start, we can display the
    summary plots for each model, look at the importance of features and the
    shapley value distributions. This gives a first general idea.
  </p>
  <span class="image fit"
    ><img
      src="/images/tree-explainer.png"
      alt="SHAP tree explainer illustrating how to compare the predictions of different models for the same samples"
  /></span>

  <p>
    Decision plot allows to compare on the same graph the predictions of
    different models for the same sample.<br />You just have to create an object
    that simulates multiclass classification.
  </p>
  <figure class="highlight"><pre><code class="language-python" data-lang="python"> <span class="c1"># we have tree models : xgb, gbt, rf # for each model
</span>  <span class="n">we</span> <span class="n">get</span> <span class="n">an</span> <span class="nf">explainer</span><span class="p">(</span><span class="o">*</span><span class="n">_explainer</span><span class="p">),</span> <span class="nf">probabilities </span><span class="p">(</span><span class="o">*</span><span class="n">_probs</span><span class="p">),</span> <span class="n">predictions</span>
  <span class="p">(</span><span class="o">*</span><span class="n">_pred</span><span class="p">)</span> <span class="ow">and</span> <span class="n">shapley</span> <span class="nf">values</span><span class="p">(</span><span class="o">*</span><span class="n">_values</span><span class="p">)</span> <span class="c1">#xgb xgb_explainer =
</span>  <span class="n">shap</span><span class="p">.</span><span class="nc">TreeExplainer</span><span class="p">(</span><span class="n">xgb</span><span class="p">,</span> <span class="n">X_background</span><span class="p">,</span> <span class="n">model_output</span><span class="o">=</span><span class="sh">"</span><span class="s">probability</span><span class="sh">"</span><span class="p">,</span>
  <span class="n">feature_perturbation</span><span class="o">=</span><span class="sh">"</span><span class="s">interventional</span><span class="sh">"</span><span class="p">)</span> <span class="n">xgb_values</span> <span class="o">=</span>
  <span class="n">xgb_explainer</span><span class="p">.</span><span class="nf">shap_values</span><span class="p">(</span><span class="n">X_background</span><span class="p">)</span> <span class="n">xgb_probs</span> <span class="o">=</span>
  <span class="n">xgb</span><span class="p">.</span><span class="nf">predict_proba</span><span class="p">(</span><span class="n">pipeline_trans</span><span class="p">.</span><span class="nf">transform</span><span class="p">(</span><span class="n">x_background</span><span class="p">))</span> <span class="n">xgb_pred</span> <span class="o">=</span>
  <span class="n">xgb</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">pipeline_trans</span><span class="p">.</span><span class="nf">transform</span><span class="p">(</span><span class="n">x_background</span><span class="p">))</span> <span class="c1"># rf rf_explainer =
</span>  <span class="n">shap</span><span class="p">.</span><span class="nc">TreeExplainer</span><span class="p">(</span><span class="n">rf</span><span class="p">,</span> <span class="n">X_background</span><span class="p">,</span> <span class="n">model_output</span><span class="o">=</span><span class="sh">"</span><span class="s">probability</span><span class="sh">"</span><span class="p">,</span>
  <span class="n">feature_perturbation</span><span class="o">=</span><span class="sh">"</span><span class="s">interventional</span><span class="sh">"</span><span class="p">)</span> <span class="n">rf_values</span> <span class="o">=</span>
  <span class="n">rf_explainer</span><span class="p">.</span><span class="nf">shap_values</span><span class="p">(</span><span class="n">X_background</span><span class="p">)</span> <span class="n">rf_probs</span> <span class="o">=</span>
  <span class="n">rf</span><span class="p">.</span><span class="nf">predict_proba</span><span class="p">(</span><span class="n">pipeline_trans</span><span class="p">.</span><span class="nf">transform</span><span class="p">(</span><span class="n">x_background</span><span class="p">))</span> <span class="n">rf_pred</span> <span class="o">=</span>
  <span class="n">rf</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">pipeline_trans</span><span class="p">.</span><span class="nf">transform</span><span class="p">(</span><span class="n">x_background</span><span class="p">))</span> <span class="c1"># gbt gbt_explainer =
</span>  <span class="n">shap</span><span class="p">.</span><span class="nc">TreeExplainer</span><span class="p">(</span><span class="n">gbt</span><span class="p">,</span> <span class="n">X_background</span><span class="p">,</span> <span class="n">model_output</span><span class="o">=</span><span class="sh">"</span><span class="s">probability</span><span class="sh">"</span><span class="p">,</span>
  <span class="n">feature_perturbation</span><span class="o">=</span><span class="sh">"</span><span class="s">interventional</span><span class="sh">"</span><span class="p">)</span> <span class="n">gbt_values</span> <span class="o">=</span>
  <span class="n">gbt_explainer</span><span class="p">.</span><span class="nf">shap_values</span><span class="p">(</span><span class="n">X_background</span><span class="p">)</span> <span class="n">gbt_probs</span> <span class="o">=</span>
  <span class="n">gbt</span><span class="p">.</span><span class="nf">predict_proba</span><span class="p">(</span><span class="n">pipeline_trans</span><span class="p">.</span><span class="nf">transform</span><span class="p">(</span><span class="n">x_background</span><span class="p">))</span> <span class="n">gbt_pred</span> <span class="o">=</span>
  <span class="n">gbt</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">pipeline_trans</span><span class="p">.</span><span class="nf">transform</span><span class="p">(</span><span class="n">x_background</span><span class="p">))</span> <span class="c1">######## # we make a list
</span>  <span class="n">of</span> <span class="n">model</span> <span class="n">explaners</span> <span class="n">base_values</span> <span class="o">=</span> <span class="p">[</span><span class="n">xgb_explainer</span><span class="p">.</span><span class="n">expected_value</span><span class="p">,</span>
  <span class="n">rf_explainer</span><span class="p">.</span><span class="n">expected_value</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">gbt_explainer</span><span class="p">.</span><span class="n">expected_value</span><span class="p">]</span> <span class="n">shap_values</span> <span class="o">=</span>
  <span class="p">[</span><span class="n">xgb_values</span><span class="p">,</span> <span class="n">rf_values</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">gbt_values</span><span class="p">]</span> <span class="n">predictions</span> <span class="o">=</span>
  <span class="p">[</span><span class="n">xgb_probs</span><span class="p">,</span><span class="n">rf_probs</span><span class="p">,</span><span class="n">gbt_probs</span><span class="p">]</span> <span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="sh">"</span><span class="s">xgb</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">rf</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">gbt</span><span class="sh">"</span><span class="p">]</span> <span class="c1"># index of a
</span>  <span class="n">sample</span> <span class="c1"># Plot idx = 100 shap.multioutput_decision_plot(base_values,
</span>  <span class="n">shap_values</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span>
  <span class="n">feature_names</span><span class="o">=</span><span class="n">X_background</span><span class="p">.</span><span class="n">columns</span><span class="p">.</span><span class="nf">to_list</span><span class="p">(),</span><span class="n">legend_labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span>
  <span class="n">legend_location</span><span class="o">=</span><span class="sh">'</span><span class="s">lower right</span><span class="sh">'</span><span class="p">)</span> </code></pre></figure>

  <span class="image fit"
    ><img
      src="/images/model-comparison.png"
      alt="Comparative analysis of machine learning models using SHAP values for data science insights."
  /></span>

  <p>
    The only difficulty consists in checking the dimensions of shapley values
    because for some models, shapley values are calculated for each class, in
    the case of the binary classification (class 0 and 1), while for others we
    obtain a single matrix which corresponds to class 1. In our example, we
    select a second matrix (index 1) for random forest.
  </p>
  <h4>
    Running Simulations with SHAP: Enhancing Model Predictions in Data Science
  </h4>
  <p>
    By default SHAP does not contain functions that make it easier to answer the
    “What if?” question. “What if I could earn an extra 10K USD a year, would my
    credit be extended?”<br />Nevertheless, it is possible to run the
    simulations by varying a feature and calculating hypothetical shapley
    values.
  </p>
  <figure class="highlight"><pre><code class="language-python" data-lang="python"> <span class="n">explainer_margin_i</span> <span class="o">=</span>
  <span class="n">shap</span><span class="p">.</span><span class="nc">TreeExplainer</span><span class="p">(</span><span class="n">xgb</span><span class="p">,</span><span class="n">X_background</span><span class="p">,</span> <span class="n">model_output</span><span class="o">=</span><span class="sh">"</span><span class="s">raw</span><span class="sh">"</span><span class="p">,</span>
  <span class="n">feature_perturbation</span><span class="o">=</span><span class="sh">"</span><span class="s">interventional</span><span class="sh">"</span><span class="p">)</span> <span class="n">idx</span> <span class="o">=</span> <span class="mi">100</span> <span class="n">rg</span> <span class="o">=</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">202</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span> <span class="n">r</span><span class="p">,</span> <span class="n">R</span><span class="p">,</span>
  <span class="n">hypothetical_shap_values</span><span class="p">,</span> <span class="n">hypothetical_predictions</span><span class="p">,</span> <span class="n">y_r</span> <span class="o">=</span>
  <span class="nf">simulate_with_shap</span><span class="p">(</span><span class="n">x_background</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="sh">"</span><span class="s">Income</span><span class="sh">"</span><span class="p">,</span> <span class="n">rg</span><span class="p">,</span> <span class="n">explainer_margin_i</span><span class="p">,</span>
  <span class="n">pipeline_trans</span><span class="o">=</span><span class="n">pipeline_trans</span><span class="p">)</span> </code></pre></figure>
  <p>
    I created a `simulate_with_shap` function that simulates different values of
    the feature and calculates the hypothetical shapley values.
  </p>
  <figure>
    <script src="https://gist.github.com/UrszulaCzerwinska/c324a8ec8e37ebe4758f4affe52456be.js"></script>
  </figure>
  <span class="image fit"
    ><img
      src="/images/simulation.png"
      alt="The SHAP simulation allows to see how we could change the prediction using new values and what the shapley values would be for these"
  /></span>

  <p>
    This simulation allows us to see for the selected sample, if we freeze all
    the features apart from Income, how we could change the prediction and what
    the shapley values would be for these new values.
  </p>
  <p>
    It is possible to simulate the changes ‘feature by feature’, it would be
    interesting to be able to make several changes simultaneously.
  </p>
  <h3>
    Future Trends in Data Science: The Growing Role of SHAP and Interpretability
  </h3>
  <p>
    AI algorithms are taking up more and more space in our lives. The
    explanability of predictions is an important topic for data scientists,
    decision-makers and individuals who are impacted by predictions.
  </p>
  <p>
    Several frameworks have been proposed in order to transform non-explainable
    models into explainable ones. One of the best known and most widely used
    frameworks is SHAP.
  </p>
  <p>
    Despite very good documentation, it is not clear how to exploit all its
    features in depth.
  </p>
  <p>
    I have proposed some simple graphical enhancements and tried to demonstrate
    the usefulness of less known and not understood features in most standard
    uses of SHAP.
  </p>
  <h3>Acknowledgements</h3>
  <p>
    I would like to thank the Saegus DATA team who participated in this work
    with good advice, in particular Manager Fréderic Brajon and Senior
    Consultant Manager Clément Moutard.
  </p>
  <h3>Bibliography</h3>
  <p>
    [1] Stop Explaining Black Box Machine Learning Models for High Stakes
    Decisions and Use Interpretable Models Instead; Cynthia Rudin
    <a
      href="https://arxiv.org/pdf/1811.10154.pdf"
      data-href="https://arxiv.org/pdf/1811.10154.pdf"
      target="_blank"
      >https://arxiv.org/pdf/1811.10154.pdf</a
    >
  </p>
  <p>
    [2] Explainable Artificial Intelligence (XAI): Concepts, Taxonomies,
    Opportunities and Challenges toward Responsible AI; Arrietaa et al.
    <a
      href="https://arxiv.org/pdf/1910.10045.pdf"
      data-href="https://arxiv.org/pdf/1910.10045.pdf"
      target="_blank"
      >https://arxiv.org/pdf/1910.10045.pdf</a
    >
  </p>
  <p>
    [3] Cloudera Fast Forward Interpretability:
    <a
      href="https://ff06-2020.fastforwardlabs.com/?utm_campaign=Data_Elixir&amp;utm_source=Data_Elixir_282"
      data-href="https://ff06-2020.fastforwardlabs.com/?utm_campaign=Data_Elixir&amp;utm_source=Data_Elixir_282"
      target="_blank"
      >https://ff06-2020.fastforwardlabs.com/?utm_campaign=Data_Elixir&amp;utm_source=Data_Elixir_282</a
    >
  </p>
  <p>
    [4]
    <a
      href="https://towardsdatascience.com/understand-the-machine-learning-blackbox-with-ml-interpreter-7b0f9a2d8e9f"
      data-href="https://towardsdatascience.com/understand-the-machine-learning-blackbox-with-ml-interpreter-7b0f9a2d8e9f"
      target="_blank"
      >https://towardsdatascience.com/understand-the-machine-learning-blackbox-with-ml-interpreter-7b0f9a2d8e9f</a
    >
  </p>
  <p>
    [5]
    <a
      href="https://github.com/slundberg/shap"
      data-href="https://github.com/slundberg/shap"
      target="_blank"
      >https://github.com/slundberg/shap</a
    >
  </p>
  <p>
    [6]
    <a
      href="http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions"
      data-href="http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions"
      target="_blank"
      >http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions</a
    >
  </p>
  <p>
    [7]
    <a
      href="https://www.nature.com/articles/s42256-019-0138-9"
      data-href="https://www.nature.com/articles/s42256-019-0138-9"
      target="_blank"
      >https://www.nature.com/articles/s42256-019-0138-9</a
    >
  </p>
  <p>
    [8]
    <a
      href="https://christophm.github.io/interpretable-ml-book/"
      data-href="https://christophm.github.io/interpretable-ml-book/"
      target="_blank"
      >https://christophm.github.io/interpretable-ml-book/</a
    >
  </p>
  <p>
    [9]
    <a
      href="https://towardsdatascience.com/shap-explained-the-way-i-wish-someone-explained-it-to-me-ab81cc69ef30"
      data-href="https://towardsdatascience.com/shap-explained-the-way-i-wish-someone-explained-it-to-me-ab81cc69ef30"
      target="_blank"
      >https://towardsdatascience.com/shap-explained-the-way-i-wish-someone-explained-it-to-me-ab81cc69ef30</a
    >
  </p>
  <p>
    [10]
    <a
      href="https://medium.com/@gabrieltseng/interpreting-complex-models-with-shap-values-1c187db6ec83"
      data-href="https://medium.com/@gabrieltseng/interpreting-complex-models-with-shap-values-1c187db6ec83"
      target="_blank"
      >https://medium.com/@gabrieltseng/interpreting-complex-models-with-shap-values-1c187db6ec83</a
    >
  </p>
  <p>
    [11]
    <a
      href="https://medium.com/@stanleyg1/a-detailed-walk-through-of-shap-example-for-interpretable-machine-learning-d265c693ac22"
      data-href="https://medium.com/@stanleyg1/a-detailed-walk-through-of-shap-example-for-interpretable-machine-learning-d265c693ac22"
      target="_blank"
      >https://medium.com/@stanleyg1/a-detailed-walk-through-of-shap-example-for-interpretable-machine-learning-d265c693ac22</a
    >
  </p>
  <p>
    [12]
    <a
      href="https://francescopochetti.com/whitening-a-black-box-how-to-interpret-a-ml-model/"
      data-href="https://francescopochetti.com/whitening-a-black-box-how-to-interpret-a-ml-model/"
      target="_blank"
      >https://francescopochetti.com/whitening-a-black-box-how-to-interpret-a-ml-model/</a
    >
  </p>
  <p>
    [13] Explaining Anomalies Detected by Autoencoders Using SHAP; Antwarg et
    al.
    <a
      href="https://arxiv.org/pdf/1903.02407.pdf"
      data-href="https://arxiv.org/pdf/1903.02407.pdf"
      target="_blank"
      >https://arxiv.org/pdf/1903.02407.pdf</a
    >
  </p>
  <p>
    [14]
    <a
      href="https://medium.com/fiddlerlabs/case-study-explaining-credit-modeling-predictions-with-shap-2a7b3f86ec12"
      data-href="https://medium.com/fiddlerlabs/case-study-explaining-credit-modeling-predictions-with-shap-2a7b3f86ec12"
      target="_blank"
      >https://medium.com/fiddlerlabs/case-study-explaining-credit-modeling-predictions-with-shap-2a7b3f86ec12</a
    >
  </p>
  <p>
    [15]
    <a
      href="https://www.kaggle.com/itsmesunil/bank-loan-modelling"
      data-href="https://www.kaggle.com/itsmesunil/bank-loan-modelling"
      target="_blank"
      >https://www.kaggle.com/itsmesunil/bank-loan-modelling</a
    >
  </p>

  <footer>
    This blog post was originally published with
    <a
      href="https://medium.com/swlh"
      alt="Get smarter at building your thing. Follow to join The Startup’s +8 million monthly readers & +778K followers."
      >The Startup</a
    >
    at
    <a
      href="https://medium.com/swlh/push-the-limits-of-explainability-an-ultimate-guide-to-shap-library-a110af566a02"
      alt="Discover how to push the boundaries of explainability in data science and machine learning. This comprehensive guide to SHAP (SHapley Additive exPlanations) covers everything from deep learning algorithms to model interpretability, making complex AI models more transparent and trustworthy. Ideal for data science professionals and enthusiasts looking to enhance their understanding of machine learning interpretability with cutting-edge tools and techniques."
      >Medium</a
    >. <br /><br />
  </footer>
  <script
    type="text/javascript"
    src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-584ec4ce89deed84"
  ></script>
  <div class="addthis_inline_share_toolbox"></div>
</section>

      <div class="backnext btnbottom">
                <div class="back">
        <div class="bpointer"><a href="/works/ner_cc"
         class="button small" title="Back">&lt; Back</a></div>
         <!-- class="icon fa-backward" title="Back"></a></div> -->
        <div class="btitle"><a href="/works/ner_cc">Named Entity Recognition Tool by Cour de Cassation</a></div>
        </div>
        <div class="next">
        <div class="npointer"><a href="/works/egg_ner"
         class="button small" title="Next">Next &gt;</a></div>
         <!-- class="icon fa-forward" title="Next"></a></div> -->
        <div class="ntitle"><a href="/works/egg_ner">Mastering Named Entity Recognition (NER) in Data Science</a></div>
        </div>

      </div>
      <script>
      document.body.onkeyup = function(e){
        if (e.keyCode == '37') { window.location = '/works/ner_cc'; }
        if (e.keyCode == '39') { window.location = '/works/egg_ner'; }
      };
      </script>
<hr style="margin-bottom:12px;" />
<div class="author">
<!-- style="margin:8px 28px 12px 0;position:relative;float:left;"> -->
  <!-- style="position: relative; float: left;margin:0;padding:0;" -->
  <div style="display:inline-block;border-radius:7px;overflow:hidden;height:100px;width:100px;background:url(/images/ula.jpg);background-size:100px;"></div>
  <div style="display:inline-block;padding-left:12px;vertical-align:top;"><b>by:<br />Urszula Czerwinska</b><br />(<a href="mailto:urszula.czerwinska@cri-paris.org">urszula.czerwinska@cri-paris.org</a>)<br
    /><i><a href="http://urszulaczerwinska.github.io" target="_blank">http://urszulaczerwinska.github.io</a></i>
  </div>
  <div class="auth-desc"><p> Senior Data Scientist / Deep Learning Engineer </p> PhD in Bio-Mathematics, Data Science & Machine Learning
</div>
</div>
<hr style="margin-top:9px;" />

  
    </div>
  </section>


      </article>

      <!-- Footer -->
<footer id="footer">
  <ul class="icons">
    <li><a target="_blank" href="https://twitter.com/ulalaparis" class="icon fa-twitter"
           ><span class="label">twitter</span></a></li>
    <li><a target="_blank" href="https://github.com/urszulaczerwinska" class="icon fa-github"
           ><span class="label">github</span></a></li>
    <li><a target="_blank" href="https://linkedin.com/in/urszulaczerwinska" class="icon fa-linkedin-square"
           ><span class="label">linkedin-square</span></a></li>
    <li><a target="_blank" href="mailto:ulcia.liberte@gmail.com" class="icon fa-envelope"
           ><span class="label">E-mail</span></a></li>
  </ul>
  <ul class="copyright">
    <li>&copy; 2016,
    2024
      Urszula Czerwinska</li>
    <li><a href="/credits/">Credits</a></li>
  </ul>
</footer>


      <!-- Scripts -->
<script src="/js/jquery.min.js"></script>
<script src="/js/jquery.scrollex.min.js"></script>
<script src="/js/jquery.scrolly.min.js"></script>
<script src="/js/skel.min.js"></script>
<script src="/js/util.js"></script>
<!--[if lte IE 8]><script src="/js/ie/respond.min.js"></script><![endif]-->
<script src="/js/main.js"></script>

    </div>

  </body>



</html>