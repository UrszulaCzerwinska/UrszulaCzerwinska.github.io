<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Urszula Czerwinska | Data Scientist &amp; Deep Learning Engineer</title>
    <description>Explore the journey of Urszula Czerwinska from PhD to Data Science, featuring insights on Data Science projects, Machine Learning, and Deep Learning. Discover how to become a Data Scientist or Machine Learning Engineer.</description>
    <link>http://urszulaczerwinska.github.io/</link>
    <atom:link href="http://urszulaczerwinska.github.io/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Wed, 18 Sep 2024 10:39:31 +0200</pubDate>
    <lastBuildDate>Wed, 18 Sep 2024 10:39:31 +0200</lastBuildDate>
    <generator>Jekyll v4.3.3</generator>
    
      <item>
        <title>Dilated Diffusion from DemoFusion</title>
        <description>&lt;head&gt;
  &lt;meta name=&quot;description&quot; content=&quot;Learn how DemoFusion’s dilated diffusion revolutionizes AI image generation, making high-resolution results accessible using standard hardware. Dive into dilated sampling and progressive upscaling techniques.&quot; /&gt;
&lt;/head&gt;

&lt;p&gt;&lt;strong&gt;TDLR:&lt;/strong&gt; see demo of dilated diffusion &lt;a href=&quot;https://colab.research.google.com/drive/1gHCjibaI91a50bXjYbemUE7khRdZmyle?usp=sharing&quot;&gt;in a collab&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*ZPCdjBebme6X_8-0.png&quot; alt=&quot;Graphical representation of dilated sampling in stable diffusion Unicorn diffusion art&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;At the #CVPR24 best paper review, I came across an exciting stable diffusion paper.&lt;/p&gt;

&lt;h2 id=&quot;demofusion-unlocking-high-resolution-ai-image-generation-with-dilated-diffusion&quot;&gt;DemoFusion: Unlocking High-Resolution AI Image Generation with Dilated Diffusion&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://ruoyidu.github.io/demofusion/demofusion.html&quot; title=&quot;https://ruoyidu.github.io/demofusion/demofusion.html&quot;&gt;&lt;strong&gt;DemoFusion&lt;/strong&gt; &lt;em&gt;Democratising High-resolution Image Generation without a Sweat&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The goal of this research is to democratize high-resolution image generation while reducing costs. DemoFusion extends Latent Diffusion Models (LDMs) by introducing Progressive Upscaling, Skip Residuals, and Dilated Sampling mechanisms.&lt;/p&gt;

&lt;h3 id=&quot;key-features-of-demofusion&quot;&gt;Key Features of DemoFusion:&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Progressive Upscaling&lt;/strong&gt;: Iteratively increases image resolution using lower-resolution results as a base.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Upsample-Diffuse-Denoise Loop&lt;/strong&gt;: Utilizes noise-inverted representations for guiding higher resolution generation.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Dilated Sampling&lt;/strong&gt;: Enhances global context, resulting in more coherent image generation.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;applications-of-demofusion&quot;&gt;Applications of DemoFusion:&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Generate high-resolution images up to &lt;strong&gt;4096×4096&lt;/strong&gt; using standard hardware like an RTX 3090 GPU.&lt;/li&gt;
  &lt;li&gt;Integrate with &lt;strong&gt;ControlNet&lt;/strong&gt; for additional functionality.&lt;/li&gt;
  &lt;li&gt;Upscale existing images by encoding them into the LDM’s latent space.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Bonus:&lt;/strong&gt; Intermediate results are available during the generation process, enabling rapid iteration and previewing.&lt;/p&gt;

&lt;p&gt;Check out more demos &lt;a href=&quot;https://replicate.com/lucataco/demofusion&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;exploring-the-concept-and-benefits-of-dilated-sampling-in-ai-image-generation&quot;&gt;Exploring the Concept and Benefits of Dilated Sampling in AI Image Generation&lt;/h2&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*-1ylqkgPXbdNxeTMw9MstA.png&quot; alt=&quot;Visual concept of dilated diffusion process, showing pixel grids and sampling gaps&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;h3 id=&quot;concept-of-dilated-sampling&quot;&gt;Concept of Dilated Sampling:&lt;/h3&gt;

&lt;p&gt;Imagine an image as a grid of pixels. Instead of processing each pixel in sequence, &lt;strong&gt;dilated sampling&lt;/strong&gt; selects every second or third pixel, which creates a broader view of the image. This technique enables fewer steps, while providing a broader context for denoising and refining images.&lt;/p&gt;

&lt;h3 id=&quot;purpose-of-dilated-sampling&quot;&gt;Purpose of Dilated Sampling:&lt;/h3&gt;

&lt;p&gt;The goal is to capture global image information instead of focusing on small local details. This method helps establish a global context, leading to more cohesive and coherent image generation.&lt;/p&gt;

&lt;h3 id=&quot;implementation-of-dilated-sampling&quot;&gt;Implementation of Dilated Sampling:&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;A regular pattern is avoided; instead, dilated sampling skips pixels based on a &lt;em&gt;dilation factor&lt;/em&gt;. For example, if the dilation factor is 2, every second pixel is picked.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Shifting and Combining&lt;/strong&gt;: The sampling shifts its starting point in each round to ensure complete image coverage.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;preventing-image-graininess&quot;&gt;Preventing Image Graininess:&lt;/h3&gt;

&lt;p&gt;One drawback of dilated sampling is the potential for graininess, as the sampled pixels are spread apart. To counter this, a &lt;strong&gt;Gaussian filter&lt;/strong&gt; smooths the image before sampling, ensuring the sampled points represent the image more accurately.&lt;/p&gt;

&lt;h3 id=&quot;conclusion-how-dilated-sampling-enhances-ai-image-generation&quot;&gt;Conclusion: How Dilated Sampling Enhances AI Image Generation&lt;/h3&gt;

&lt;p&gt;Think of dilated sampling like stepping back to admire an entire painting before focusing on the details. This technique strikes a balance between global perspective and fine detail, resulting in high-quality images.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;step-by-step-code-implementation&quot;&gt;Step-by-Step Code Implementation&lt;/h2&gt;

&lt;p&gt;For those interested in the technical details, full code is available on GitHub: &lt;a href=&quot;https://github.com/PRIS-CV/DemoFusion/blob/main/pipeline_demofusion_sdxl.py&quot;&gt;DemoFusion GitHub Repository&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;code-step-bystep&quot;&gt;CODE STEP BY STEP&lt;/h3&gt;

&lt;p&gt;Full code can be found in author’s github: &lt;a href=&quot;https://github.com/PRIS-CV/DemoFusion/blob/main/pipeline_demofusion_sdxl.py&quot;&gt;https://github.com/PRIS-CV/DemoFusion/blob/main/pipeline_demofusion_sdxl.py&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;views&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;current_scale_num&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;current_scale_num&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;views_batch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;views&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;view_batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;views&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;view_batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Here, `views` and `views_batch` set up the grid for dilated sampling. `current_scale_num` determines the dilation factor, creating a sparse sampling grid.&lt;/p&gt;

&lt;p&gt;Gather more global information about the image rather than focusing on local details.&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Grid for dilated sampling
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;count_global&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;zeros_like&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;latents_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;value_global&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;zeros_like&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;latents_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;`count_global` and `value_global` are initialized to aggregate global information.&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Loop for picking pixels with gaps
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_view&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;views_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;latents_for_view&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;cat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;latents_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;current_scale_num&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;current_scale_num&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_view&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The loop iterates through `views_batch`, picking pixels with a gap determined by `current_scale_num`.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_view&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;views_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;latents_for_view_gaussian&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;cat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;latents_gaussian&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;current_scale_num&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;current_scale_num&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_view&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;`latents_for_view_gaussian` ensures the global context is gathered, then combined with local details later.&lt;/p&gt;

&lt;p&gt;Shifted dilated sampling means the starting point shifts to cover different parts of the image. The global context is combined with local details to refine the final image.&lt;/p&gt;

&lt;p&gt;Gaussian filter is applied to smooth the image before sampling&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;std_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mean_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;latents_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;latents_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;latents_gaussian&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;gaussian_filter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;latents_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;current_scale_num&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sigma&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigma&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;latents_gaussian&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;latents_gaussian&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;—&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;latents_gaussian&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;latents_gaussian&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mean_&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h3 id=&quot;smart-blending&quot;&gt;&lt;strong&gt;Smart Blending&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;Combining global and local details ensures that the image retains the broader context and finer details.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;latents_view_denoised&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;latents_denoised_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;chunk&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vb_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_view&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
 &lt;span class=&quot;n&quot;&gt;value_global&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;current_scale_num&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;current_scale_num&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;latents_view_denoised&lt;/span&gt;
 &lt;span class=&quot;n&quot;&gt;count_global&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;current_scale_num&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;current_scale_num&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Here, denoised views (`latents_view_denoised`) are added to `value_global`, blending the global and local contexts.&lt;/p&gt;

&lt;p&gt;The final latent representation is formed by blending global and local contexts.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;value_global&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value_global&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h_pad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w_pad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value_global&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c2&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;count&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;ones_like&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value_global&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c2&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;latents&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;where&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;count&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The global values are combined with local values (`value += value_global * c2`) and normalized (`latents = torch.where(count &amp;gt; 0, value / count, value)`).&lt;/p&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;CONCLUSION &lt;/h3&gt;

&lt;p&gt;This code implements dilated sampling by creating a grid with gaps (dilation), applying a Gaussian filter to smooth out graininess, gathering global context, and then blending it with local details to form the final denoised image. This process ensures a balance between capturing the big picture and refining the details.&lt;/p&gt;

&lt;p&gt;Try out a simple demo illustrating the concept of dilated sampling&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://colab.research.google.com/drive/1gHCjibaI91a50bXjYbemUE7khRdZmyle?usp=sharing&quot;&gt;Collab demo&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The demo is a simple illustration of the dilated sampling concept using simulated data. The visualizations help in understanding how dilated sampling and smoothing work together.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Original Image:
 — A simple checkboard patten image is created for demonstration.&lt;/li&gt;
  &lt;li&gt;Smoothed Image (Gaussian Filter):
 — The original image is smoothed using a Gaussian filter to reduce graininess.&lt;/li&gt;
  &lt;li&gt;Dilated Sampling after Smoothing:
 — Dilated sampling is applied to the smoothed image, resulting in a more coherent global context.&lt;/li&gt;
  &lt;li&gt;Dilated Sampling:
 — Pixels are sampled with a gap (dilation factor).&lt;/li&gt;
&lt;/ol&gt;

&lt;footer&gt;
  &lt;p&gt;Exported from &lt;a href=&quot;https://medium.com&quot;&gt;Medium&lt;/a&gt; in June 2024.&lt;/p&gt;
  &lt;p&gt;&lt;a href=&quot;https://medium.com/@ulalaparis/dilated-difusion-concept-from-demofusion-e32a7b5d09d6&quot;&gt;View
      the original&lt;/a&gt;&lt;/p&gt;
&lt;/footer&gt;
&lt;script type=&quot;text/javascript&quot; src=&quot;//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-584ec4ce89deed84&quot;&gt;&lt;/script&gt;

&lt;div class=&quot;addthis_inline_share_toolbox&quot;&gt;&lt;/div&gt;

</description>
        <pubDate>Wed, 19 Jun 2024 00:00:00 +0200</pubDate>
        <link>http://urszulaczerwinska.github.io/thoughts/dilated-difusion-concept-from-demofusion/</link>
        <guid isPermaLink="true">http://urszulaczerwinska.github.io/thoughts/dilated-difusion-concept-from-demofusion/</guid>
        
        
        <category>thoughts</category>
        
      </item>
    
      <item>
        <title>Embracing the Unknown 2/2</title>
        <description>&lt;head&gt;
  &lt;meta name=&quot;description&quot; content=&quot;Explore key takeaways from the AI Startup School seminars, including lessons on entrepreneurship, risk management, and the future of AI from industry leaders.&quot; /&gt;
&lt;/head&gt;

&lt;p&gt;I &lt;strong&gt;was granted exclusive online access to the AI Startup School seminars. Here are some awesome and very transforming things I learned from the top startup and tech speakers.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In order to read about my application to AI Startup School and my thoughts on entrepreneurship and risk, check out part I:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/thoughts/embracing-the-unknown-1-2-applying-to-ai-startup-school-reflections&quot;&gt;Embracing the Unknown 1/2: Applying to AI Startup School — Reflections&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*mnx_Q7FmNfRTzb3W.jpeg&quot; alt=&quot;A diverse group of people engaging in a seminar setting, representing the AI Startup School experience.&quot; title=&quot;Participants at AI Startup School seminar&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;As I navigated my own journey, exploring my relationship with risk and entrepreneurship, I followed the AI Startup School seminar series each week. This transition—from introspective exploration to external insights—shifted both my setting and perspective. I discovered that my personal relationship with risk and uncertainty echoed the stories and lessons shared by the speaking entrepreneurs and innovators. The difference being they took the step into the unknown, armed with resilience and a thirst for knowledge.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*chNP4occqmr87cEa&quot; alt=&quot;A speaker addressing an audience during the AI Startup School seminar series.&quot; title=&quot;Speaker at AI Startup School&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;I am not allowed to share the exact content of the seminar series, but I would like to convey the messages and thoughts that impacted me the most over the 9 lectures of this special EF talks.&lt;/p&gt;

&lt;p&gt;All lectures took the form of interviews with the speaker(s). The speakers were a unique blend of famous figures in the AI startup scene, young entrepreneurs, and VC investors.&lt;/p&gt;

&lt;h3 id=&quot;key-takeaways-from-the-ai-startup-school-seminars&quot;&gt;&lt;strong&gt;Key Takeaways from the AI Startup School Seminars&lt;/strong&gt;&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;“Being in too early is the same as being wrong.”
— &lt;a href=&quot;https://www.linkedin.com/in/eisokant/&quot;&gt;Eiso Kant&lt;/a&gt;, &lt;a href=&quot;https://www.poolside.ai/&quot;&gt;Poolside&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*Dgt-Daih3LEYuGaz&quot; alt=&quot;A thoughtful entrepreneur reflecting on timing and market fit during the seminar.&quot; title=&quot;Eiso Kant sharing insights on timing and market fit&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;If the product you develop isn’t tailored to the public, or if the technology you invest in is too immature to add value, your venture is unlikely to succeed. &lt;a href=&quot;https://www.linkedin.com/in/eisokant/&quot;&gt;Eiso Kant&lt;/a&gt;, CTO and co-founder of &lt;a href=&quot;https://www.poolside.ai/&quot;&gt;Poolside&lt;/a&gt;, shared insights from his experience in founding a startup focused on developing cognitive abilities through neural networks in 2016. Similarly, &lt;a href=&quot;https://www.linkedin.com/in/arthur-mensch/?locale=fr_FR&quot;&gt;Arthur Mensch&lt;/a&gt; from &lt;a href=&quot;https://mistral.ai/fr/&quot;&gt;Mistral AI&lt;/a&gt; highlighted the importance of capitalizing on opportunities at the opportune moment—neither too early nor too late. If the technology isn’t sufficiently mature, the business might struggle. It’s possible to choose the right technology but at an inopportune moment.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*Br-KuJWcK2glhmaz&quot; alt=&quot;A speaker from the seminar discussing the balance between effort and impact.&quot; title=&quot;Matt Clifford discussing the correlation between effort and impact&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“The faster you acknowledge the lack of correlation between effort and impact, the faster it stops you complexes.”
“Life is unfair, get over it.”
— &lt;a href=&quot;https://www.linkedin.com/in/mattcliffordef/?originalSubdomain=uk&quot;&gt;Matt Clifford&lt;/a&gt;, co-founder of &lt;a href=&quot;https://www.joinef.com/&quot;&gt;Entrepreneur First&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Matt Clifford, co-founder of Entrepreneur First, shared a wealth of knowledge about AI business models and the entrepreneurial spirit, as well as life in general. Having witnessed the rise and fall of many startups and invested in numerous ventures, he has gathered a trove of stories and insights. One of the most impactful statements was about acceptance—the acceptance of the fact that we don’t always get back what we give. Working tirelessly on a project does not guarantee success. Sometimes, factors like luck or intuition play a decisive role in a company’s fate.&lt;/p&gt;

&lt;h3 id=&quot;the-role-of-open-source-in-ai&quot;&gt;&lt;strong&gt;The Role of Open Source in AI&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;The figures in the AI field, such as &lt;a href=&quot;https://twitter.com/emostaque?lang=en&quot;&gt;Emad Mostaque&lt;/a&gt; of &lt;a href=&quot;https://stability.ai/&quot;&gt;Stability AI&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/arthur-mensch/?locale=fr_FR&quot;&gt;Arthur Mensch&lt;/a&gt; of &lt;a href=&quot;https://mistral.ai/fr/&quot;&gt;Mistral AI&lt;/a&gt;, and &lt;a href=&quot;https://www.linkedin.com/in/karim-beguir-2350161/?originalSubdomain=uk&quot;&gt;Karim Beguir&lt;/a&gt; of &lt;a href=&quot;https://www.instadeep.com/&quot;&gt;Instadeep&lt;/a&gt;, are strong supporters of open source. They see significant business opportunities in building around open-source models, with community engagement and customer acquisition being strong arguments in favor of this approach. Arthur Mensch believes in enhancing open-source business with exceptional customer service and thriving through partnerships. Karim Beguir finds great reward in community contributions to their work, which strengthens the product and unlocks business potential in scaling open source to meet client needs.&lt;/p&gt;

&lt;p&gt;Stability AI’s business model involves providing access to advanced generative AI models through a subscription membership. This membership includes all available models, similar to Amazon Prime. This approach aims to make generative AI models predictable and easy to use. Additionally, the company offers consulting services for top organizations in need of expertise in generative AI. The goal is to revolutionize the industry by providing model base and support while adapting to the market’s growth.&lt;/p&gt;

&lt;p&gt;Leading players in the AI space have varying perspectives on open vs. closed models. OpenAI initially started with open models but has become cautious due to concerns about misuse. Google is open in some areas but avoids open sourcing models. Meta embraces openness, especially in language models. Microsoft supports open AI but has a mixed approach. Amazon focuses more on infrastructure. Apple is very secretive about anything they make, with some rare exceptions. Smaller labs and Japanese companies are aggressively pursuing open models.&lt;/p&gt;

&lt;p&gt;The balance between open and closed AI models is shifting, with more players recognizing the benefits of openness.&lt;/p&gt;

&lt;h3 id=&quot;the-future-of-ai&quot;&gt;&lt;strong&gt;The Future of AI&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;Many speakers emphasized the shift towards ‘quality over quantity’ as an emerging trend. This could take the form of providing the best data to achieve better model performance, or a shift towards specialized models and swarms of models.&lt;/p&gt;

&lt;p&gt;The importance of developing robust evaluation frameworks and corrective mechanisms was also a common theme. The significance of exploring multimodality and enhancing model inference capabilities beyond current paradigms should not be neglected.&lt;/p&gt;

&lt;h3 id=&quot;insights-on-the-entrepreneurial-journey&quot;&gt;&lt;strong&gt;Insights on the Entrepreneurial Journey&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;The speakers extensively discussed their journeys with business ideas and the people they collaborated with. A speaker highlighted that a ‘&lt;strong&gt;moment of audacity&lt;/strong&gt;’ is essential for taking the leap into entrepreneurship.&lt;/p&gt;

&lt;p&gt;Another important step is &lt;strong&gt;finding a co-founder&lt;/strong&gt;. This person should be both your best friend and your challenger, offering support and constructive disagreement. It’s commonly recommended to seek a business partner with complementary skills. However, a few speakers mentioned they did not follow this advice, and it still worked out for them as they are complementary on a different level, even though they have similar hard skills.&lt;/p&gt;

&lt;p&gt;To build a successful business, the right idea involves &lt;strong&gt;identifying a niche&lt;/strong&gt;, such as industry-specific issues, novel technologies, or regulatory gaps. For instance, Mistral AI identified a gap in the AI startup landscape and aimed to expedite progress in foundational models.&lt;/p&gt;

&lt;p&gt;They also advised against becoming too lazy in a stable job. &lt;strong&gt;The quicker you immerse yourself in entrepreneurship, the better.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Another vital skill mentioned was the need to rapidly obtain feedback. The speaker encouraged aspiring entrepreneurs to begin creating, even if the idea isn’t perfect, and to engage with potential customers for feedback. Prioritizing swift iteration over waiting for the perfect final product is crucial. An environment that facilitates rapid learning and exposure is key.&lt;/p&gt;

&lt;p&gt;Additionally, the speakers underscored the importance of embracing failure and perseverance throughout the entrepreneurial journey. Cultivate and pursue your ambition, and think beyond traditional boundaries.&lt;/p&gt;

&lt;p&gt;To elevate the culture of entrepreneurship, Matt Clifford recommended carefully studying successful individuals. Understand their decisions and learn from their mistakes. He argued that reading books is more beneficial than spending time on platforms like Twitter and watching short videos.&lt;/p&gt;

&lt;p&gt;Lastly, investors can act as true partners and guides on the entrepreneurial path, with strategic partnerships and foundational funding serving as catalysts for change.&lt;/p&gt;

&lt;h3 id=&quot;the-next-wave-of-startups-what-to-expect&quot;&gt;&lt;strong&gt;The Next Wave of Startups: What to Expect&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;Currently, the media sector holds great potential for impactful innovation. Entrepreneurs are encouraged to tackle problems that deeply concern them and focus on regulated industries where they can make significant contributions. Within the AI ecosystem, opportunities for value creation span from specialized&lt;/p&gt;

&lt;footer&gt;
  &lt;p&gt;Exported from &lt;a href=&quot;https://medium.com&quot;&gt;Medium&lt;/a&gt; on December 19
    2023.&lt;/p&gt;
  &lt;p&gt;&lt;a href=&quot;https://medium.com/@ulalaparis/embracing-the-unknown-2-2-takeaways-from-ai-startup-school-seminars-9d6bfe7cf4ab&quot;&gt;View
      the original&lt;/a&gt;&lt;/p&gt;
&lt;/footer&gt;
&lt;script type=&quot;text/javascript&quot; src=&quot;//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-584ec4ce89deed84&quot;&gt;&lt;/script&gt;

&lt;div class=&quot;addthis_inline_share_toolbox&quot;&gt;&lt;/div&gt;

</description>
        <pubDate>Thu, 25 Apr 2024 00:00:00 +0200</pubDate>
        <link>http://urszulaczerwinska.github.io/thoughts/embracing-the-unknown-2-2-applying-to-ai-startup-school-reflections</link>
        <guid isPermaLink="true">http://urszulaczerwinska.github.io/thoughts/embracing-the-unknown-2-2-applying-to-ai-startup-school-reflections</guid>
        
        
        <category>thoughts</category>
        
      </item>
    
      <item>
        <title>Embracing the Unknown 1/2</title>
        <description>&lt;head&gt;
  &lt;meta name=&quot;description&quot; content=&quot;A personal reflection on the challenges and insights gained from applying to AI Startup School, exploring the themes of risk-taking, comfort zones, and the entrepreneurial spirit.&quot; /&gt;
&lt;/head&gt;

&lt;p&gt;I &lt;strong&gt;have recently applied for the AI startup school organized by Entrepreneur First.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*kZpUj5gHFo82d49h&quot; alt=&quot;Beautiful sky with coloured clouds.&quot; title=&quot;AI Startup School Application Journey&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;With the rapid pace of AI transformations and new challenges brought by Generative AI, I feel like I am always behind and never have enough time to catch up. I am motivated to learn more, accelerate, and be part of the innovation movement. I aim to contribute more to “INtrapreneurship” in my company’s Computer Vision team and be able to drive change.&lt;/p&gt;

&lt;p&gt;The “Entrepreneurs First” program in Paris is an initiative aimed at fostering entrepreneurial talent and facilitating the creation of new startups, particularly in the deep tech sector. EF invests in talented individuals based on their skills or expertise, regardless of whether they already have a business idea or team in place. Entrepreneur First’s Paris program conveys a belief that some of the best potential startup founders never embark on entrepreneurial journeys due to barriers to entry.&lt;/p&gt;

&lt;p&gt;They organized the AI Startup School, a series of evening seminars, and a great networking opportunity.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“An exclusive lecture series in Paris delivered by renowned speakers in the AI startup scene (…) EF’s AI Startup School in Paris will bring together talented individuals excited by the opportunities within AI, to share knowledge and build a wider network within the local AI ecosystem.”
 — &lt;a href=&quot;https://www.joinef.com/ai-startup-school/&quot;&gt;Entrepreneur First AI Startup School&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&quot;https://www.actuia.com/actualite/entrepreneur-first-annonce-le-lancement-dun-nouveau-programme-a-paris-et-poursuit-son-expansion-internationale/&quot; title=&quot;https://www.actuia.com/actualite/entrepreneur-first-annonce-le-lancement-dun-nouveau-programme-a-paris-et-poursuit-son-expansion-internationale/&quot;&gt;&lt;strong&gt;Entrepreneur First annonce le lancement d’un nouveau programme à Paris et poursuit son expansion…&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;SCOOP: I was not accepted.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;If you want to read directly about AI Startup School seminars, skip to &lt;a href=&quot;/thoughts/embracing-the-unknown-2-2-applying-to-ai-startup-school-reflections&quot;&gt;Embracing the Unknown 2/2: Takeaways from AI Startup School — Seminars&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Of course, I cannot know the specific reasons; however, there was one question I reckon I did not nail:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“What was the most risky thing you have ever done?”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;risk&quot;&gt;Risk?&lt;/h3&gt;

&lt;p&gt;During the interview, my mind raced with seemingly trivial or unimpressive ideas such as: “I tested my code in prod” (lol, never do that), “I drive a motorcycle” or “I changed jobs on my own initiative”.&lt;/p&gt;

&lt;p&gt;Some time later, &lt;strong&gt;&lt;em&gt;I started to think about my relationship with risk.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Growing up in Poland, I was ingrained with a cautious ethos from my parents: a conviction that regular life can surprise us with enough troubles that deliberately taking risks is pure stupidity and equals a miserable outcome.&lt;/p&gt;

&lt;p&gt;We have this saying: “do not praise the day before the sunset”, “do not say ‘hop’ before you jump”, “do not share the skin of the bear before you’ve killed the beast”, “enjoy today, tomorrow will be worse”. I was told I should not be too (or at all) optimistic and always think about the worst-case scenario.&lt;/p&gt;

&lt;p&gt;For most of my life, this cautious approach underpinned every decision I made, be it choosing a high school or committing to a relationship. Always a Plan A, followed by Plan B and C, etc. Living life being scared and putting my energy into foreseeing what can go wrong and which backup plan I should put in place.&lt;/p&gt;

&lt;p&gt;Some (including myself) might argue I’ve taken risks: moving solo from Poland to a tiny, unknown, cute town in French Brittany for bio-mathematics studies. But to be honest, I had worked out a solid Plan B; I was subscribed to a university in Poland, starting a month later than the one in France. It would only cost me a plane ticket to go back and live my life like nothing ever happened (I stayed in France, after all). Would I have chosen the adventure if there were no way back?&lt;/p&gt;

&lt;p&gt;I suppose all these plans and being “realistic” (or one could say rather “pessimistic”) about my life and career steps helped me to survive and saved me from trouble many times. But, also, maybe, stopped me from getting somewhere further or somewhere else.&lt;/p&gt;

&lt;p&gt;It’s not that I lack ambition or curiosity. I have always had tons of it. Being the best in the class, being the most performant, getting recognition for what I do, giving my time and passion to projects, trying different sports: kite surfing, skiing, biking, boxing. From some perspective now, I just realize I have never taken a real risk, going into a total unknown with faith.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;To me, entrepreneurship embodies this very essence of risk-taking: a belief so strong it borders on the edge of madness.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Once, with two colleagues, we had a start-up idea. We pitched it to founders, attended startup events, and when we faced the question of commitment, all of us chose a more secure career choice, granting a stable income.&lt;/p&gt;

&lt;p&gt;I convinced myself that those who embark on this entrepreneurial journey are invariably privileged. They are usually white men from good families, supporting them with money or a professional network so that they do not have to stress about what to eat, and therefore they can live most of their entrepreneurial adventure. Even if this is true for some, I think now, I was telling myself a story I wanted to hear.&lt;/p&gt;

&lt;p&gt;Let’s think of the controversial protagonist of “WeCrashed” Netflix series, Adam Neumann, who tells a tale of ambition, innovation, and ultimate downfall. He managed, with an idea and determination, to reach a valuation of $47 billion at its peak. Also, think about Gordon Ramsay, the multi-Michelin starred chef and star of the small screen. In his interview at Masterclass.com, he shared that in order to build his business he left his comfortable position and went to France to learn everything “from scratch”, even though he worked for a starred restaurant in the UK. Later, he also sold his house in order to put all the money in his new restaurant, working like a madman.&lt;/p&gt;

&lt;p&gt;All those people who apply to EF to learn, grow, and thrive, even if they don’t always have a comfortable backup plan, have earned my respect and admiration. They are getting out of their comfort zone because they believe there is something worth fighting for, whatever the motivation is — be it praise, money, or saving the world and curing cancer.&lt;/p&gt;

&lt;h3 id=&quot;comfort&quot;&gt;Comfort?&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Another important keyword: comfort&lt;/strong&gt;. Nowadays, in Western Europe, in France, I think, we are really used to our comfort. Comfort is a heated house, a warm meal but also a stable job, health insurance, a partner to talk to and government aids. I realize that I like my comfort too much, that I am privileged enough, but not ready to face the discomfort of the unknown.&lt;/p&gt;

&lt;p&gt;So the one remarkable question emerges: &lt;strong&gt;is there a cause, a dream, something I value so profoundly that I’m willing to step beyond my comfort zone and embrace the unknown?&lt;/strong&gt; Accept that there is no Plan B, that there is Plan A or nothing? Will I allow myself to dream?&lt;/p&gt;

&lt;p&gt;In these reflections, there is a deeper quest to understand what drives me, what scares me, and what it truly means to step into the unknown.&lt;/p&gt;

&lt;h3 id=&quot;ai-startup-school-seminars&quot;&gt;AI Startup School Seminars&lt;/h3&gt;

&lt;p&gt;As I mentioned before, I was not accepted into the AI Startup School program. However, I am grateful that &lt;strong&gt;I was granted exclusive online access to the seminars.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Read about my learnings in the next post:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/thoughts/embracing-the-unknown-2-2-applying-to-ai-startup-school-reflections&quot;&gt;Embracing the Unknown 2/2: Takeaways from AI Startup School — Seminars&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The series of nine interviews with the speaker(s) representing a unique blend of famous figures of AI startup scene, young entrepreneurs and VC investors.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Maybe the learning will change your life? At least, I hope, you will have an enjoyable read if you are interested in entrepreneurship, AI, investing, or all of it. Let’s dive into the AI startup world together.&lt;/strong&gt;&lt;/p&gt;

&lt;footer&gt;
  &lt;p&gt;Exported from &lt;a href=&quot;https://medium.com&quot;&gt;Medium&lt;/a&gt; on April 2024.&lt;/p&gt;
  &lt;p&gt;&lt;a href=&quot;https://medium.com/@ulalaparis/embracing-the-unknown-1-2-applying-to-ai-startup-school-reflections-9364c4814d0c&quot;&gt;View
      the original&lt;/a&gt;&lt;/p&gt;
&lt;/footer&gt;
&lt;script type=&quot;text/javascript&quot; src=&quot;//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-584ec4ce89deed84&quot;&gt;&lt;/script&gt;

&lt;div class=&quot;addthis_inline_share_toolbox&quot;&gt;&lt;/div&gt;

</description>
        <pubDate>Sat, 13 Apr 2024 00:00:00 +0200</pubDate>
        <link>http://urszulaczerwinska.github.io/thoughts/embracing-the-unknown-1-2-applying-to-ai-startup-school-reflections</link>
        <guid isPermaLink="true">http://urszulaczerwinska.github.io/thoughts/embracing-the-unknown-1-2-applying-to-ai-startup-school-reflections</guid>
        
        
        <category>thoughts</category>
        
      </item>
    
      <item>
        <title>The Mamba Effect</title>
        <description>&lt;head&gt;
  &lt;meta name=&quot;description&quot; content=&quot;Unlock the potential of AI with our comprehensive guide on ML Mamba models. Discover how these advanced machine learning frameworks are revolutionizing data analysis, predictive analytics, and automated decision-making. Learn about their key features, benefits, and applications across various industries. Enhance your understanding and leverage ML Mamba models to stay ahead in the rapidly evolving world of artificial intelligence.&quot; /&gt;
&lt;/head&gt;

&lt;p&gt;Already 8 papers since December 2023 !&lt;/p&gt;

&lt;p&gt;Mamba is already a new wave starting as a replacement for vanilla transformer, it has already been adapted to text, vison, video…&lt;/p&gt;

&lt;p&gt;Mamba models represent a significant breakthrough in neural network architecture.&lt;/p&gt;

&lt;p&gt;Among published papers, besides the original Mamba paper, I would like to distinguish two spin-offs: VMamba and MambaBytes. There it also a bunch of papers with specific biomedical applications that I am not expert to evaluate impact.&lt;/p&gt;

&lt;p&gt;Here I compiled a short overview of all (?) those papers adapting the template from &lt;a href=&quot;https://www.deeplearning.ai/the-batch/&quot;&gt;The Batch newsletter from deeplearning.ai&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;here is a list of mamba papers : &lt;a href=&quot;https://github.com/yyyujintang/Awesome-Mamba-Papers&quot;&gt;https://github.com/yyyujintang/Awesome-Mamba-Papers&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;arxiv-231201-mamba-linear-time-sequence-modeling-with-selective-state-spaces&quot;&gt;Arxiv 23.12.01: Mamba: Linear-Time Sequence Modeling with Selective State Spaces&lt;/h2&gt;

&lt;p&gt;#mamba&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2312.00752&quot;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/state-spaces/mamba&quot;&gt;Code&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This paper is the first to introduce the Mamba architecture. Mamba offers faster inference, linear scaling with sequence length, and strong performance.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*aPbcg2rPGh68SViRpTi21Q.png&quot; alt=&quot;Deep learing mamaba model schema&quot; /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*bAj0TMSgZetUghDij6U-zA.png&quot; alt=&quot;Mamba model illustration of architecture&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What’s New&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A new architecture, Mamba, integrating SSMs without relying on attention or MLP blocks.&lt;/li&gt;
  &lt;li&gt;Implementation of a hardware-aware parallel algorithm for efficient computation.&lt;/li&gt;
  &lt;li&gt;Long context: the quality and efficiency together yield performance improvements on real data up to sequence length 1M&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;How it Works:&lt;/strong&gt; Mamba adds recurrent and convolutional models, to &lt;strong&gt;a unique selection mechanism&lt;/strong&gt; that enables the model to prioritize or ignore inputs based on the content relevance. This approach allows for &lt;strong&gt;linear scalability&lt;/strong&gt; in sequence length. Mamba integrates selective SSMs into a simplified neural network architecture with gates. They are structured to enable the model to selectively propagate or to forget information based on the current token.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Results:&lt;/strong&gt; Mamba demonstrates state-of-the-art performance across various modalities. In language modeling, &lt;strong&gt;Mamba-3B outperforms Transformers&lt;/strong&gt; of the same size, matches Transformers twice its size in both pretraining and downstream evaluation. In terms of efficiency, it achieves &lt;strong&gt;5× higher throughput than Transformers&lt;/strong&gt; and scales linearly with sequence length.
This in various domains such as language, genomics, audio modeling. It efficiently handles &lt;strong&gt;sequences up to a million lengths&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Behind the News:&lt;/strong&gt; The development of Mamba marks a significant &lt;strong&gt;shift from the dominant Transformer-based architectures&lt;/strong&gt;. It opens new avenues in sequence modeling, especially for applications requiring efficient processing of long data sequences. Authors mentions their ambition to make Mamba alternative to Tranformers and CNN as a &lt;a href=&quot;https://medium.com/adevinta-tech-blog/foundation-models-a-new-vision-for-e-commerce-76904a3066e8&quot;&gt;Foundation Model&lt;/a&gt; backbone.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Why it Matters:&lt;/strong&gt; Mamba has the potential to revolutionize various applications in deep learning. This first paper already resulted in 7 other papers in the same month as code is under apache licene and public.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;We’re Thinking:&lt;/strong&gt; We are missing distance to see how Mamba module will perform in practice. However the fact that many publicaitons already applied Mamba, modified it and obtained pulishable resutls is promising.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;In Detail:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Selective SSMs&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/2560/1*j4g2N5BtJUvjEsXqfVPmJg.png&quot; alt=&quot;&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;The key contribution of the paper lies in the novel implementation of &lt;strong&gt;Selective&lt;/strong&gt; State Space Models. It leverages parameters that control if the model response to current inputs or it maintains its existing state. For instance, a parameter ∆ (Delta) in the model’s architecture determines the balance between focusing on the current input (larger ∆ values) and preserving the ongoing state (smaller ∆ values). The selective modulation of parameters B and C tunes how the inputs influence the state and, conversely, how the state influences the outputs. The selective approach also manages context and resets boundaries in scenarios where sequences are concatenated. This prevents the unwanted bleed of information between concatenated sequences.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Other ressources:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/ai-insights-cobet/building-mamba-from-scratch-a-comprehensive-code-walkthrough-5db040c28049&quot;&gt;building-mamba-from-scratch-a-comprehensive-code-walkthrough&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/@joeajiteshvarun/mamba-revolutionizing-sequence-modeling-with-selective-state-spaces-8a691319b34b&quot;&gt;mamba-revolutionizing-sequence-modeling-with-selective-state-spaces&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/@athekunal/mamba-and-state-space-models-explained-b1bf3cb3bb77&quot;&gt;mamba-and-state-space-models-explained&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;arxiv-240108-moe-mamba-efficient-selective-state-space-models-with-mixture-of-experts&quot;&gt;Arxiv 24.01.08: &lt;strong&gt;MoE-Mamba: Efficient Selective State Space Models with Mixture of Experts&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;#LLM&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2401.04081&quot;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The paper introduces a novel model in the field of sequential modeling.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*wCH-77Q_ThqTubu4H-Sl9w.png&quot; alt=&quot;MoE-Mamba: Efficient Selective State Space Models with Mixture of Experts&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Introduction:&lt;/strong&gt; The paper presents MoE-Mamba, a model that integrates State Space Models (SSMs) with Mixture of Experts (MoE) to enhance sequential modeling. This combination aims to leverage the strengths of both SSMs, known for their efficient performance, and MoE, a technique for scaling up models efficiently.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What’s New:&lt;/strong&gt; Integration of SSMs and MoE. MoE layers are used by Mistral, one of SOTA LLM models.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How it Works:&lt;/strong&gt; The MoE-Mamba architecture replaces every other Mamba layer with a MoE feed-forward layer.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;
- MoE-Mamba achieves better performance than both Mamba and Transformer-MoE models.
- It reaches the same performance as Mamba in significantly fewer training steps.
- The model scales well with the number of experts, with optimal results at 32 experts.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Behind the News:&lt;/strong&gt; The scalability potential of MoE-Mamba is remarkable. Although the current study focuses on smaller models, the architecture suggests a promising avenue for handling larger, more complex models efficiently.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Why it Matters:&lt;/strong&gt; Given the efficiency in training and inference, MoE-Mamba shows promise for deployment in large-scale language models.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;We’re Thinking:&lt;/strong&gt; It is also not clear what are the scalability limits of MoE-Mamba, especially in comparison to existing large-scale models like GPT-3? Also, we would like to have a look a the code…&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;In Detail&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Integration of MoE layers into the Mamba architecture&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This design choice enables MoE-Mamba to leverage the conditional processing capabilities of MoE and the context integration of Mamba. By alternating between unconditional processing by the Mamba layer and conditional processing by a MoE layer, MoE-Mamba achieves a balance between efficient state compression and selective information retention.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*hsiJ04bKp4QspHpyFmrApw.png&quot; alt=&quot;Integration of MoE layers into the Mamba architecture&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Authors also investigate a unified Mamba module containg MoE&lt;/p&gt;

&lt;h2 id=&quot;arxiv-240109-u-mamba-enhancing-long-range-dependency-for-biomedical-image-segmentation&quot;&gt;Arxiv 24.01.09: &lt;strong&gt;U-Mamba: Enhancing Long-range Dependency for Biomedical Image Segmentation&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2401.04722&quot;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/bowang-lab/U-Mamba&quot;&gt;Code&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;#biomedical #segmentation #cv&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*RM4DAO5rm5n7kE79iWtw_w.png&quot; alt=&quot;U-Mamba: Enhancing Long-range Dependency for Biomedical Image Segmentation&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;This paper presents an innovative network architecture for biomedical image segmentation.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What’s new:&lt;/strong&gt; U-Mamba is a novel network integrating Mamba blocks, into a U-Net based architecture. This hybrid CNN-SSM structure enables modeling of long-range dependencies in images.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How it works:&lt;/strong&gt; U-Mamba follows an encoder-decoder structure, where each building block comprises Residual blocks followed by a Mamba block. This design captures both local features and long-range dependencies. The network’s self-configuring mechanism allows it to adapt automatically to various datasets.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Results:&lt;/strong&gt; The experiments across biomedical segmentation tasks show that U-Mamba outperforms the state-of-the-art CNN/Transformer-based networks in terms of segmentation accuracy by a small margin.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Behind the news:&lt;/strong&gt; The integration of Mamba blocks within a U-Net architecture represents an interesting integration of two architectures, highlighting the potential of State Space Models in this domain.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Why it matters:&lt;/strong&gt; Yet one more architecture smoothly integrating Mamba with good results.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;We’re thinking:&lt;/strong&gt; The U-Mamba network’s adaptability and performance set a new benchmark in medical image segmentation. This work might stimulate further exploration of hybrid architectures in medical image analysis.&lt;/p&gt;

&lt;p&gt;While U-Mamba shows some advantages over existing methods, the improvement margin in some cases appears modest. For example, in 3D organ segmentation, U-Mamba’s DSC scores are marginally higher than nnU-Net, which is one of the closest competitors.&lt;/p&gt;

&lt;p&gt;Authors do not support with numbers efficiency of the model, model size and comparison in that matter of the two architecture variants.&lt;/p&gt;

&lt;p&gt;Should we wait for U-Mamba perfomance for standart segmentation benchmarks such as &lt;a href=&quot;https://paperswithcode.com/sota/semantic-segmentation-on-ade20k&quot;&gt;ADE20K&lt;/a&gt; or PASCAL to decide U-Mamba generic value?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;In Detail&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;U-Mamba Architecture&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The U-Mamba architecture follows the encoder-decoder pattern of U-Net, known for its effectiveness in medical imaging.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Building Blocks:&lt;/strong&gt; Each block contains two successive Residual blocks followed by a Mamba block. The Residual block includes a plain convolutional layer, Instance Normalization, and Leaky ReLU.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Mamba Block:&lt;/strong&gt; It processes image features that are flattened and transposed, followed by Layer Normalization. The Mamba block has two parallel branches:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;The first branch&lt;/strong&gt; expands the features and processes them through a linear layer, a 1D convolutional layer, SiLU activation, and the SSM layer.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;The second branch&lt;/strong&gt; also expands the features, followed by SiLU activation. Features from both branches are then merged using the Hadamard product and projected back to their original shape.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Encoder and Decoder:&lt;/strong&gt; The U-Mamba encoder, captures both local features and long-range dependencies. The decoder focuses on local information and resolution recovery, using Residual blocks, transposed convolutions, and inherits skip connections from U-Net. The output is passed through a convolutional layer and a Softmax layer for the final segmentation probability map.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Variants:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;U-Mamba_Bot: Uses the U-Mamba block only in the bottleneck.&lt;/li&gt;
  &lt;li&gt;U-Mamba_Enc: Employs the U-Mamba block in all encoder blocks.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;arxiv-240117-vision-mamba-efficient-visual-representation-learning-with-bidirectional-state-space-model&quot;&gt;Arxiv 24.01.17: &lt;strong&gt;Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2401.09417``&quot;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/hustvl/Vim&quot;&gt;Code&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;#computervision #classification #segmentation&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*D6KVLVs8MJYnd_DBCDUKuw.png&quot; alt=&quot;Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Vim utilizes bidirectional State Space Models (SSMs) to process image sequences.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What’s New&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Vim proposes a pure-SSM-based method for vision tasks, differing from self-attention-based models.&lt;/li&gt;
  &lt;li&gt;Incorporation of &lt;strong&gt;bidirectional SSMs&lt;/strong&gt; for efficient visual data processing.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;How It Works&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Vim transforms images into sequences of flattened 2-D patches, applying bidirectional SSM.&lt;/li&gt;
  &lt;li&gt;The system uses a combination of position embeddings and bidirectional state space models for visual data processing.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;On tasks like ImageNet classification Vim outperforms established models like DeiT and ViTs in terms of accuracy with smaller size.&lt;/li&gt;
  &lt;li&gt;Authors compare also segmentation UperNet framework with Vim, DeiT and ResNet on ADE20k with similar conclusion.&lt;/li&gt;
  &lt;li&gt;For object detection benchmark Vim slighly outperforms DeiT in the scope of Cascade Mask R-CNN on COCO.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Behind the News:&lt;/strong&gt; Vim challenges the dominance of self-attention in visual representation, offering an alternative that’s more efficient in handling large-scale and high-resolution datasets.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Why It Matters:&lt;/strong&gt; Vim’s efficiency in processing high-resolution images makes it a promising backbone for future vision foundation models.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;We’re Thinking:&lt;/strong&gt; Will Vim show its efficiency/accuracy across other frameworks than UperNet or Cascade Mask R-CNN ?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;In Detail&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Bidirectional State Space Model&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;It processes the visual data both forward and backward, unlike traditional unidirectional models.&lt;/li&gt;
  &lt;li&gt;This bidirectionality allows for more robust capturing of visual contexts and dependencies, particularly in dense prediction tasks.&lt;/li&gt;
  &lt;li&gt;The model effectively compresses the visual representation, leveraging position embeddings to maintain spatial awareness.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;arxiv-240118-vmamba-visual-state-space-model&quot;&gt;Arxiv 24.01.18 : &lt;strong&gt;VMamba: Visual State Space Model&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2401.10166&quot;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/MzeroMiko/VMamba&quot;&gt;Code&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;#computervision #classification #segmentation #detection&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;VMamba&lt;/strong&gt; presents new approach in visual representation learning. It merges CNNs’ and ViTs’ strengths and does not have their limitations.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What’s new:&lt;/strong&gt; VMamba has a unique architecture: integrates global receptive fields and dynamic weights within a linear computational complexity framework.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How it works:&lt;/strong&gt; The model is based on a &lt;strong&gt;Cross-Scan Module (CSM)&lt;/strong&gt;. This module processes visual data in &lt;strong&gt;four directions&lt;/strong&gt;. This ensures global information integration without increased complexity. The 2D Selective Scan combines CMS with S6 mamba block and merge output features creating 2D feature map.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Results:&lt;/strong&gt; VMamba shows significant performance improvements in different task : image classification, object detection, and semantic segmentation. For instance, it surpasses established benchmarks like ResNet and Swin in ImageNet-1K classification. In COCO object detection, VMamba models outperform their counterparts in mean Average Precision (mAP) and mean Intersection over Union (mIoU).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Behind the news:&lt;/strong&gt; VMamba maintains high performance across various input image sizes, which can indicate a robustness to changes in input conditions. This feature is crucial for practical applications where image resolutions can vary significantly.&lt;/p&gt;

&lt;p&gt;It dethrones Vim just the next day of Vim publication.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Why it matters:&lt;/strong&gt; VMamba’s approach combines the strengths of CNNs and ViTs and on the top it is computationally efficient.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*F7uFR9RPyq4bDvrO4a3Ycg.png&quot; alt=&quot;VMamba: Visual State Space Model&quot; /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*yK0hfDpKYFnGk0NupiBHXg.png&quot; alt=&quot;VMamba has a unique architecture: integrates global receptive fields and dynamic weights within a linear computational complexity framework&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;We’re thinking:&lt;/strong&gt; While VMamba’s results are promising, its practical applicability in diverse real-world scenarios require time verification. For data scientists, VMamba holds a promise of efficient processing of large-scale image datasets and a new playground.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;In Detail&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;VMamba’s 2D Selective Scan&lt;/strong&gt; operates by dynamically adjusting weights based on the importance of different areas in an image. This process involves an algorithm that assesses each pixel’s contribution to the target task. The scan prioritizes regions with higher information content, therefore it reduces computational load on less relevant areas. This method contrasts with traditional approaches where all pixels are treated equally. Which also leads to higher computational costs.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/1200/1*Ajt_aoa_ElU82EEXM63n2g.jpeg&quot; alt=&quot;VMamba’s 2D Selective Scan&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Vim vs VMamba&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;VMamba beats Vim on ImageNet-1k benchmark&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;VMamba-T with 22M params achieves 82.2 % acc while comparable VimS with 26M achieves 80.3% acc&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;VMamba beats Vim on COCO benchmark&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;the smalles model VMamba-T achieves APbox of 46.5 and APmask of 42.1 while Vim-T achives 45.7 and 39.2 respecitively&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;VMamba beats Vim on ADE20k benchmark&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;numbers are not directly comparable but Vmamaba seems to have better perfomance&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;arxiv-240124-segmamba-long-range-sequential-modeling-mamba-for-3d-medical-image-segmentation&quot;&gt;Arxiv 24.01.24 : &lt;strong&gt;SegMamba: Long-range Sequential Modeling Mamba For 3D Medical Image Segmentation&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2401.13560&quot;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/ge-xing/SegMamba&quot;&gt;Code&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;#segmentation #computervision&lt;/p&gt;

&lt;p&gt;This paper integrates the Mamba model with a U-shape structure for 3D medical image segmentation, aiming to efficiently process high-dimensional images.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What’s New&lt;/strong&gt;: SegMamba’s combines the Mamba model, known for handling long-range dependencies, with a U-shaped architecture, with application in 3D medical image segmentation.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How it Works&lt;/strong&gt;: SegMamba employs a Mamba encoder, a 3D decoder, and skip-connections. The Mamba encoder, with depth-wise convolution and flattening operations, reduces computational load while handling high-dimensional features.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;: On the BraTS2023 dataset, SegMamba achieved best performance.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Why it Matters:&lt;/strong&gt; SegMamba’s is quite niche but confims again universality of Mamba block.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;We’re Thinking:&lt;/em&gt; Questions arise about SegMamba’s applicability to various medical imaging forms and datasets, and its comparison with other state-of-the-art methods.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;In Detail&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;SegMamba vs Umamba&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The key difference is in their approach to enhancing the U-Net architecture. SegMamba emphasizes the Mamba model for 3D segmentation, while U-Mamba combines CNNs with SSMs for versatile segmentation tasks.&lt;/p&gt;

&lt;h2 id=&quot;arxiv-240124-mambabyte-token-free-selective-state-space-model&quot;&gt;Arxiv 24.01.24: &lt;strong&gt;MambaByte: Token-free Selective State Space Model&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2401.13660&quot;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/kyegomez/MambaByte&quot;&gt;Code&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;#llm #tokenfree&lt;/p&gt;

&lt;p&gt;This model is token-free, directly learning from raw bytes and bypassing the biases inherent in subword tokenization.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What’s New&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;“MambaByte” is a token-free language model, a novel approach as it directly learns from raw bytes, no tokenization.&lt;/li&gt;
  &lt;li&gt;A unique perspective on efficiency and performance compared to other byte-level models.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;How It Works:&lt;/strong&gt; “MambaByte” slightly modfies Mamba module to accept raw bytes.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Results:&lt;/strong&gt; The model surpasses the performance of state-of-the-art subword Transformers with lower computational resources. It demonstrates linear scaling in sequence length, leading to faster inference times.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Behind the News:&lt;/strong&gt; “MambaByte” proposes an alternative to autoregressive Transformers, libertaing us from tokenization.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Why It Matters:&lt;/strong&gt; This efficient byte sequence processing opens new avenues for language models in large-scale and diverse applications.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;We’re Thinking:&lt;/strong&gt; This paper shows a potential for token free llm learning. New applications outside of LLM should come soon.&lt;/p&gt;

&lt;h2 id=&quot;arxiv-240125-vivim-a-video-vision-mamba-for-medical-video-object-segmentation&quot;&gt;Arxiv 24.01.25 &lt;strong&gt;Vivim: a Video Vision Mamba for Medical Video Object Segmentation&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2401.14168&quot;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/scott-yjyang/Vivim&quot;&gt;Code&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;#video #biomedical&lt;/p&gt;

&lt;p&gt;A framework for medical video object segmentation, focusing on addressing challenges in long-sequence modeling in video analysis. It uses a Temporal Mamba Block, which allows the model to obtain excellent segmentation results with improved speed performance compared to existing methods.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*uqxFkqdGvoi3vf7opR57mQ.png&quot; alt=&quot;Vivim: a Video Vision Mamba for Medical Video Object Segmentation&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What’s New:&lt;/strong&gt; Vivim integrates the Mamba model into a multi-level transformer architecture, transforming video clips into feature sequences containing spatiotemporal information at various scales.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How It Works&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The Temporal Mamba Block employs a sequence reduction process for efficiency, integrating a spatial self-attention module and a Mix-FeedForward layer.&lt;/li&gt;
  &lt;li&gt;The Mamba module explores correlations among patches of input frames, while a Detail-specific FeedForward preserves fine-grained details.&lt;/li&gt;
  &lt;li&gt;A lightweight CNN- based decoder head integrates multi-level feature sequences to predict segmentation masks.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Results:&lt;/strong&gt; Vivim demonstrates superior performance on the breast US dataset outperforming existing video- and image-based segmentation methods.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Why It Matters:&lt;/strong&gt; Vivim’s approach represents a significant advancement in medical video analysis, particularly for tasks like lesion segmentation in ultrasound videos.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;We’re Thinking:&lt;/strong&gt; What are the potential limitations or challenges in scaling Vivim for broader clinical applications? The paper focuses solely on breast ultrasound videos, which may limit generalizability.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;In Detail:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Temporal Mamba Block&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;This block starts with a spatial self-attention module for extracting spatial features, followed by a Mix-FeedForward layer.&lt;/li&gt;
  &lt;li&gt;For temporal modeling, it transposes and flattens the spatiotemporal feature embedding into a 1D long sequence.&lt;/li&gt;
  &lt;li&gt;The Mamba module within the block tackles the correlation among patches in input frames, while the Detail-specific FeedForward focuses on preserving fine-grained details through depth-wise convolution.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;arxiv-240125-mambamorph-a-mamba-based-backbone-with-contrastive-feature-learning-for-deformable-mr-ct-registration&quot;&gt;Arxiv 24.01.25: &lt;strong&gt;MambaMorph: a Mamba-based Backbone with Contrastive Feature Learning for Deformable MR-CT Registration&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2401.13934.pdf&quot;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/guo-stone/mambamorph&quot;&gt;Code&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;#biomedical&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*QNfM9VLXeYFlpF57yU69Ag.png&quot; alt=&quot;MambaMorph: a Mamba-based Backbone with Contrastive Feature Learning for Deformable MR-CT Registration&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;A multi-modality deformable registration network designed specifically for aligning Magnetic Resonance (MR) and Computed Tomography (CT) images.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What’s New:&lt;/strong&gt; MambaMorph combines Mamba blocks with a feature extractor.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How It Works:&lt;/strong&gt; MambaMorph integrates a Mamba-based backbone with contrastive feature learning for deformable MR-CT registration.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Mamba-Based Registration Module: This module utilizes the Mamba blocks for efficient handling and processing of high-dimensional imaging data.&lt;/li&gt;
  &lt;li&gt;Contrastive Feature Learning: a feature extractor that employs supervised contrastive learning. This is designed to learn fine-grained, modality-specific features from the MR and CT images.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;: MambaMorph demonstrates superior performance over existing methods in MR-CT registration, showing improvements in accuracy and efficiency.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Behind the News:&lt;/strong&gt; The development of MambaMorph is a significant step in addressing the prevalent issues in multi-modality image registration, particularly in the context of MR and CT images.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Why It Matters:&lt;/strong&gt; The success of MambaMorph in MR-CT image registration has significant implications for medical imaging analysis.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;We’re Thinking:&lt;/strong&gt; While MambaMorph shows promising results, questions remain about its applicability to other forms of medical imaging and its performance in varied clinical scenarios.&lt;/p&gt;

&lt;footer&gt;
  &lt;p&gt;Exported from &lt;a href=&quot;https://medium.com&quot;&gt;Medium&lt;/a&gt; on Feb 03
    2024.&lt;/p&gt;
  &lt;p&gt;&lt;a href=&quot;https://medium.com/@ulalaparis/the-mamba-effect-mamba-models-gaining-ground-f2d2c9b9245c&quot;&gt;View
      the original&lt;/a&gt;&lt;/p&gt;
&lt;/footer&gt;
&lt;script type=&quot;text/javascript&quot; src=&quot;//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-584ec4ce89deed84&quot;&gt;&lt;/script&gt;

&lt;div class=&quot;addthis_inline_share_toolbox&quot;&gt;&lt;/div&gt;

</description>
        <pubDate>Sat, 03 Feb 2024 00:00:00 +0100</pubDate>
        <link>http://urszulaczerwinska.github.io/thoughts/the-mamba-effect-mamba-models-gaining-ground/</link>
        <guid isPermaLink="true">http://urszulaczerwinska.github.io/thoughts/the-mamba-effect-mamba-models-gaining-ground/</guid>
        
        
        <category>thoughts</category>
        
      </item>
    
      <item>
        <title>From PCA to SSL - A personal odyssey in Data Science</title>
        <description>&lt;head&gt;
  &lt;meta name=&quot;description&quot; content=&quot;A reflection on my personal journey through the data science field, tracing the evolution from fundamental techniques like Principal Component Analysis (PCA) and Independent Component Analysis (ICA) to modern Self-Supervised Learning (SSL). This post explores how foundational concepts have influenced my career and the field&apos;s progress.&quot; /&gt;
&lt;/head&gt;

&lt;p&gt;Welcome to a journey through the evolving landscape of Data Science, a journey that parallels my own academic and professional path. In this article, I will share how fundamental concepts like Principal Component Analysis (PCA) and Independent Component Analysis (ICA) have been instrumental in leading me to the field of Self-Supervised Learning (SSL).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;So, why share this story?&lt;/strong&gt; It’s not merely a chronicle of progress in Data Science; it’s a testament to the profound synergy between the foundational theories I mastered during my doctoral studies and the avant-garde techniques I am navigating now. This path hasn’t just shaped my career; it has fundamentally altered my perspective on the potential and direction of Data Science.&lt;/p&gt;

&lt;p&gt;I hope it can inspire you, dear reader, that fundamental concepts are reborn under a different form.&lt;/p&gt;

&lt;p&gt;We will start by looking at the basics of PCA and ICA, the building blocks of my entry into data analysis. From there, we’ll see how these methods have evolved into today’s more dynamic and autonomous SSL approaches.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What lessons did I learn in this transition? How do these experiences mirror the broader changes in the field of Data Science?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This exploration aims to provide a deeper understanding of how the field is evolving and what it means to us as practitioners and enthusiasts.&lt;/p&gt;

&lt;h2 id=&quot;my-roots-in-unsupervised-learning&quot;&gt;My roots in Unsupervised Learning&lt;/h2&gt;

&lt;h3 id=&quot;discovering-pca-and-ica-during-myphd&quot;&gt;Discovering PCA and ICA during my PhD&lt;/h3&gt;

&lt;h4 id=&quot;beginning-my-journey-in-unsupervised-learning&quot;&gt;&lt;strong&gt;Beginning my journey in Unsupervised Learning&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;My initial encounter with unsupervised learning occurred during my undergraduate studies, deepening significantly through my P&lt;a href=&quot;https://urszulaczerwinska.github.io/UCzPhDThesis/&quot;&gt;h.D. research at the Curie Institute.&lt;/a&gt; This experience wasn’t just academic; it represented my first foray into applying complex data science concepts to real-world biological data. It was here that I delved into Principal Component Analysis (PCA) and Independent Component Analysis (ICA), exploring their capabilities in a practical, research-driven environment.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*zuJ046QrOnct_PPF.jpg&quot; alt=&quot;Graphical representation of Principal Component Analysis (PCA) and Independent Component Analysis (ICA) in the context of data science, illustrating their application to complex biological data.&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;h4 id=&quot;applying-pca-and-ica-to-transcriptomic-analysis&quot;&gt;&lt;strong&gt;Applying PCA and ICA to transcriptomic analysis&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;In my early systems bilolgy research days, PCA was a fundamental tool for analyzing gene expression data. Its primary function was to reduce the dimensionality of large datasets, allowing me to identify and focus on the most significant variables in the data. This method was crucial for managing the complexity of transcriptomes and extracting meaningful insights from vast amounts of data.&lt;/p&gt;

&lt;p&gt;ICA, in contrast, served a different but equally vital role. It was instrumental in separating mixed signals in the data, enabling the identification of independent sources of variation. This technique was particularly useful in dissecting a complex gene expression pattern in tissue representing a mix of normal, cancer and immune cells, allowing for a clearer understanding of the underlying biological processes of an immune response to immunotherapy.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*kTN_M6iax9U6zrqK.png&quot; alt=&quot;Visualization of Principal Component Analysis (PCA) and Independent Component Analysis (ICA) applied to gene expression data, highlighting their roles in reducing dimensionality and identifying independent sources of variation in transcriptomic analysis.&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;h4 id=&quot;navigating-the-maze-challenges-and-limitations&quot;&gt;&lt;strong&gt;Navigating the maze: challenges and limitations&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*WZpscmnhCoIiTCHOaggWbA.png&quot; alt=&quot;Diagram illustrating the challenges and limitations of PCA and ICA, including issues with linear assumptions and component stability in the analysis of biological data.&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Despite their utility, both PCA and ICA have limitations, especially when applied to biological data. One major limitation of PCA is its inherent assumption of linear relationships and multivariate normal distribution of data. However, biological processes are often intricate and non-linear, and microarray gene expression measurements, for instance, tend to follow a super-Gaussian distribution, not accurately &lt;a href=&quot;https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-13-24&quot;&gt;captured by PCA&lt;/a&gt;​​. This limitation becomes particularly evident when the biological questions at hand are not directly related to the highest variance in the data, a fundamental aspect of PCA’s data decomposition approach.&lt;/p&gt;

&lt;p&gt;ICA, while effective in separating mixed signals and identifying non-Gaussian components, also presents challenges. Its results can be unstable and dependent on the number of components extracted. This is compounded by the fact that ICA does not inherently order its components by relevance, necessitating multiple runs and averaging of results to obtain robust outcomes. The high dimensionality of biological datasets often requires PCA to be used as a pre-processing step before applying ICA, which can further &lt;a href=&quot;https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-13-24&quot;&gt;complicate the analysis&lt;/a&gt;​​.&lt;/p&gt;

&lt;p&gt;Also, both approaches, belongs to the realm of statistical analysis. In a world of fast-paced innovation and deep learning, they seem covered with dust and old-fashioned.&lt;/p&gt;

&lt;p&gt;The challenges and insights gained from working extensively with ICA during my Ph.D. significantly contributed to my expertise in machine learning. Post-Ph.D., I ventured into the field of natural language processing, applying deep learning techniques to tackle complex linguistic data. My career then led me to Adevinta, where I expanded my focus to include computer vision, leveraging deep learning in new and innovative ways. This diverse experience in ML and DL paved the way for my current exploration into Self-Supervised Learning (SSL), marking a continuous journey through the evolving landscape of artificial intelligence.&lt;/p&gt;

&lt;h4 id=&quot;transitioning-to-self-supervised-learning&quot;&gt;&lt;strong&gt;Transitioning to Self-Supervised Learning&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;These experiences with PCA and ICA were more than just academic exercises; they were foundational in shaping my approach to data analysis. They prepared me for the another phase of my career in Self-Supervised Learning (SSL), where I would apply the principles of unsupervised learning to even more complex and dynamic datasets, such as image datasets. &lt;strong&gt;The core idea of finding robust abstraction of a data object (image, dataset, text) remains common to PCA and SSL.&lt;/strong&gt; This non-linear transition marked a significant shift from analyzing biological data to exploring the frontiers of artificial intelligence, but at the same time it is very connected in an intriguing way.&lt;/p&gt;

&lt;h2 id=&quot;the-evolution-of-unsupervised-learning&quot;&gt;&lt;strong&gt;The Evolution of Unsupervised Learning&lt;/strong&gt;&lt;/h2&gt;

&lt;h3 id=&quot;from-pca-and-ica-to-deep-learning&quot;&gt;&lt;strong&gt;From PCA and ICA to Deep Learning&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;The evolution from traditional unsupervised methods like PCA and ICA to Self-Supervised Learning (SSL) in Deep Learning is a significant development in the field of machine learning. While PCA and ICA were pivotal in their time for dimensionality reduction and signal separation, they have limitations with non-linear, high-dimensional data structures common in modern datasets.&lt;/p&gt;

&lt;p&gt;The advent of SSL in Deep Learning has revolutionized our approach to data. SSL, unlike its predecessors, leverages deep neural networks to learn from unlabeled data, extracting complex patterns and features without the need for explicit annotations. This advancement not only overcomes the constraints of labeled data but also opens new possibilities in diverse domains, ranging from natural language processing to computer vision.&lt;/p&gt;

&lt;p&gt;SSL represents a paradigm shift, offering a more nuanced and comprehensive understanding of data. It signifies the ongoing evolution and sophistication of machine learning techniques, marking a new era in the exploration and utilization of data.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;“If intelligence was a cake, unsupervised learning would be the cake, supervised learning would be the icing on the cake, and reinforcement learning would be the cherry on the cake.”&lt;/em&gt; — Yann LeCun&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;in-2019-yann-lecun-updated-the-above-quote-changing-unsupervised-learning-to-self-supervised-learning-and-in-2020-he-declared-that-self-supervised-learning-ssl-is-the-future-of-machine-learning-source&quot;&gt;&lt;strong&gt;In 2019, Yann LeCun updated the above quote, changing &lt;em&gt;“unsupervised learning”&lt;/em&gt; to “&lt;/strong&gt;&lt;a href=&quot;https://www.eyerys.com/articles/people/1560388243/opinions/the-next-ai-revolution-will-not-be-supervised?ref=blog.salesforceairesearch.com&quot;&gt;&lt;strong&gt;&lt;em&gt;self-supervised learning&lt;/em&gt;&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;&lt;em&gt;,”&lt;/em&gt; and in 2020 he declared that self-supervised learning (SSL) is &lt;em&gt;the future of machine learning (&lt;/em&gt;&lt;/strong&gt;&lt;a href=&quot;https://blog.salesforceairesearch.com/learning-vision-without-labels/&quot;&gt;&lt;strong&gt;&lt;em&gt;source&lt;/em&gt;&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;&lt;em&gt;)&lt;/em&gt;.&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*4QwOfU5uUYZrliLz.png&quot; alt=&quot;Image featuring a quote by Yann LeCun about the future of machine learning, emphasizing the importance of Self-Supervised Learning (SSL) in the field.&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;h2 id=&quot;introduction-to-self-supervised-learningssl&quot;&gt;Introduction to Self-Supervised Learning (SSL)&lt;/h2&gt;

&lt;h4 id=&quot;what-is-ssl&quot;&gt;&lt;strong&gt;What is SSL?&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;Self-Supervised Learning (SSL) represents the latest frontier in this evolutionary journey. SSL, in a sense, is a clever workaround to the challenge of labeled data scarcity. It leverages the data itself to generate its own supervision. Think of it as a student who, instead of relying solely on a teacher’s guidance, learns by exploring and questioning the world around them.&lt;/p&gt;

&lt;p&gt;The system learns to understand and work with data by creating its own labels from the inherent structure of the data. This is a significant departure from traditional supervised learning, where models are trained on a dataset explicitly labeled by humans.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*W6QT1xPyNisB2ptcmN8i6w.png&quot; alt=&quot;Diagram or visualization explaining the concept of Self-Supervised Learning (SSL), illustrating how the model generates its own supervision from the data.&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;For instance, in image processing, an SSL algorithm might learn to predict missing parts of an image, or in text processing, to predict the next word in a sentence. Through these tasks, the model gains an intrinsic understanding of the structure and context of the data.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*Gz_gg4DZBQzd72sKpiphmA.png&quot; alt=&quot;Illustration of an SSL algorithm in action, such as predicting missing parts of an image or the next word in a sentence, demonstrating how Self-Supervised Learning models understand data structure.&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Self-Supervised Learning, a subset of unsupervised learning, has evolved dramatically, introducing various families of models, each with its unique approach to learning from unlabeled data. Here are some of the primary &lt;a href=&quot;https://arxiv.org/abs/2301.05712&quot;&gt;families of SSL algorithms&lt;/a&gt;:&lt;/p&gt;

&lt;h4 id=&quot;1-contrastive-learning-models&quot;&gt;&lt;strong&gt;1. Contrastive Learning Models:&lt;/strong&gt;&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Principle: These models learn representations by contrasting positive pairs (similar or related data points) against negative pairs (dissimilar or unrelated data points).&lt;/li&gt;
  &lt;li&gt;SimCLR (Simple Framework for Contrastive Learning of Visual Representations): Utilizes a simple contrastive learning framework for visual representations.&lt;/li&gt;
  &lt;li&gt;MoCo (Momentum Contrast for Unsupervised Visual Representation Learning): Focuses on building dynamic dictionaries for contrastive learning in vision.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;2-predictive-learning-models&quot;&gt;&lt;strong&gt;2. Predictive Learning Models:&lt;/strong&gt;&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Principle: These models predict some parts of the data using other parts, thereby learning useful representations.&lt;/li&gt;
  &lt;li&gt;BERT (Bidirectional Encoder Representations from Transformers): Predicts missing words in a sentence, gaining contextual understanding in NLP.&lt;/li&gt;
  &lt;li&gt;GPT (Generative Pretrained Transformer): Predicts the next word in a sequence, learning sequential and contextual patterns in text.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;3-generative-models&quot;&gt;&lt;strong&gt;3. Generative Models:&lt;/strong&gt;&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Principle: These models learn to generate or reconstruct data, thereby understanding the distribution and structure of the dataset.&lt;/li&gt;
  &lt;li&gt;VAE (Variational Autoencoders): Learns to reconstruct input data, capturing the probabilistic distribution.&lt;/li&gt;
  &lt;li&gt;GANs (Generative Adversarial Networks): Involves a generator and a discriminator learning in a competitive manner to produce realistic data.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;4-clustering-based-models&quot;&gt;&lt;strong&gt;4. Clustering-Based Models:&lt;/strong&gt;&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Principle: This approach involves clustering data points and learning representations that respect these cluster assignments.&lt;/li&gt;
  &lt;li&gt;DeepCluster: Utilizes clustering of features and subsequent representation learning.&lt;/li&gt;
  &lt;li&gt;SwAV (Swapping Assignments between Views): Employs a unique approach where it clusters data points and enforces consistency between cluster assignments of different augmented views of the same image.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;5-transformation-recognition-models&quot;&gt;&lt;strong&gt;5. Transformation Recognition Models:&lt;/strong&gt;&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Principle: These models learn by recognizing transformations applied to the input data.&lt;/li&gt;
  &lt;li&gt;Jigsaw Puzzles as a task: The model learns by solving jigsaw puzzles, essentially recognizing spatial relations and transformations.&lt;/li&gt;
  &lt;li&gt;RotNet: Involves learning by recognizing the rotation applied to images.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Each of these families represents a different angle of approaching the challenge of learning from unlabeled data. Self-Supervised Learning (SSL) marks a significant evolution in machine learning, especially in the realms of deep learning. It transforms the challenge of learning from unlabeled data into an opportunity. In practice, SSL has been revolutionary, particularly in fields like natural language processing and computer vision, where it has expanded the boundaries of machine learning applications.&lt;/p&gt;

&lt;h4 id=&quot;limitations&quot;&gt;&lt;strong&gt;Limitations&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;SSL’s transformative potential is undeniable, but its journey to widespread industry adoption is hindered by several key challenges. The most notable is the high computational cost. Training SSL models demands significant resources, posing a barrier for smaller entities, as highlighted in industry analyses. Additionally, the technical complexity of SSL algorithms is a daunting hurdle, requiring deep expertise for effective implementation and task-specific fine-tuning.&lt;/p&gt;

&lt;p&gt;Data quality and variety are crucial for SSL effectiveness. In data-limited or sensitive industries, SSL models face difficulties in learning efficiently. Moreover, the industry lags in developing readily usable SSL frameworks, slowing down practical application despite rapid academic progress.&lt;/p&gt;

&lt;p&gt;Another critical aspect is the ethical and privacy implications of using large datasets essential for SSL. The industry must navigate this delicate balance to ensure ethical data utilization.&lt;/p&gt;

&lt;h4 id=&quot;bridge-between-pca-and-ssl&quot;&gt;&lt;strong&gt;Bridge between PCA and SSL&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;The intriguing aspect of SSL, especially in the context of contrastive learning (CL) and its relation to PCA, is highlighted in studies such as “&lt;a href=&quot;https://arxiv.org/pdf/2201.12680v2.pdf&quot;&gt;Deep Contrastive Learning is Provably (almost) Principal Component Analysis&lt;/a&gt;.” This research provides a novel perspective on CL, showing that with deep linear networks, the representation learning aspect of CL aligns closely with PCA’s principles​​. This connection underlines the evolutionary link from traditional statistical methods like PCA to advanced SSL techniques.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*_5M3NTt8CSpFtazdwIY_QQ.png&quot; alt=&quot;Diagram or illustration from research showing the relationship between Deep Contrastive Learning (CL) and Principal Component Analysis (PCA), highlighting the theoretical connection between these methods.&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;We provide a novel game-theoretical perspective of con- trastive learning (CL) over loss functions (e.g., InfoNCE) and prove that with deep linear network, the representation learning part is equivalent to Principal Component Analysis (PCA).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;By leveraging deep neural networks, CL in SSL transcends PCA’s linear constraints, enabling the extraction of complex, nonlinear relationships within data. This progression from PCA to SSL illustrates how foundational data science concepts continue to shape contemporary technologies. Understanding this link allows us to appreciate SSL, particularly in deep learning, as a modern interpretation of long-standing principles in data analysis.&lt;/p&gt;

&lt;p&gt;The transition from PCA and ICA to SSL represents a leap forward in our capacity to not just recognize but deeply comprehend patterns in data, opening new horizons in data science and beyond.&lt;/p&gt;

&lt;h2 id=&quot;linking-the-past-with-the-present-personal-perspective&quot;&gt;Linking the past with the present: personal perspective&lt;/h2&gt;

&lt;h3 id=&quot;my-encounter-withssl&quot;&gt;My encounter with SSL&lt;/h3&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*WKF4iR5aQVEX9Mv73dsNPQ.png&quot; alt=&quot;Image representing the personal journey into Self-Supervised Learning (SSL), reflecting the transition from traditional methods like PCA and ICA to advanced SSL techniques in data science&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;My journey into the realm of Self-Supervised Learning (SSL) was like stepping into a new world, yet one that felt strangely familiar. I first encountered SSL while expanding my horizons in the ever-evolving landscape of data science. Coming from a background heavily influenced by PCA and ICA in computational biology, the leap to SSL was both intriguing and formidable.&lt;/p&gt;

&lt;p&gt;Initially, SSL seemed like a puzzle. It promised a more nuanced understanding of data without the explicit need for labels, a concept that was both challenging and exciting and above all very familiar. This resonated with my earlier work where we often grappled with unlabeled biological datasets. SSL’s approach of learning from the data itself, finding patterns, and using them to build more robust models, was a game changer. It was like watching the evolution of my previous work in PCA and ICA but on a more intricate and expansive scale.&lt;/p&gt;

&lt;h3 id=&quot;insights-from-the-recent-sotaarticles&quot;&gt;&lt;strong&gt;Insights from the recent SOTA articles&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;The insights from ICCV 2023, particularly the &lt;a href=&quot;https://bigmac-vision.github.io/&quot;&gt;BigMAC workshop,&lt;/a&gt; illuminated the recent strides in SSL. The workshop’s focus on large model adaptation for computer vision highlighted the challenges and opportunities arising from the increasing size and complexity of neural networks, especially in adapting them to novel tasks and domains​​.&lt;/p&gt;

&lt;p&gt;Key talks such as Neil Houlsby’s on “&lt;a href=&quot;https://www.youtube.com/watch?v=ZwtMEF0u5cM&quot;&gt;Advances in Visual Pretraining for LLMs&lt;/a&gt;” emphasized the scalability of Vision Transformers and their growing visual capabilities, marking significant progress in visual pre-training​​. I&lt;a href=&quot;https://arxiv.org/abs/2111.06377&quot;&gt;shan Misra’s discussion&lt;/a&gt; on leveraging SSL for scaling multimodal models demonstrated how SSL can enhance model efficiency and improve foundational multimodal models, even in the context of scarce paired data​​.&lt;/p&gt;

&lt;p&gt;The session on fine-tuning pretrained models revealed crucial insights into mitigating feature distortion, a challenge often overlooked but vital for preserving the robustness and accuracy of models​​. Additionally, talks on controlling large-scale text-to-image diffusion models like DALL-E 2 and Imagen offered perspectives on enhancing user control in the generation process, blending training-time and inference-time techniques​​.&lt;/p&gt;

&lt;p&gt;These developments at ICCV 2023 underscored SSL’s dynamic evolution and its transition from traditional PCA and ICA methodologies to more sophisticated, nuanced models capable of deeper and more intuitive data understanding.&lt;/p&gt;

&lt;h3 id=&quot;the-future-of-multimodal-data-alignment-inbiology&quot;&gt;The future of multimodal data alignment in Biology&lt;/h3&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*SodBq0lnsOvDWBNvSGjCVQ.png&quot; alt=&quot;Diagram or illustration related to the alignment of multimodal biological data using advanced AI techniques, showcasing concepts discussed at ICCV 2023 and models like I-JEPA.&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;One of the most exciting prospects brought to light at &lt;a href=&quot;https://openaccess.thecvf.com/ICCV2023?day=2023-10-04&quot;&gt;ICCV 2023&lt;/a&gt;, a conference rich in innovative ideas as evident in its proceedings, was the potential of AI in aligning multimodal biological data. The discussions led by pioneers like Yann LeCun, whose publications and &lt;a href=&quot;https://www.youtube.com/playlist?list=PL80I41oVxglK--is17UhoHVosOLFEJzKQ&quot;&gt;talks&lt;/a&gt; have been a personal beacon for me, particularly on AI models like &lt;a href=&quot;https://ai.meta.com/blog/yann-lecun-ai-model-i-jepa/&quot;&gt;I-JEPA&lt;/a&gt; for different modalities, opened up a new realm of possibilities. This approach could revolutionize how we handle complex biological datasets, like transcriptomes, microscopy images, immunoscores, single-cell data, and proteomics.&lt;/p&gt;

&lt;p&gt;The idea of integrating these diverse data types using advanced SSL techniques is not just a technological leap; it’s a new way of thinking about biological research. Reflecting on my journey, where resources like “&lt;a href=&quot;https://www.nature.com/articles/d41586-018-02174-z&quot;&gt;Deep Learning for Biology&lt;/a&gt;” have been instrumental in understanding the application of deep learning in biology, I see how we’re transitioning from isolated data types to a holistic, interconnected understanding of life’s complexities. Likewise, the survey ‘&lt;a href=&quot;https://arxiv.org/abs/1705.09406&quot;&gt;Multimodal Machine Learning: A Survey and Taxonomy&lt;/a&gt;’ by Morency et al. has been enlightening in understanding the methodologies of multimodal data integration. These readings have not only informed my professional growth but have also mirrored the evolution of data science itself — from focusing on singular data types to embracing an integrated, multi-dimensional approach that resonates with my own professional evolution and reaffirms my belief in the transformative power of data science.&lt;/p&gt;

&lt;h2 id=&quot;practical-implications-and-future-directions&quot;&gt;Practical implications and future directions&lt;/h2&gt;

&lt;h3 id=&quot;applying-ssl-in-my-currentprojects&quot;&gt;Applying SSL in my current projects&lt;/h3&gt;

&lt;p&gt;Integrating Self-Supervised Learning (SSL) into my work at Adevinta has been akin to embarking on an exhilarating expedition into uncharted territories of computer vision. Reding about models like &lt;a href=&quot;https://betterprogramming.pub/dinov2-the-new-frontier-in-self-supervised-learning-b3a939f6d533&quot;&gt;DINO v2&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/2111.06377&quot;&gt;masked autoencoders&lt;/a&gt;, the versatility and transformative power of these tools continually astound me. In my role, which orbits primarily around vision classifiers and deep learning frameworks like TensorFlow and PyTorch, SSL has emerged not just as a tool but as a concept, enabling me to push for an operational paradigm shift for data users and ML/DS teams.&lt;/p&gt;

&lt;h3 id=&quot;the-boundless-horizon-ofssl&quot;&gt;The Boundless Horizon of SSL&lt;/h3&gt;

&lt;p&gt;Peering into the future, I envision SSL as the an important advancement. Its prowess in harnessing the untapped potential of unlabeled data heralds a new era of possibilities. Imagine a world where SSL is the norm in data science, especially in data-rich yet label-poor realms like healthcare and finance. The implications are monumental.&lt;/p&gt;

&lt;p&gt;Inspired by visionaries like Yann LeCun, the prospect of SSL in aligning disparate biological data types — from the intricate patterns of transcriptomics to the dynamic landscapes of proteomics — is not just exciting; it’s revolutionary. It’s akin to finding a Rosetta Stone in a sea of biological data, offering us a more integrated and nuanced understanding of complex systems.&lt;/p&gt;

&lt;h3 id=&quot;the-vanguard-of-computer-vision-ssls-transformative-role&quot;&gt;The vanguard of Computer Vision: SSL’s transformative role&lt;/h3&gt;

&lt;p&gt;The realm of computer vision (CV) is undergoing a metamorphosis, thanks to SSL. The ICCV 2023 conference in Paris was a revelatory experience for me, showcasing the boundless potential of SSL in CV. It’s not just about big models and fine-tuning; it’s a paradigm shift in how we interact with visual data. Think of SSL combined with weak supervision as the new alchemists in the world of visual understanding.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://medium.com/adevinta-tech-blog/foundation-models-a-new-vision-for-e-commerce-76904a3066e8&quot;&gt;Foundation models&lt;/a&gt; in SSL, these behemoths of pre-trained knowledge, are poised to redefine tasks from image classification to the more intricate challenges of visual reasoning. Their true magic lies in their chameleon-like ability to adapt to specific tasks, offering unparalleled versatility and efficiency.&lt;/p&gt;

&lt;h3 id=&quot;ssl-the-dawn-of-a-newera&quot;&gt;SSL, the dawn of a new era&lt;/h3&gt;

&lt;p&gt;The journey of SSL is just beginning. Its evolution promises to be more than just incremental; it’s set to be revolutionary, reshaping how we approach, analyze, and derive meaning from data. The future of SSL, particularly in computer vision and the broader landscape of data science, is not just brimming with promise — it’s poised to be a transformative force.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;As I reflect on this odyssey from the structured realms of PCA and ICA during my Ph.D. at the Curie Institute, to the explorative and innovative universe of Self-Supervised Learning (SSL), it feels like an enlightening journey, threading together various phases of my life. This transition symbolizes more than a shift in methodologies; it represents a personal evolution in understanding and harnessing the power of data.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*HMFH9LJvEuar1UqaLl8kww.png&quot; alt=&quot;Image symbolizing the journey from traditional PCA and ICA to the modern landscape of Self-Supervised Learning (SSL), reflecting the personal and professional evolution in understanding and applying data science methodologies.&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Throughout this journey, several &lt;strong&gt;key learnings&lt;/strong&gt; stand out. Firstly, the importance of foundational understanding — the principles of PCA and ICA have been crucial in grasping the nuances of SSL. Secondly, adaptability and continuous learning are not just beneficial but essential in the ever-evolving field of data science. Lastly, interdisciplinary collaboration, as I experienced in Paris, has been invaluable, teaching me that diverse perspectives often lead to groundbreaking innovations.&lt;/p&gt;

&lt;p&gt;Looking ahead, I foresee SSL playing a pivotal role in areas beyond its current applications. The integration of SSL with emerging technologies like quantum computing or augmented reality could open new frontiers in data analysis and interpretation. Additionally, as artificial intelligence becomes increasingly autonomous, SSL may become central in developing more intuitive, self-improving AI systems.&lt;/p&gt;

&lt;p&gt;For the data science industry, the implications are vast. SSL’s ability to leverage unlabeled data will become increasingly crucial as data volumes grow exponentially. This could lead to more efficient data processing and a deeper understanding of complex patterns, significantly benefiting sectors like healthcare, finance, and environmental science. Moreover, as SSL continues to evolve, it will likely drive a shift towards more sophisticated, nuanced data analysis methods across the industry.&lt;/p&gt;

&lt;p&gt;In my current role, exploring different facets of data science, including my recent involvement in computer vision at &lt;a href=&quot;https://adevinta.com/&quot;&gt;Adevinta&lt;/a&gt;, I’ve seen firsthand the transformative impact of SSL. It’s a vivid reminder that our expertise is always evolving, built upon the foundations of our past experiences.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;To my readers, I share this journey as an encouragement to embrace the vast and varied landscape of data science. Let your experiences guide and inspire you to explore new territories. The application of SSL across different fields, including my own explorations in computer vision, demonstrates the exciting potential of integrating past knowledge with cutting-edge innovations.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In the ever-changing world of data science, our greatest strength lies in our willingness to learn and adapt. The path of learning is endless, and I am eager to see where our collective curiosity and innovation will lead us next.&lt;/p&gt;

&lt;h3 id=&quot;recommended-reading&quot;&gt;Recommended reading:&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2301.05712&quot;&gt;https://arxiv.org/abs/2301.05712&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/2201.12680v2.pdf&quot;&gt;https://arxiv.org/pdf/2201.12680v2.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2305.13689&quot;&gt;https://arxiv.org/abs/2305.13689&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2305.00729&quot;&gt;https://arxiv.org/abs/2305.00729&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2111.06377&quot;&gt;https://arxiv.org/abs/2111.06377&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2301.08243&quot;&gt;https://arxiv.org/abs/2301.08243&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2304.07193&quot;&gt;https://arxiv.org/abs/2304.07193&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;my-relatedworks&quot;&gt;My related works&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;PCA : &lt;a href=&quot;http://bioinfo-out.curie.fr/projects/dedal/&quot;&gt;DeDaL&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;ICA: &lt;a href=&quot;https://urszulaczerwinska.github.io/DeconICA/&quot;&gt;DeconICA&lt;/a&gt; &amp;amp; &lt;a href=&quot;https://urszulaczerwinska.github.io/UCzPhDThesis/&quot;&gt;PhD Thesis&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/adevinta-tech-blog/foundation-models-a-new-vision-for-e-commerce-76904a3066e8&quot;&gt;Foundation models blog post&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://urszulaczerwinska.github.io/about/&quot;&gt;about me&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;footer&gt;
  &lt;p&gt;Exported from &lt;a href=&quot;https://medium.com&quot;&gt;Medium&lt;/a&gt; on Jan 03
    2024.&lt;/p&gt;
  &lt;p&gt;&lt;a href=&quot;https://medium.com/@ulalaparis/from-pca-to-ssl-a-personal-odyssey-in-data-science-ba41ef311c5b&quot;&gt;View
      the original&lt;/a&gt;&lt;/p&gt;
&lt;/footer&gt;
&lt;script type=&quot;text/javascript&quot; src=&quot;//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-584ec4ce89deed84&quot;&gt;&lt;/script&gt;

&lt;div class=&quot;addthis_inline_share_toolbox&quot;&gt;&lt;/div&gt;

</description>
        <pubDate>Wed, 03 Jan 2024 00:00:00 +0100</pubDate>
        <link>http://urszulaczerwinska.github.io/thoughts/from-pca-to-ssl-a-personal-odyssey-in-data-science</link>
        <guid isPermaLink="true">http://urszulaczerwinska.github.io/thoughts/from-pca-to-ssl-a-personal-odyssey-in-data-science</guid>
        
        
        <category>thoughts</category>
        
      </item>
    
      <item>
        <title>AI Foundation Models</title>
        <description>&lt;head&gt;
  &lt;meta name=&quot;description&quot; content=&quot;Discover how AI foundation models are revolutionizing e-commerce by enhancing product development, driving sustainability, and fostering collaboration. Explore the challenges and strategies for successful implementation.&quot; /&gt;
&lt;/head&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;&lt;em&gt;“Foundation models are models that are trained on broad data and can be adapted to a wide range of downstream tasks. (…) In choosing this term, we take ‘foundation’ to designate the function of these models: a foundation is built first and it alone is fundamentally unfinished, requiring (possibly substantial) subsequent building to be useful. ‘Foundation’ also conveys the gravity of building durable, robust, and reliable bedrock through deliberate and judicious action.”&lt;/em&gt;&lt;/strong&gt;
 — the Stanford Institute for Human-Centred AI founded the &lt;a href=&quot;https://arxiv.org/pdf/2108.07258.pdf&quot;&gt;Center for Research on Foundation Models&lt;/a&gt; (CRFM), 2021&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*PDC2WWQyZPpIcKZU&quot; alt=&quot;Members of the Cognition team at Adevinta discussing the latest trends in AI and computer vision.&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;In the competitive realm of digital commerce, embracing technological advancements is not a luxury but a necessity for maintaining success. Among ML tools, foundation models are emerging as a formidable force. But what are foundation models, and why have they become a focal point among technologists and business leaders?&lt;/p&gt;

&lt;p&gt;As members of ‘Cognition’, a team dedicated to computer vision at &lt;a href=&quot;https://adevinta.com/&quot;&gt;Adevinta&lt;/a&gt;, we are eager to share the insights we have gathered through our technology trends watch and recent attendance of the &lt;a href=&quot;https://iccv2023.thecvf.com/&quot;&gt;ICCV 2023&lt;/a&gt; conference. This practice not only ensures our internal services remain up to date but also improves the standards experienced by our users, enhancing the services provided to customers across &lt;a href=&quot;https://adevinta.com/our-brands&quot;&gt;Adevinta’s brands&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;what-are-foundation-models&quot;&gt;What are foundation models ?&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Foundation models&lt;/strong&gt; are a breed of artificial intelligence (AI) models pre-trained on a vast amount of data, laying a robust groundwork for further customisation on specific tasks.&lt;/p&gt;

&lt;p&gt;Unlike traditional machine learning models, which require from-scratch training or fine tuning for every new task, &lt;strong&gt;foundation models offer a substantial head start&lt;/strong&gt;. They have already learned a good deal from the data they were initially trained on, which includes recognising patterns, objects, and in the domain of computer vision, even understanding the semantics of a scene.&lt;/p&gt;

&lt;p&gt;Foundation models can be leveraged in various ways, each with its own balance of resource consumption and performance enhancement. The most resource-efficient method involves extracting features from an image, “freezing” them, and then using them directly as a zero-shot retrieval, classifier or detector. This zero-shot approach requires no further learning, allowing for immediate application.&lt;/p&gt;

&lt;p&gt;Alternatively, these embeddings can serve as inputs to other models, such as an MLP or an XGBoost classifier, through transfer learning. This strategy necessitates a minimal training dataset, yet it remains swift and cost-effective. &lt;a href=&quot;https://arxiv.org/abs/2209.07932&quot;&gt;Pastore et al&lt;/a&gt; reported that there can be &lt;strong&gt;10x to 100x speed increase&lt;/strong&gt; coupled with limited accuracy decrease (1–5% on average), depending on the dataset, when using a kernel classifier on top of frozen features. For a well-known CIFAR100 dataset, the authors observed 10x to 12x speed increase and −3.70% accuracy decrease. From our preliminary experiments, preparing for deploying image embedding services for Adevinta marketplaces, we noted a 5x to 10x speed increase with less than 3% accuracy drop for ImageNet1K dataset with Dinov2 frozen features compared to fine tuning a CNN backbone.&lt;/p&gt;

&lt;p&gt;For those seeking even greater performance enhancements, fine-tuning either the last layers or the entire network is an option. This process may demand a deeper understanding of machine learning and a larger dataset for model refinement, but can lead to substantial improvements. A key challenge in this approach is maintaining the model’s generalisability and preventing it from “forgetting” previously learned datasets.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*yvtVGxc_UkJgw2q6&quot; alt=&quot;A visual representation of the AI model development process, highlighting the efficiency gained by using foundation models.&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;To further enrich the model with bespoke data, one can explore self-supervised learning techniques for pre-training or distillation on domain-specific data. Moreover, to ensure the model remains current with new data, continuous learning methodologies can be employed. These advanced techniques not only enhance the model’s performance, but also tailor it more closely to specific business needs and data environments.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*2ccVFc0mWHKRw6L0&quot; alt=&quot;An illustration of the fine-tuning process in AI model development, emphasizing the balance between performance and resource use&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Beyond the singular applications of foundation models lies the potential for a transformative synergy. By harnessing models trained on diverse datasets with various loss functions, we can unlock new heights of performance. This approach was masterfully demonstrated by &lt;a href=&quot;https://arxiv.org/abs/2306.00984?utm_campaign=The%20Batch&amp;amp;utm_medium=email&amp;amp;_hsmi=281785463&amp;amp;utm_content=281787502&amp;amp;utm_source=hs_email&quot;&gt;Krishnan et al&lt;/a&gt;, who capitalised on images synthesised by Stable Diffusion. They adeptly trained another model (StableRep) using a contrastive loss approach and ViT backbone to achieve remarkable success in a classification benchmark. This strategy showcases the innovative fusion of generative and discriminative model capabilities, setting a new standard for adaptive and robust AI applications.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;&lt;em&gt;“Different foundation models understand different aspects of the world. It’s exciting that a large diffusion model, which is good at generating images, can be used to train a large vision transformer, which is good at analysing images!”&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;&lt;em&gt;—&lt;/em&gt;&lt;/strong&gt; &lt;a href=&quot;https://info.deeplearning.ai/openai-empowers-developers-ai-risk-in-the-spotlight-decoding-schizophrenic-language-synthetic-data-helps-image-classification-1?ecid=ACsprvum6jLSdI3_MtO8GVvBlJrsfj1iNuU7d7wJk3k6DmAu6jDwlmqxh0ZqOYQpd5P6T9SkgLDq&amp;amp;utm_campaign=The%20Batch&amp;amp;utm_medium=email&amp;amp;_hsmi=281785463&amp;amp;_hsenc=p2ANqtz-_AfPCrj7xxHY4m3H4td4jKSdynMLio8p3y-HqpQE0KbMIn5qoGh6dicKnKqf-6eVEcThLfdSR4_uMpwahHLcZqGQKfVg&amp;amp;utm_content=281787502&amp;amp;utm_source=hs_email&quot;&gt;The Batch @Deeplearning.ai newsletter&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;the-rise-of-foundation-models&quot;&gt;The rise of foundation models&lt;/h2&gt;

&lt;p&gt;The history of foundation models is closely tied to the rise of deep learning, particularly with the advent of large-scale models like GPT (Generative Pre-trained Transformer) by OpenAI and BERT (Bidirectional Encoder Representations from Transformers) by Google, which demonstrated the feasibility and effectiveness of pre-training models on vast datasets and then fine-tuning them for specific tasks.&lt;/p&gt;

&lt;p&gt;As technology advanced, so did the scale and capabilities of these models, with models like GPT-3, GPT-4 and T5 showcasing unprecedented levels of generalisation and adaptability across numerous domains including natural language processing, computer vision, and even multimodal tasks combining both vision and text. The success of these models started &lt;strong&gt;a new era where the focus shifted from training task-specific models from scratch to developing robust, versatile foundation models.&lt;/strong&gt; This new type of model could be fine-tuned or used in transfer-learning to excel at a broad spectrum of tasks. This shift not only catalysed significant advancements in AI research but also broadened adoption of AI across various industries, paving the way for more sophisticated and capable foundation models that continue to push the boundaries of what’s achievable with Artificial Intelligence.&lt;/p&gt;

&lt;p&gt;Notable examples of foundation models abound in the tech landscape. For instance, DINOv2 and MAE (Masked Autoencoder) by Meta AI for image understanding. On the other hand, models like CLIP and BLIP from OpenAI have shown the potential of bridging the gap between vision and language. These models, pre-trained on diverse and voluminous datasets, encapsulate a broad spectrum of knowledge that can be adapted for more specialised tasks, making them particularly advantageous for industries with data-rich environments like e-commerce.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*EHZAOKUjrq7Sz6uP&quot; alt=&quot;An illustration demonstrating the innovative use of foundation models to achieve advanced AI capabilities.&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Here is a short description of a few of those models:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;DINOv2:&lt;/strong&gt; Developed by Meta, &lt;a href=&quot;https://ai.meta.com/blog/dino-v2-computer-vision-self-supervised-learning/&quot;&gt;DINOv2&lt;/a&gt; is recognised for its self-supervised learning approach in training computer vision models, achieving significant results.&lt;/p&gt;

&lt;p&gt;The model underscores the potency of self-supervised learning in advancing computer vision capabilities​​.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Masked Autoencoders (&lt;/strong&gt;&lt;a href=&quot;https://arxiv.org/abs/2111.06377#:~:text=This%20paper%20shows%20that%20masked,based%20on%20two%20core%20designs&quot;&gt;&lt;strong&gt;MAE&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;):&lt;/strong&gt; MAE is a scalable self-supervised learning approach for computer vision that involves masking random patches of the input image and reconstructing the missing pixels.&lt;/p&gt;

&lt;p&gt;Meta AI demonstrated the effectiveness of MAE pre-pre training for billion-scale pretraining, combining self-supervised (1st stage) and weakly-supervised learning (2nd stage) for improved performance​.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2103.00020&quot;&gt;&lt;strong&gt;CLIP&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;(Contrastive Language-Image Pre-Training):&lt;/strong&gt; Developed by OpenAI, CLIP is a groundbreaking model that bridges computer vision and natural language processing, leveraging an abundantly available source of supervision: the text paired with images found across the internet.&lt;/p&gt;

&lt;p&gt;CLIP is the first multimodal model tackling computer vision, trained on a variety of (image, text) pairs, achieving competitive zero-shot performance on a variety of image classification datasets. It brings many of the recent developments from the realm of natural language processing into the mainstream of computer vision, including unsupervised learning, transformers, and multimodality​&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Segment Anything Model (&lt;/strong&gt;&lt;a href=&quot;https://segment-anything.com/&quot;&gt;&lt;strong&gt;SAM&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;):&lt;/strong&gt; Developed by Meta’s FAIR lab, SAM is a state-of-the-art image segmentation model that aims to revolutionise the field of computer vision by identifying which pixels in an image belong to which object, producing detailed object masks from input prompts.&lt;/p&gt;

&lt;p&gt;SAM is built on foundation models that have significantly impacted natural language processing (NLP), and focuses on promptable segmentation tasks, adapting to diverse downstream segmentation problems using prompt engineering​.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2211.06220&quot;&gt;&lt;strong&gt;OneFormer&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;/&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2105.15203&quot;&gt;&lt;strong&gt;SegFormer&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;:&lt;/strong&gt; A state-of-the-art multi-task image segmentation framework implemented using transformers. Parameters: 219 million. Architecture: ViT&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.microsoft.com/en-us/research/publication/florence-a-new-foundation-model-for-computer-vision/#:~:text=While%20existing%20vision%20foundation%20models,videos&quot;&gt;&lt;strong&gt;Florence&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;:&lt;/strong&gt; Introduced by Microsoft, this foundation model has set new benchmarks on several leaderboards such as TextCaps Challenge 2021, nocaps, Kinetics-400/Kinetics-600 action classification, and OK-VQA Leaderboard. Florence aims to expand representations from coarse (scene) to fine (object), and from static (images) to dynamic (videos)​.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2112.10752&quot;&gt;&lt;strong&gt;Stable Diffusion&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;:&lt;/strong&gt; A generative model utilising AI and deep learning to generate images, functioning as a diffusion model with a sequential application of denoising autoencoders​.&lt;/p&gt;

&lt;p&gt;It employs a U-Net model, specifically a Residual Neural Network (ResNet), originally developed for image segmentation in biomedicine, to denoise images and control the image generation process without retraining​​.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://openai.com/dall-e-3&quot;&gt;&lt;strong&gt;DALL-E&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;:&lt;/strong&gt; Developed by OpenAI, DALL-E is a generative model capable of creating images from textual descriptions, showcasing a unique blend of natural language understanding and image generation. It employs a version of the GPT-3 architecture to generate images, demonstrating the potential of transformer models in tasks beyond natural language processing​&lt;/p&gt;

&lt;p&gt;The tech titans, often bundled as GAFA (Google, Amazon, Facebook and Apple), alongside several other companies such as Hugging Face, Anthropic, AI21 Labs, Cohere, Aleph Alpha, Open AI and Salesforce have been instrumental in developing, utilising and advancing foundation models. Substantial investments in these models underscore their potential, as these corporations harness foundation models to augment various facets of their operations, setting a benchmark for &lt;a href=&quot;https://crfm.stanford.edu/2021/10/18/reflections.html#:~:text=Simultaneously%2C%20in%20industry%2C%20several%20startups,that%20impact%20billions%20of%20people&quot;&gt;other sectors&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Insights from industry leaders at &lt;a href=&quot;https://venturebeat.com/ai/foundation-models-2022s-ai-paradigm-shift/#:~:text=Foundation%20models%20like%20DALL,computer%20science%20%20department%20at&quot;&gt;Google&lt;/a&gt;, &lt;a href=&quot;https://www.microsoft.com/en-us/research/academic-program/accelerate-foundation-models-research-fall-2023/#:~:text=About%20the%20program,society%20while%20mitigating%20risks&quot;&gt;Microsoft&lt;/a&gt; and &lt;a href=&quot;https://research.ibm.com/topics/foundation-models&quot;&gt;IBM&lt;/a&gt;, alongside academic institutions, provide a rich tapestry of knowledge and perspectives​.&lt;/p&gt;

&lt;p&gt;Percy Liang, a director of the Center for Research on Foundation Models, emphasised in &lt;a href=&quot;https://www.protocol.com/enterprise/foundation-models-ai-standards-stanford&quot;&gt;this article&lt;/a&gt; that foundation models like DALL-E and GPT-3 herald new creative opportunities and novel interaction mechanisms with systems, showcasing the innovation that these models can bring to the table. He also mentions potential risks of such powerful models​.&lt;/p&gt;

&lt;p&gt;At the &lt;a href=&quot;https://iccv2023.thecvf.com/&quot;&gt;ICCV 2023 conference&lt;/a&gt;, held this year in Paris, foundation models were a very present topic. William T. Freeman, Professor of Computer Science, MIT, talked about the foundation models in his talk in &lt;a href=&quot;https://gkioxari.github.io/Tutorials/iccv2023/&quot;&gt;QUO VADIS Computer Vision&lt;/a&gt; workshop. He cited reasons why he &lt;a href=&quot;https://drive.google.com/file/d/1HfSrxSMS54c6-rYQNKBZnqgk_eRYqwOx/view&quot;&gt;does not like foundation models&lt;/a&gt; as an academic:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;em&gt;They don’t tell us how vision works.&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;They’re not fundamental (and therefore not stable)&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;They separate academia from industry&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This highlights the importance of foundation models for the future of computer vision and their established position and pragmatic aspect of those models focusing on performance.&lt;/p&gt;

&lt;p&gt;IBM Research posits that &lt;a href=&quot;https://research.ibm.com/topics/foundation-models&quot;&gt;foundation models will significantly expedite AI adoption&lt;/a&gt; in business settings. The general applicability of these models, enabled through self-supervised learning and fine-tuning, allows for a wide range of AI applications, thereby accelerating AI deployment across various business domains​.&lt;/p&gt;

&lt;p&gt;Microsoft Research highlights that foundation models are instigating &lt;a href=&quot;https://research.ibm.com/topics/foundation-models&quot;&gt;a fundamental shift in computing research&lt;/a&gt; and across various scientific domains. This shift is underpinned by the models’ ability to fuel industry-led advances in AI, thereby contributing to a vibrant and diverse research ecosystem that’s poised to unlock the promise of AI for societal benefit while addressing associated risks.&lt;/p&gt;

&lt;p&gt;Experts also underscore the critical role of computer vision foundation models in solving real-world applications, emphasising their &lt;a href=&quot;https://crfm.stanford.edu/2021/10/18/reflections.html#:~:text=Simultaneously%2C%20in%20industry%2C%20several%20startups,that%20impact%20billions%20of%20people&quot;&gt;adaptability to a myriad of downstream&lt;/a&gt; tasks due to training on diverse, large-scale datasets​. Moreover, foundation models like CLIP &lt;a href=&quot;https://arxiv.org/abs/2103.00020&quot;&gt;enable zero-shot learning&lt;/a&gt;, allowing for versatile applications like classifying video frames, identifying scene changes and building semantic image search engines without necessitating prior training.&lt;/p&gt;

&lt;p&gt;In another workshop of ICCV 2023, &lt;a href=&quot;https://bigmac-vision.github.io/&quot;&gt;BigMAC&lt;/a&gt;: Big Model Adaptation for Computer Vision, the &lt;a href=&quot;https://bigmac-vision.github.io/pdfs/ludwig.pdf&quot;&gt;robustness of the CLIP model&lt;/a&gt; on the popular ImageNet benchmark was discussed. In conclusion, thanks to training on a large, versatile dataset means that zero-shot predictions of the CLIP model are less vulnerable to data drift than popular CNN models trained and fine tuned on imageNet. In this &lt;a href=&quot;https://www.youtube.com/watch?v=XiouM3MEOKs&amp;amp;t=4546s&quot;&gt;recording of Ludwig’s presentation&lt;/a&gt; different ways to preserve CLIP robustness while fine-tuned are discussed.&lt;/p&gt;

&lt;p&gt;On a side note, the ICCV conference was quite an event. With five days of workshops, talks and demos! Big tech companies such as Meta marked their presence with impressive hubs, answering attendees’ questions. Numerous poster sessions gave us a chance to interact with authors and select some ideas we would like to contribute to the tech stack at Adevinta.&lt;/p&gt;

&lt;p&gt;In the subsequent sections, we will dig into real-world instances, underscoring their impact on e-commerce and elaborate how investing in this technology can galvanise collaboration and innovation across various teams within a company.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*ZtACVLp3aedfQYAy&quot; alt=&quot;An overview of popular foundation models and their applications in various AI domains.&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;h2 id=&quot;real-world-adoption-of-foundation-models&quot;&gt;Real-World Adoption of foundation models&lt;/h2&gt;

&lt;p&gt;Major tech companies have paved the way in producing and distributing ready-to-use foundation models, which are now being utilised by various businesses to &lt;a href=&quot;https://www.forbes.com/sites/moorinsights/2023/07/21/the-extraordinary-ubiquity-of-generative-ai-and-how-major-companies-are-using-it/&quot;&gt;enhance or create new products&lt;/a&gt; for tech-savvy consumers&lt;a href=&quot;https://www.forbes.com/sites/moorinsights/2023/07/21/the-extraordinary-ubiquity-of-generative-ai-and-how-major-companies-are-using-it/&quot;&gt;​&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;e-commerce-andretail&quot;&gt;E-commerce and retail&lt;/h2&gt;

&lt;p&gt;In the sphere of e-commerce, companies like Pinterest and eBay, have &lt;a href=&quot;https://developer.nvidia.com/blog/pinterest-uses-ai-to-enhance-its-recommendations-system/#:~:text=Developers%20from%20Pinterest%2C%20along%20with,objects%20saved%20has%20crossed&quot;&gt;invested in deep learning&lt;/a&gt; and machine learning technologies to enhance user experiences. Pinterest has developed PinSage for advertising and shopping recommendations and a multi-task deep metric &lt;a href=&quot;https://blog.acolyer.org/2019/10/11/learning-a-unified-embedding-for-visual-search-at-pinterest/#:~:text=The%20foundation%20of%20Pinterest%E2%80%99s%20approach,task%20learning&quot;&gt;learning system for unified image embedding&lt;/a&gt; to aid in &lt;a href=&quot;https://arxiv.org/abs/1908.01707&quot;&gt;visual search&lt;/a&gt; and recommendation systems​&lt;a href=&quot;https://blog.acolyer.org/2019/10/11/learning-a-unified-embedding-for-visual-search-at-pinterest/#:~:text=The%20foundation%20of%20Pinterest%E2%80%99s%20approach,task%20learning&quot;&gt;​&lt;/a&gt;. eBay, on the other hand, utilises a convolutional neural network for its &lt;a href=&quot;https://www.ebayinc.com/stories/news/an-easier-way-to-search-ebay-computer-vision-with-find-it-on-ebay-and-image-search-is-now-live/#:~:text=When%20you%20upload%20images%20to,the%20live%20listings%20on%20eBay&quot;&gt;image search feature&lt;/a&gt;, “Find It On eBay.”​&lt;/p&gt;

&lt;p&gt;Computer vision applications are transforming e-commerce, aiding in creating seamless omnichannel shopping experiences​. When it comes to the importance of visuals in shopping experiences, a study by PowerReviews found that &lt;strong&gt;88% of consumers specifically&lt;/strong&gt; &lt;a href=&quot;https://www.ebayinc.com/stories/news/an-easier-way-to-search-ebay-computer-vision-with-find-it-on-ebay-and-image-search-is-now-live/#:~:text=When%20you%20upload%20images%20to,the%20live%20listings%20on%20eBay&quot;&gt;&lt;strong&gt;look for visuals&lt;/strong&gt;&lt;/a&gt; submitted by other consumers prior to making a purchase​.&lt;/p&gt;

&lt;h3 id=&quot;broader-techindustry&quot;&gt;Broader tech industry&lt;/h3&gt;

&lt;p&gt;In the broader tech industry, Microsoft has introduced &lt;a href=&quot;https://www.ebayinc.com/stories/news/an-easier-way-to-search-ebay-computer-vision-with-find-it-on-ebay-and-image-search-is-now-live/#:~:text=When%20you%20upload%20images%20to,the%20live%20listings%20on%20eBay&quot;&gt;Florence&lt;/a&gt;, a novel foundation model for computer vision. The underlying technology of foundation models is designed to provide a solid base that can be fine-tuned for various specific tasks, an advantage that has been recognised and harnessed by industry giants.&lt;/p&gt;

&lt;p&gt;Take Copenhagen-based startup Modl.ai for instance, which relies on foundation models, self-supervised training and computer vision for &lt;a href=&quot;https://the-decoder.com/ai-startup-wants-to-bring-foundation-models-to-game-development/#:~:text=Copenhagen,with%20and%20against%20human%20players&quot;&gt;developing AI bots&lt;/a&gt; to test video games for bugs and performance. Such applications demonstrate the versatility and potential of foundation models in different sectors​.&lt;/p&gt;

&lt;p&gt;The practical implementations of foundation models in these different sectors underscores their potential to drive innovation, enhance user experiences and foster cross-functional collaboration within and beyond the e-commerce spectrum. The flexibility and adaptability of foundation models, as demonstrated by these real-world examples, make them a valuable asset for companies striving to stay ahead in the competitive e-commerce landscape.&lt;/p&gt;

&lt;h2 id=&quot;investing-in-foundation-models-cost-benefit-analysis&quot;&gt;Investing in foundation models: Cost-benefit analysis&lt;/h2&gt;

&lt;p&gt;The investment in foundation models for computer vision transcends the mere financial outlay. It encapsulates a strategic foresight to harness advanced AI technologies for bolstering e-commerce operations.&lt;/p&gt;

&lt;p&gt;Investing in foundation models for computer vision in e-commerce does entail upfront costs such as acquiring computational resources and the requisite expertise. OpenAI’s GPT-3 model, for example, reportedly cost $4.6M to train. According to another OpenAI report, the cost of training a large AI model is &lt;a href=&quot;https://encord.com/blog/visual-foundation-models-vfms-explained/#:~:text=OpenAI%E2%80%99s%20GPT%2D3%20model%2C%20for%20example%2C%20reportedly%20cost%20%244.6MM%20to%20train.%20According%20to%20another%20OpenAI%20report%2C%20the%20cost%20of%20training%20a%20large%20AI%20model%20is%20projected%20to%20rise%20from%20%24100MM%20to%20%24500MM%20by%202030.&quot;&gt;projected to rise&lt;/a&gt; from $100M to $500M by 2030.&lt;/p&gt;

&lt;p&gt;However, the potential benefits could justify the investment. For instance, the &lt;strong&gt;global visual search market,&lt;/strong&gt; which is significantly powered by computer vision technology, is projected to reach &lt;strong&gt;$15 billion by 2023&lt;/strong&gt;. Early adopters who incorporate visual search on their platforms could see &lt;a href=&quot;https://blog.taskmonk.ai/what-role-will-computer-vision-play-in-the-future-of-ecommerce/#:~:text=Early%20adopters%20who%20incorporate%20visual,of%20their%20online%20shopping%20experience&quot;&gt;&lt;strong&gt;revenues increase by 30%&lt;/strong&gt;&lt;/a&gt;. The computer vision market itself is soaring with an expected &lt;strong&gt;annual growth rate of 19.5%&lt;/strong&gt;, predicted to reach a value of $100.4 billion by 2023​&lt;a href=&quot;https://encord.com/blog/visual-foundation-models-vfms-explained/#:~:text=April%2024%2C%202023%20%E2%80%A2%205,9Bn%20in%202022&quot;&gt;​&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;These figures suggest that the integration of computer vision, particularly through foundation models, can be a lucrative venture in the long-term. Consumers are increasingly leaning towards platforms that offer visual search and other AI-driven features. Therefore, the cost of investment could be offset by the subsequent increase in revenue, enhanced user engagement and improved operational efficiency brought about by the advanced capabilities of foundation models in computer vision.​&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;&lt;em&gt;“Foundation models cut down on data labelling requirements anywhere from a factor of like 10 times, 200 times, depending on the use case”&lt;/em&gt;&lt;/strong&gt;— &lt;a href=&quot;https://venturebeat.com/ai/foundation-models-2022s-ai-paradigm-shift/#:~:text=%E2%80%9CFoundation%20models%20cut%20down%20on%20data%20labeling%20requirements%20anywhere%20from%20a%20factor%20of%20like%2010%20times%2C%20200%20times%2C%20depending%20on%20the%20use%20case%2C%E2%80%9D%20Dakshi%20Agrawal%2C%20IBM%20fellow%20and%20CTO%20of%20IBM%20AI%2C&quot;&gt;Dakshi Agrawal&lt;/a&gt;, IBM fellow and CTO of IBM AI&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Moreover, the global computer vision market, which encompasses technologies enabling such visual experiences, is expected to &lt;a href=&quot;https://www.syte.ai/blog/visual-ai/how-visual-ai-is-changing-omnichannel-retail/&quot;&gt;grow substantially&lt;/a&gt;, indicating the increasing importance of investment in visual technologies for retail and e-commerce​. The role of visual AI, which includes &lt;a href=&quot;https://research.aimultiple.com/computer-vision-retail/#:~:text=The%20global%20computer%20vision%20market,improve%20efficiency%20in%20omnichannel&quot;&gt;computer vision&lt;/a&gt;, is also highlighted in how it’s changing omnichannel retail, showcasing the intertwined relationship between visual technology and &lt;a href=&quot;https://losspreventionmedia.com/computer-vision-future-of-retail/&quot;&gt;enhanced shopping experiences&lt;/a&gt; across channels​.&lt;/p&gt;

&lt;h2 id=&quot;examples-of-application-of-foundation-models-in-e-commerce&quot;&gt;Examples of application of foundation models in e-commerce&lt;/h2&gt;

&lt;p&gt;Because of their pre-training on expansive datasets, foundation models in computer vision bring a treasure trove of capabilities to the table. &lt;strong&gt;The pre-trained nature of foundation models significantly accelerates the deployment of computer vision applications in e-commerce, as they require less data and resources for fine-tuning compared to training models from scratch.&lt;/strong&gt; Let’s illustrate this through real-world examples within the e-commerce sector.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Product Categorisation&lt;/strong&gt;: Leveraging a foundation model for automated product categorisation can be a time and resource-saver.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Visual Search&lt;/strong&gt;: Implementing visual search features can be expedited with foundation models. Their pre-trained knowledge can be leveraged to recognise fashion or product trends, making visual search more intuitive.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Counterfeit Detection&lt;/strong&gt;: Counterfeit detection is a complex task; however, with a foundation model, the pre-existing knowledge about different objects can be fine-tuned to identify subtle discrepancies between genuine and counterfeit products&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Moderation&lt;/strong&gt;: Detection of unwanted or harmful content can be done through a classification head added on top of image embeddings generated by a foundation model.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*xDEvavmYRc4MF2xE&quot; alt=&quot;A diagram showcasing the various applications of foundation models in e-commerce, from product categorization to counterfeit detection.&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Beyond these examples, foundation models also hold promise in enhancing user experiences in recommendation systems and augmented reality (AR) shopping.&lt;/p&gt;

&lt;p&gt;Most of this use-case could be applied to Adevinta marketplaces or replace existing services based on more traditional models.&lt;/p&gt;

&lt;h2 id=&quot;empowering-teams-across-the-e-commerce-spectrum&quot;&gt;Empowering teams across the e-commerce spectrum&lt;/h2&gt;

&lt;p&gt;Foundation models in computer vision open up avenues for fostering cross-functional collaboration, expediting product development, and making data-driven decision-making a norm across an e-commerce enterprise. Let’s delve into how these models can act as catalysts in harmonising the efforts of various teams and speeding up the journey from conception to market-ready solutions.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*3hnJ3nPcTBr68D70&quot; alt=&quot;Members of the Cognition team at Adevinta discussing the latest trends in AI and computer vision.&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;h2 id=&quot;accelerating-the-product-development-cycle&quot;&gt;Accelerating the product development cycle&lt;/h2&gt;

&lt;p&gt;The pre-trained nature of foundation models significantly &lt;strong&gt;cuts down the time traditionally required to develop, train and deploy machine learning models&lt;/strong&gt;. This acceleration in the product development cycle is invaluable in the fiercely competitive e-commerce market, where being the first to introduce innovative features can provide a substantial competitive edge. Moreover, the resource efficiency of foundation models ensures that &lt;strong&gt;teams can iterate and improve upon models swiftly&lt;/strong&gt;, aligning with dynamic market trends and customer expectations.&lt;/p&gt;

&lt;h2 id=&quot;stepping-stone-to-broader-business-objectives&quot;&gt;Stepping stone to broader business objectives&lt;/h2&gt;

&lt;p&gt;Foundation models can act as a springboard towards achieving broader business goals such as sustainability and promoting the second-hand goods trade. By enabling smarter product listings and verifications through image recognition and visual search capabilities, these models can streamline the process of listing and verifying second-hand goods. This, in turn, &lt;strong&gt;promotes a circular economy, encouraging the reuse and recycling of products&lt;/strong&gt;, which aligns with the sustainability goals of many modern e-commerce platforms.&lt;/p&gt;

&lt;h2 id=&quot;challenges-and-overcoming-strategies&quot;&gt;Challenges and overcoming strategies&lt;/h2&gt;

&lt;p&gt;Incorporating foundation models for computer vision within an e-commerce setting comes with a range of challenges, but with the right strategies, these hurdles can be navigated to unlock the models’ full potential.&lt;/p&gt;

&lt;h2 id=&quot;computational-requirements&quot;&gt;Computational requirements&lt;/h2&gt;

&lt;p&gt;Foundation models are computationally intensive due to their large-scale nature, which necessitates &lt;a href=&quot;https://snorkel.ai/foundation-models/#:~:text=Cost,for%20their%20end%20use%20caes&quot;&gt;significant computational resources&lt;/a&gt; for training and fine-tuning. The good news is that, once the substantial work of domain-learning or fine tuning is done, numerous teams and projects can benefit from the foundation model with minimal additional effort and cost.&lt;/p&gt;

&lt;h2 id=&quot;bias-andfairness&quot;&gt;Bias and fairness&lt;/h2&gt;

&lt;p&gt;Foundation models may inherit biases present in the training data, which can lead to unfair or discriminatory behaviour. For instance, &lt;a href=&quot;https://datagen.tech/blog/the-opportunities-and-risks-of-foundation-models/&quot;&gt;DALL-E and CLIP have shown biases&lt;/a&gt; regarding gender and race when generating images or interpreting text and images​. Implementing robust data preprocessing and bias mitigation strategies will help to address potential biases in training data.&lt;/p&gt;

&lt;h2 id=&quot;interpretability-andcontrol&quot;&gt;Interpretability and control&lt;/h2&gt;

&lt;p&gt;Understanding and controlling the behaviour of foundation models like CLIP remains a challenge due to their black-box nature. This makes it difficult to interpret the models’ predictions, which is a hurdle in applications where &lt;a href=&quot;https://arxiv.org/abs/2103.00020&quot;&gt;explainability is crucial&lt;/a&gt;​​. CRFM released recently a &lt;a href=&quot;https://crfm.stanford.edu/fmti/?utm_campaign=The%20Batch&amp;amp;utm_medium=email&amp;amp;_hsmi=280825441&amp;amp;utm_content=280827829&amp;amp;utm_source=hs_email&quot;&gt;Foundation Model Transparency Index&lt;/a&gt; “scoring 10 popular models on how well their makers disclosed details of their training, characteristics and use.”&lt;/p&gt;

&lt;p&gt;Foundation models, if widely adopted, could introduce &lt;strong&gt;single points of failure&lt;/strong&gt; in machine learning systems. If adversaries find vulnerabilities in a foundation model, they could &lt;a href=&quot;https://arxiv.org/abs/2103.11251&quot;&gt;exploit these weaknesses&lt;/a&gt; across multiple systems utilising the same model​.&lt;/p&gt;

&lt;p&gt;Foundation models are &lt;strong&gt;not the answer to all&lt;/strong&gt; machine learning problems.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;&lt;em&gt;“Foundation models are neither ‘foundational’ nor the foundations of AI. We deliberately chose ‘foundation’ rather than ‘foundational,’ because we found that ‘foundational’ implied that these models provide fundamental principles in a way that ‘foundation’ does not. (…) Further, ‘foundation’ describes the (role of) model and not AI; we neither claim nor believe that foundation models alone are the foundation of AI, but instead note they are ‘only one component (though an increasingly important component) of an AI system.’”&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;— the Stanford Institute for Human-Centred AI founded the Center for &lt;a href=&quot;https://arxiv.org/pdf/2108.07258.pdf&quot;&gt;Research on Foundation Models&lt;/a&gt; (CRFM), 2021&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;The transformative potential of foundation models in computer vision is unmistakable and pivotal for advancing the e-commerce domain. They encapsulate a significant stride towards creating smarter, more intuitive and user-centric online shopping experiences. The notable successes of early adopters, alongside the burgeoning global visual search market, exhibit the financial promise inherent in embracing these models​.&lt;/p&gt;

&lt;p&gt;The real-world implications extend beyond just improved product discovery and categorisation, to fostering a sustainable trading ecosystem for second-hand goods. &lt;strong&gt;The expertise and investment in these models can expedite the product development cycle, encourage data-driven decision-making and stimulate cross-functional collaboration across various company departments.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;However, it’s crucial to acknowledge the technical and ethical challenges that come with the deployment of foundation models. The computational costs, potential biases and the necessity for robust infrastructures demand a well-thought-out strategic approach. Yet, with the right investment in computational infrastructure, continuous learning and a commitment to ethical AI practices, these hurdles can be navigated successfully.&lt;/p&gt;

&lt;p&gt;While the promise of foundation models in computer vision is evident, discerning which model will perform optimally with your specific data remains a complex challenge.&lt;/p&gt;

&lt;p&gt;This uncertainty underscores the vital &lt;strong&gt;need for comprehensive benchmarks&lt;/strong&gt; that can guide businesses in selecting the most appropriate model. Investing time in testing and evaluation is crucial, as it enables a more informed decision-making process. A recent study highlighted in the article “&lt;a href=&quot;https://arxiv.org/pdf/2310.19909.pdf&quot;&gt;A Comprehensive Study on Backbone Architectures for Regular and Vision Transformers&lt;/a&gt;” delves into this subject by testing different model backbones across a range of downstream tasks and datasets. Such research is invaluable for businesses looking to capitalise on foundation models, as it provides critical insights into model performance and applicability, ensuring that their investment in AI is both strategic and effective.&lt;/p&gt;

&lt;p&gt;In Adevinta, as an e-commerce leader, we are evaluating the pros and cons of foundation models to best leverage their potential within our company. In Cognition, we are also working on internal benchmarks that will help to chose right foundation model for the task, estimate ressources needed and showcase its potential performance on the marketplace data.&lt;/p&gt;

&lt;p&gt;With industry behemoths and experts leading the era of foundation models, the call to action for e-commerce directors is clear: &lt;strong&gt;Embrace the paradigm shift that foundation models represent, and consider them as a long-term strategic asset for maintaining a competitive edge in the rapidly evolving e-commerce landscape.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Check out this mine of knowledge about foundation models: &lt;a href=&quot;https://cs.uwaterloo.ca/~wenhuche/teaching/cs886/&quot;&gt;https://cs.uwaterloo.ca/~wenhuche/teaching/cs886/&lt;/a&gt;&lt;/p&gt;

&lt;footer&gt;
  &lt;p&gt;Exported from &lt;a href=&quot;https://medium.com&quot;&gt;Medium&lt;/a&gt; on December 19
    2023.&lt;/p&gt;
  &lt;p&gt;&lt;a href=&quot;https://medium.com/adevinta-tech-blog/foundation-models-a-new-vision-for-e-commerce-76904a3066e8&quot;&gt;View
      the original&lt;/a&gt;&lt;/p&gt;
&lt;/footer&gt;
&lt;script type=&quot;text/javascript&quot; src=&quot;//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-584ec4ce89deed84&quot;&gt;&lt;/script&gt;

&lt;div class=&quot;addthis_inline_share_toolbox&quot;&gt;&lt;/div&gt;

</description>
        <pubDate>Tue, 19 Dec 2023 00:00:00 +0100</pubDate>
        <link>http://urszulaczerwinska.github.io/thoughts/foundation-models-a-new-vision-for-e-commerce</link>
        <guid isPermaLink="true">http://urszulaczerwinska.github.io/thoughts/foundation-models-a-new-vision-for-e-commerce</guid>
        
        
        <category>thoughts</category>
        
        <category>featured</category>
        
      </item>
    
      <item>
        <title>Deep Dive in PaddleOCR inference</title>
        <description>&lt;head&gt;
  &lt;meta name=&quot;description&quot; content=&quot;A deep dive into the complexities of using PaddleOCR for text extraction from images and how the Cognition team improved the service. Learn about the challenges and solutions that enhanced user experience in OCR services.&quot; /&gt;
&lt;/head&gt;

&lt;p&gt;This article is a deep dive into part of our work as described in &lt;a href=&quot;/works/text-in-image-2-0-improving-ocr-service-with-paddleocr&quot;&gt;&lt;strong&gt;Article 1: Text in Image 2.0: improving OCR service with PaddleOCR&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We are Cognition, an &lt;a href=&quot;https://www.adevinta.com/&quot;&gt;Adevinta&lt;/a&gt; Computer Vision Machine Learning (ML) team working on solutions for our marketplaces. Adevinta is a global classifieds specialist with market-leading positions in key European markets that aims to find perfect matches between its users and the platforms’ goods. As a Global Team, our team, Cognition, provides image processing APIs to all of our marketplaces.&lt;/p&gt;

&lt;p&gt;In the process of improving our OCR API for text extraction from images, we updated our existing Text in Image service to the &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR&quot;&gt;PaddleOCR&lt;/a&gt; framework, which was the winner of our benchmarks. In order to test if this framework was the most suitable solution, we carried out a deeper analysis of their code base. This article shares the challenges we encountered and how we overcame them.&lt;/p&gt;

&lt;p&gt;We believe our code version is easier to work with, given the use case of text extraction from images. The different steps and pre-processing and post-processing parts are clearly separated so they can be called independently, which should make further community extensions easier to add. It also makes putting into production easier, as the simplified, modular code combines well with the structure of &lt;em&gt;inference.py&lt;/em&gt; for serving SageMaker endpoints. Our proposed code version does not alter predictions (compared to the 2.6 release) for images.&lt;/p&gt;

&lt;h2 id=&quot;understanding-the-paddleocr-framework&quot;&gt;Understanding the PaddleOCR framework&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/PaddlePaddle/Paddle&quot;&gt;PaddlePaddle&lt;/a&gt; (short for Parallel Distributed Deep Learning) is an open source deep learning platform developed by Baidu Research. It is written in C++ and Python, and is designed to be easy to use and efficient for large-scale machine learning tasks.&lt;/p&gt;

&lt;p&gt;PaddlePaddle provides a range of tools and libraries for building and training deep learning models, including support for convolutional neural networks (CNNs), recurrent neural networks (RNNs) and long short-term memory (LSTM) networks.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR&quot;&gt;PaddleOCR&lt;/a&gt; builds on PaddlePaddle, an unfamiliar framework that our team had not used before. To make things even more challenging, PaddleOCR is not just one algorithm, it includes a range of pre-trained models and tools for recognising text in images and documents, as well as for training custom OCR models.&lt;/p&gt;

&lt;p&gt;PaddleOCR is divided into two main sections:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;PP-OCR&lt;/strong&gt;, an OCR system used for text extraction from images&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;PP-Structure&lt;/strong&gt;, a document analysis system which aims to perform layout analysis and table recognition&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;PP-OCR exists in three different versions (V1, V2 and V3). In these different releases, major improvements were brought to the models’ architecture.&lt;/p&gt;

&lt;p&gt;For our Text in Image service update, we focused on the most recent and most performant PP-OCRv3 release.&lt;/p&gt;

&lt;h3 id=&quot;the-paddleocrv3-models-architecture&quot;&gt;The PaddleOCRv3 models architecture&lt;/h3&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*1mI3YTIjAut_QMrl&quot; alt=&quot;PaddleOCRv3 Architecture&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;PP-OCRv3 is composed of three parts: detection, classification and recognition, all of which can be used independently. Each part has its own model trained with the PaddlePaddle framework. For those interested, model details can be found in this dedicated research article PP-OCRv3: &lt;a href=&quot;https://arxiv.org/abs/2206.03001v2&quot;&gt;More Attempts for the Improvement of Ultra Lightweight OCR System (Yanjun et al., 2022)&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;PP-OCRv3 text detection is made with the Differentiable Binarization algorithm (&lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.6/doc/doc_en/algorithm_det_db_en.md&quot;&gt;DB&lt;/a&gt;) trained using distillation strategy. The PP-OCRv3 recogniser is optimised based on the text recognition algorithm, Scene Text Recognition with a Single Visual Model (&lt;a href=&quot;https://arxiv.org/abs/2205.00159&quot;&gt;SVTR, Du et al. 2022)&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;PP-OCRv3 adopts the text recognition network SVTR_LCNet, and uses &lt;a href=&quot;https://arxiv.org/abs/2002.01276&quot;&gt;the guided training of Connectionist Temporal Classification (CTC&lt;/a&gt;, Z&lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Lin%2C+Z&quot;&gt;hiping&lt;/a&gt; et al., 2020) by the attention, data augmentation strategy, TextConAug, Unified Deep Mutual Learning and Unlabelled Images Mining (first introduced in &lt;a href=&quot;https://arxiv.org/abs/2109.03144&quot;&gt;PaddleOCRv2, Yanjun et al. 2021&lt;/a&gt;). The Text classifier is a simple binary classifier with classes 0 and 180°.&lt;/p&gt;

&lt;h3 id=&quot;paddleocr-inference-in-practice&quot;&gt;PaddleOCR inference in practice&lt;/h3&gt;

&lt;p&gt;While testing on our benchmarks, we used the PaddleOCR code for inference with default parameters and “latin” as a language (see their &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.6/doc/doc_en/quickstart_en.md&quot;&gt;QuickStart page&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Reading the documentation and looking into the class parameters, we saw lots of model combinations to test and therefore more opportunities to potentially improve our score.&lt;/p&gt;

&lt;p&gt;For instance, the &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.6/doc/doc_en/detection_en.md&quot;&gt;documentation&lt;/a&gt; suggests there is a choice between “DB” and “EAST” algorithms for detection, but it’s only the main inference &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.6/paddleocr.py&quot;&gt;script&lt;/a&gt; where the algorithm has to be “DB” — the &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/blob/23e034c40ecd5755af48d7b14dcc1bf6c5cf1128/tools/infer/predict_det.py#L62&quot;&gt;script&lt;/a&gt; of detection inference goes through a long list of algorithms. A similar situation occurs with text recognition where the pre-trained algorithm for Latin is “SVTR_LCNet”, but in &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/blob/23e034c40ecd5755af48d7b14dcc1bf6c5cf1128/paddleocr.py#L51&quot;&gt;theory&lt;/a&gt;, the accepted values are “‘CRNN’ and ‘SVTR_LCNet’ with the general &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/blob/18ddb6d5f9bdc2c1b0aa7f6e399ec0f76119dc87/doc/doc_en/inference_args_en.md&quot;&gt;documentation&lt;/a&gt; mentioning a plethora of models.&lt;/p&gt;

&lt;p&gt;Pre-trained English models are available in “‘CRNN’ and ‘SVTR_LCNet’ architectures. However, to find the information, the user would need to look into the pretrained model &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/blob/18ddb6d5f9bdc2c1b0aa7f6e399ec0f76119dc87/configs/rec/PP-OCRv3/en_PP-OCRv3_rec.yml#L39&quot;&gt;config&lt;/a&gt;. If the user does not specify the “rec_algorithm”, the default value, “SVTR_LCNet”, would be used, even if it isn’t correct. This doesn’t actually make any difference to the inference &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.6/tools/infer/predict_rec.py&quot;&gt;code&lt;/a&gt; as none of the “if” applies to ‘CRNN’ or ‘SVTR_LCNet’.&lt;/p&gt;

&lt;p&gt;In order to test a different architecture, we would need to train it ourselves and chain dedicated scripts.&lt;/p&gt;

&lt;h2 id=&quot;clarifying-paddleocr-inference&quot;&gt;Clarifying PaddleOCR inference&lt;/h2&gt;

&lt;p&gt;From digging into the code, we discovered several complexities, unnecessary for our use case. Firstly, the code seemed to grow organically, where the inference version is a limited choice entry to the multi-option code. This leaves us with numerous “factory patterns” and “if .. elses”, where the user has no choice at all. The English documentation was confusing and referenced different usage cases. We struggled to follow the logic as it neither explained parameters, nor clearly defined the limitations of the inference code.&lt;/p&gt;

&lt;p&gt;Despite these complexities, we managed to clarify the general way of working, calling the PaddleOCR.ocr() method from the ‘master’ file, &lt;em&gt;paddleocr.py&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*zwImfJ-4pOxDvrEI&quot; alt=&quot;PaddleOCR.ocr() Method&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;The input image and parameters are entered into the PaddleOCR.ocr() method which calls TextSystem class in order: TextDetector, TextClassifier and TextRecogniser, with a selection of helper functions, including one that formats the outputs of TextDetector into a list of cropped images being input to TextClassifier and TextRecogniser.&lt;/p&gt;

&lt;p&gt;The PaddleOCR.ocr() method is parsing params, including the language, version, type of OCR (or structure), downloads inference models and imports actual image (with check_image).&lt;/p&gt;

&lt;p&gt;If we want our image to go through a full OCR process, the TextSystem class will sequentially call classes responsible for detection, classification and recognition.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*B-7pY0A4Xv7eNTcr&quot; alt=&quot;TextSystem Class Flow&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Each of the main classes has an &lt;em&gt;__init__&lt;/em&gt; method that initialises pre- &amp;amp; post- processing classes and loads the model (create_predictor), and &lt;em&gt;__call__&lt;/em&gt; method that executes (pre- &amp;amp;) post-processing on the image and performs the model inference for the input image(s).&lt;/p&gt;

&lt;p&gt;Most of the scripts used for inference can be found under ‘tools/infer/’. The pre-processing scripts are under “ppocr/data/imaug/operators.py”. The post-processing classes are under ‘ppocr/postprocess/’.&lt;/p&gt;

&lt;p&gt;This schema enables us to reduce the essential inference code to just a couple of files and better understand exactly how the code works. To make it easier to maintain, we decided to reformat the code, keeping only the essential parts for our use case.&lt;/p&gt;

&lt;h2 id=&quot;paddleocr-inference-code-caveats-andfixes&quot;&gt;PaddleOCR inference code caveats and fixes&lt;/h2&gt;

&lt;p&gt;Let’s walk you through the PaddleOCR features we didn’t like and suggestions on how they could be improved.&lt;/p&gt;

&lt;h3 id=&quot;spaghetti-code&quot;&gt;Spaghetti code&lt;/h3&gt;

&lt;p&gt;Overall, most of the code is in object oriented programming style where classes are not modular and most things happen in very long &lt;em&gt;__init__&lt;/em&gt; and &lt;em&gt;__call__&lt;/em&gt; methods. We have noticed (fig. 2 and fig. 3) that generally, three parts can be extracted: pre-processing, inference and post-processing. We have removed ‘create_operators’ and ‘build_post_process’ intermediate functions and called directly the class performing the task such as “DBPostprocess” and “NormalizeImage”. To make things more straightforward, we transformed them into simple functions, performing what their &lt;em&gt;__call__&lt;/em&gt; method was doing before. This leaves us with more modular code and direct logic that fits our needs.&lt;/p&gt;

&lt;h3 id=&quot;parameter-parsing&quot;&gt;&lt;em&gt;Parameter parsing&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;We found it problematic that the inference class requires 105 parameters, of which more than 70 were ignored.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*jPMJx-wOF-R5DsmqJFs5BA.png&quot; alt=&quot;PaddleOCR inference parameters are not all used&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/blob/18ddb6d5f9bdc2c1b0aa7f6e399ec0f76119dc87/doc/doc_en/inference_args_en.md&quot;&gt;English documentation&lt;/a&gt; lists the parameters and gives a succinct definition of them. In the code, they are defined in at least three different places: &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/blob/23e034c40ecd5755af48d7b14dcc1bf6c5cf1128/paddleocr.py#L307&quot;&gt;paddleocr.py&lt;/a&gt;, &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/blob/23e034c40ecd5755af48d7b14dcc1bf6c5cf1128/ppstructure/utility.py#L21&quot;&gt;utility.py&lt;/a&gt; and different &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/blob/34b9569800a38af41a27ed893b12567757ef6c89/tools/infer/utility.py#L34&quot;&gt;utility.py&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;However while executing the code, we found that only 20 parameters were useful in our refactored code:&lt;/p&gt;

&lt;p&gt;When rewriting the code, we cleaned the parameter list, leaving only the relevant parameters.&lt;/p&gt;

&lt;h3 id=&quot;parameter-impact-on-prediction&quot;&gt;&lt;em&gt;Parameter impact on prediction&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;Some of the parameter definitions and effect they would have when changed from default, were not clear to us. We built a &lt;a href=&quot;https://streamlit.io/&quot;&gt;Streamlit app&lt;/a&gt; to visualise the changes in params on the predictions. For instance, “unclip ratio” would impact the size of the box, and “threshold” would detect two bounding boxes instead of one. We advise you to play with your own data and model to see how different parameters affect the detection. Overall, we were not able to see a major improvement from changing defaults.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*B4uqn-7vcxfu5aPz&quot; alt=&quot;The illustration of PaddleOCR parameters impact on the machine learning model prediction&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;h3 id=&quot;language-choice&quot;&gt;&lt;em&gt;Language choice&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;Normally in our role, we work with “‘PP-OCRv3”, the most recent version of the framework. As we are dealing with European languages, we would choose “fr”, “en”, “es” as the “lang” param, thinking that this means different models are being called. However, while looking into the paddleocr.py, we saw how the languages are interpreted:&lt;/p&gt;

&lt;p&gt;The first definition serves to define the recognition model name/path. But if we typed “fr” or “es”, it becomes lang = “latin”, yet “en” remains “en”. Then another simplification happens for the detection model.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;if lang in [“en”, “latin”]:&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;det_lang = “en”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;We are left with an English detection model and a Latin recognition model for any European language written with Latin characters except English, which has its own recognition model.&lt;/p&gt;

&lt;h3 id=&quot;downloading-models&quot;&gt;&lt;em&gt;Downloading models&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;Based on the language parameter and framework version, the first time we call the PaddleOCR class with those parameters, the model will be downloaded from the url encoded in paddleocr.py.&lt;/p&gt;

&lt;p&gt;Firstly, this could cause some issues when running the code in secure or offline environments.&lt;/p&gt;

&lt;p&gt;Secondly, we found inconsistencies between the model urls in the paddleocr.py and the models provided in the dedicated &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/blob/18ddb6d5f9bdc2c1b0aa7f6e399ec0f76119dc87/doc/doc_en/models_list_en.md&quot;&gt;documentation page&lt;/a&gt;. For instance, “en_PP-OCRv3_det_slim” is not an option when models are downloaded by the paddleocr.py script. In order to use some of the models from Model Zoo, a database of pre-trained models and code, you would need to download the model and provide the path to it manually.&lt;/p&gt;

&lt;p&gt;In order to remove this ambiguity and use the specific model we needed, we decided to pre-download the chosen model, then provide the path directly. In the original code, it is possible to provide det_model_dir, cls_model_dir and rec_model_dir. The language param will then be ignored and any pre-trained model with the accepted backbones can be used. After this process, we removed the model download functionality from our code.&lt;/p&gt;

&lt;h3 id=&quot;using-onnxmodels&quot;&gt;&lt;em&gt;Using ONNX models&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;PaddleOCR provides a &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/blob/18ddb6d5f9bdc2c1b0aa7f6e399ec0f76119dc87/deploy/paddle2onnx/readme.md&quot;&gt;handy way&lt;/a&gt; to export models to the &lt;a href=&quot;https://onnx.ai/&quot;&gt;ONNX framework&lt;/a&gt; that can serve or integrate in different pipelines. We exported the pre-trained models using PaddleOCR instructions. In the PaddleOCR class, there is a parameter “use_onnx”. If one sets “use_onnx” and provides a direct path to the ONNX models to PaddleOCR(), the model would use the ONNX model for prediction. However, there is a small bug that occurs while running ONNX with GPUs, described further in this &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/issues/8688&quot;&gt;issue&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We applied the modification suggested and tested the code with ONNX models, obtaining satisfactory results on both CPU and GPU (even though we noticed small numerical differences between the Paddle and ONNX model versions).&lt;/p&gt;

&lt;h3 id=&quot;documentation&quot;&gt;&lt;em&gt;Documentation&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;If you look at the &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/tree/18ddb6d5f9bdc2c1b0aa7f6e399ec0f76119dc87/doc&quot;&gt;documentation pages&lt;/a&gt;, you will find a lot of resources in both English and Chinese. However, when looking at &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/issues&quot;&gt;Issues&lt;/a&gt;, you will find most of them are in Chinese, Japanese or Korean. The same applies to blog posts and community resources online. We also found that some documentation is only partially translated to English and the Chinese version contains much more detail.&lt;/p&gt;

&lt;p&gt;We did not find a solution for this. We made sure to always check both the English and Chinese documentation (translated to English by an automatic translator) to ensure that we have all the possible information.&lt;/p&gt;

&lt;h3 id=&quot;tests--pylint-typing&quot;&gt;&lt;em&gt;Tests &amp;amp;&lt;/em&gt; &lt;a href=&quot;https://pylint.pycqa.org/en/latest/&quot;&gt;&lt;em&gt;pylint&lt;/em&gt;&lt;/a&gt; &lt;em&gt;&amp;amp;&lt;/em&gt; &lt;a href=&quot;https://docs.python.org/3/library/typing.html&quot;&gt;&lt;em&gt;typing&lt;/em&gt;&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;In general, as the original code is not modular, it was not tested according to the standards of our team. Once we cleaned and simplified the code, we worked on linting and variable typing. Our next step will be to write meaningful unit tests to secure the code base.&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;PaddleOCR is a powerful and optimised library for the extraction of text from images. However, we found that the code doesn’t fit the standards of our team as it is too complex to maintain and understand. In this article, we pointed out some of the pain points for us that other PaddleOCR users may experience when working with this framework. The fixes we proposed made our lives easier and the code more transparent for any team member and the wider community, without compromising the speed or the original model accuracy.&lt;/p&gt;

&lt;footer&gt;
  &lt;p&gt;Exported from &lt;a href=&quot;https://medium.com&quot;&gt;Medium&lt;/a&gt; on June 06,
    2023.&lt;/p&gt;
  &lt;p&gt;&lt;a href=&quot;https://medium.com/adevinta-tech-blog/deep-dive-in-paddleocr-inference-e86f618a0937&quot;&gt;View
      the original. This article was orignally co-authored by Cognition team members, special credits to Joaquin Cabezas&lt;/a&gt;&lt;/p&gt;
&lt;/footer&gt;
&lt;script type=&quot;text/javascript&quot; src=&quot;//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-584ec4ce89deed84&quot;&gt;&lt;/script&gt;

&lt;div class=&quot;addthis_inline_share_toolbox&quot;&gt;&lt;/div&gt;

</description>
        <pubDate>Mon, 06 Mar 2023 00:00:00 +0100</pubDate>
        <link>http://urszulaczerwinska.github.io/works/deep-dive-in-paddleocr-inference</link>
        <guid isPermaLink="true">http://urszulaczerwinska.github.io/works/deep-dive-in-paddleocr-inference</guid>
        
        <category>data science</category>
        
        <category>OCR</category>
        
        <category>deep learning</category>
        
        <category>computer vision</category>
        
        <category>machine learning</category>
        
        <category>featured</category>
        
        
        <category>works</category>
        
      </item>
    
      <item>
        <title>Text in Image 2.0 - improving OCR service with PaddleOCR</title>
        <description>&lt;head&gt;
  &lt;meta name=&quot;description&quot; content=&quot;Discover how the Cognition team at Adevinta enhanced the Text in Image service using PaddleOCR, leading to significant improvements in OCR accuracy and performance.&quot; /&gt;
&lt;/head&gt;

&lt;h2 id=&quot;understanding-ocr-what-is-optical-character-recognition&quot;&gt;Understanding OCR: What is Optical Character Recognition?&lt;/h2&gt;

&lt;p&gt;Optical Character Recognition (OCR) is a popular topic for both industry and personal use. In this article, we share how we tested and used an existing open source library, PaddleOCR, to extract text from an image. This read is for anyone who would like to find out more about OCR, the needs of our customers at &lt;a href=&quot;https://www.adevinta.com/&quot;&gt;Adevinta&lt;/a&gt;, and the challenges we face in attending to them. You’ll find out how we upgraded an existing service, benchmarked different solutions and delivered the selected one to satisfy our customers.&lt;/p&gt;

&lt;h2 id=&quot;key-ocr-applications-how-ocr-transforms-business-and-daily-operations&quot;&gt;Key OCR applications: How OCR transforms business and daily operations&lt;/h2&gt;

&lt;p&gt;OCR stands for “Optical Character Recognition” and is a technology that allows computers to recognise and extract text from images and scanned documents. OCR software uses optical recognition algorithms to interpret the text in images and convert it into machine-readable text that can be edited, searched and stored electronically.&lt;/p&gt;

&lt;p&gt;There are numerous use-cases where OCR can be used:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Digitising paper documents&lt;/strong&gt;: to convert scanned images of text into digital text. This is useful for organisations that want to reduce their reliance on paper and improve their document management processes.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Extracting data from images&lt;/strong&gt;: eg from documents such as invoices, receipts and forms. This can be useful for automating data entry tasks and reducing the need for manual data entry.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Translating documents&lt;/strong&gt;: to extract text from images of documents written in foreign languages and translate them into a different language.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Archiving&lt;/strong&gt;: to create digital copies of important documents that need to be preserved for long periods of time.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Improving accessibility&lt;/strong&gt;: to make scanned documents more accessible to people with disabilities by converting the text into a format that can be read by assistive technologies such as screen readers.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Searching documents&lt;/strong&gt;: to make scanned documents searchable, allowing users to easily find specific information within a large collection of documents.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;the-adevinta-context-why-ocr-matters-in-global-marketplace&quot;&gt;The Adevinta context: Why OCR matters in global marketplace&lt;/h2&gt;

&lt;p&gt;Within &lt;a href=&quot;https://www.adevinta.com/&quot;&gt;Adevinta&lt;/a&gt;, a global classifieds specialist with market-leading positions in key European markets, there is space for all of the cited use cases. However, for this article, we focus specifically on “extracting data from images.”&lt;/p&gt;

&lt;p&gt;Applying deep learning to images is the main expertise of our team, Cognition. We are Data Scientists and Machine Learning (ML) Engineers that work together to develop image-based ML solutions at scale, helping Adevinta’s marketplaces build better products and experiences for their customers. Adevinta’s mission is to connect buyers and sellers, enabling people to find jobs, homes, cars, consumer goods and more. By making an accessible ML API with features tailored to our different marketplaces’ needs, Adevinta’s marketplaces are empowered with ML tools at a reasonable cost.&lt;/p&gt;

&lt;h2 id=&quot;text-extraction-in-images-why-its-crucial-for-adevintas-services&quot;&gt;Text Extraction in Images: Why It’s Crucial for Adevinta’s Services&lt;/h2&gt;
&lt;p&gt;Text extraction from images enables us to:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Detect unwanted content in ads (e.g., insults, hidden messages).&lt;/li&gt;
  &lt;li&gt;Better understand image content to improve search capabilities.&lt;/li&gt;
  &lt;li&gt;Support more efficient searches using visible text on items.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With over 100 million requests per month and growing, our existing Text in Image service was ripe for enhancement. We aimed to improve accuracy and performance, leading to the development of Text in Image 2.0.&lt;/p&gt;

&lt;h2 id=&quot;why-we-chose-paddleocr-benchmarking-the-best-ocr-solution&quot;&gt;Why we chose PaddleOCR: Benchmarking the best OCR solution&lt;/h2&gt;

&lt;p&gt;The existing service was based on &lt;a href=&quot;https://arxiv.org/abs/1801.01671&quot;&gt;Fast Oriented Text Spotting with a Unified Network (Yan et al., 2018)&lt;/a&gt;. Despite being state of the art in 2018, the algorithm achieved 0.4 accuracy on our internal benchmark of 200 marketplace images. Nevertheless, accuracy was not the sole criteria of choice for the Text in Image 2.0, so we compiled a list of edge cases where our partner marketplaces require high-performing algorithms.&lt;/p&gt;

&lt;p&gt;After reviewing different open source OCR frameworks (including &lt;a href=&quot;https://github.com/open-mmlab/mmocr&quot;&gt;MMOCR&lt;/a&gt;, &lt;a href=&quot;https://github.com/JaidedAI/EasyOCR&quot;&gt;EASY OCR&lt;/a&gt;, &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR&quot;&gt;PaddleOCR&lt;/a&gt; and &lt;a href=&quot;https://thehive.ai/apis/ocr&quot;&gt;HiveOCR&lt;/a&gt;) and different combinations of proposed models on our internal benchmark and on the edge cases, a indisputable winner was PaddleOCR with an average accuracy of 0.8 and an acceptable performance on our edge cases. This result competes with the paid &lt;a href=&quot;https://cloud.google.com/vision/docs/ocr&quot;&gt;Google Cloud Vision OCR API&lt;/a&gt; on the best accuracy we measured.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*UUEf-TKs1Lfn7_wx&quot; alt=&quot;Graph showing benchmark results for various OCR frameworks&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;h2 id=&quot;how-we-validated-paddleocr-building-a-comprehensive-benchmark&quot;&gt;How We Validated PaddleOCR: Building a Comprehensive Benchmark&lt;/h2&gt;

&lt;p&gt;In order to construct our independent benchmark and validate the choice of PaddleOCR at scale, we built a “Text in Image generator” that uses open source images from &lt;a href=&quot;https://unsplash.com/license&quot;&gt;Unsplash&lt;/a&gt; and &lt;a href=&quot;https://pikwizard.com/free-license&quot;&gt;Pikwizard&lt;/a&gt; and adds randomly generated text on top of them. The created tool is highly customisable in order to simulate a wide variety of cases that combine factors such as font type, rotation, text length, background type, image resolution etc. Using a simulated benchmark of 20k images with a distribution of cases matching business needs, we obtained an improvement factor of x1.4.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*sWpBlrJtdxsRlqj4&quot; alt=&quot;Sample of Text in Image generator output showing simulated text scenarios&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;h2 id=&quot;challenges-with-paddleocr-identifying-and-mitigating-issues&quot;&gt;Challenges with PaddleOCR: Identifying and mitigating issues&lt;/h2&gt;

&lt;p&gt;We identified several cases where PaddleOCR fails. This is mostly when there are different angles of rotated text, some alternative fonts and differing colour/contrast. We also observed that in some cases, the correct words are detected but the spaces between them are not placed correctly. This may or may not be an issue depending on the way the extracted text is used further.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*3CO2dWUYPpVPPBZJDpx4EA.png&quot; alt=&quot;Example of OCR results with incorrectly spaced text&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;h2 id=&quot;deep-dive-how-we-optimized-paddleocr-for-production&quot;&gt;Deep Dive: How We Optimized PaddleOCR for Production&lt;/h2&gt;

&lt;p&gt;In order to evaluate the potential for improvement and mitigation of these errors, in addition to defining the serving strategy, we had to deep dive into the PaddleOCR framework.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR&quot;&gt;PaddleOCR&lt;/a&gt; builds on &lt;a href=&quot;https://github.com/PaddlePaddle/Paddle&quot;&gt;PaddlePaddle.&lt;/a&gt; Our team had no previous experience with this and it’s less popular in our community than other frameworks such as Tensorflow, Keras or Pytorch.&lt;/p&gt;

&lt;p&gt;From a technical point of view, PaddleOCR is composed of three distinct models:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Detection&lt;/strong&gt;, for detecting a bounding box where possible text is&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Classification&lt;/strong&gt;, rotating the text 180° if necessary&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Recognition&lt;/strong&gt;, translating the detected image frame to raw text&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Pre-trained models in different languages are &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/blob/18ddb6d5f9bdc2c1b0aa7f6e399ec0f76119dc87/doc/doc_en/models_list_en.md&quot;&gt;provided by authors&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;refactoring-paddleocr-creating-a-clean-production-ready-codebase&quot;&gt;Refactoring PaddleOCR: Creating a Clean, Production-Ready Codebase&lt;/h3&gt;

&lt;p&gt;Whilst exploring the code base of PaddleOCR for inference, we were faced with convoluted code, which was difficult to read and understand. As we wanted to use the PaddleOCR solution in production, we decided to refactor the code, keeping in mind to preserve the performance and the speed of the original code. You can read about the details of that process and the PaddleOCR model in the complementary article of this series. After refactoring the code, we had created a clean and readable code base.&lt;/p&gt;

&lt;p&gt;We believe our code version is easier to work with, given the use case of text extraction from images, and are working on making the code available open source. The different steps and pre-processing and post-processing parts are clearly separated, so they can be called independently, which should make further community extensions easier to add. It also makes putting into production easier, as the simplified, modular code combines well with the structure of inference.py for serving SageMaker endpoints. Our proposed code version does not alter predictions (compared to the 2.6 release) for images.&lt;/p&gt;

&lt;h2 id=&quot;deploying-text-in-image-20-achieving-superior-performance-with-paddleocr&quot;&gt;Deploying Text in Image 2.0: Achieving Superior Performance with PaddleOCR&lt;/h2&gt;

&lt;p&gt;Using the refactored code, we made the model available as an API. To help our customers’ transition, we maintained the same API contract used in the previous service.&lt;/p&gt;

&lt;p&gt;Serving PaddleOCR can be done in multiple ways. The straightforward approach is calling its own Python API (provided by the &lt;a href=&quot;https://pypi.org/project/paddleocr/&quot;&gt;PaddleOCR&lt;/a&gt; package) from within a well-known framework. We selected Multi Model Server, Flask and FastAPI to conduct our benchmark. All our proposed solutions are served by AWS SageMaker Endpoint, building our own container (BYOC) from the same Docker base image.&lt;/p&gt;

&lt;p&gt;MultiModel Server uses its own JAVA ModelServer, while for Flask and FastAPI, we use nginx+gunicorn (combined with &lt;a href=&quot;https://fastapi.tiangolo.com/deployment/server-workers/&quot;&gt;uvicorn workers for the ASGI FastAPI&lt;/a&gt;). The frontend for our customers is served by an API Gateway, which is out of the scope of this article.&lt;/p&gt;

&lt;h2 id=&quot;benchmarking-deployment-options-multi-model-server-flask-and-fastapi&quot;&gt;Benchmarking Deployment Options: Multi-Model Server, Flask, and FastAPI&lt;/h2&gt;

&lt;p&gt;For the performance testing, we recreated a number of requests with a controlled amount of text and different image sizes, mimicking the expected distribution from our customers. We used &lt;a href=&quot;https://locust.io/&quot;&gt;Locust&lt;/a&gt; as the testing framework, and stimulated heavy bursts in the &lt;a href=&quot;https://docs.locust.io/en/stable/writing-a-locustfile.html#wait-time-attribute&quot;&gt;waiting time&lt;/a&gt; as a stress test.&lt;/p&gt;

&lt;p&gt;With the data gathered from the performance tests, we were able to define our infrastructure (type of instance and autoscaling policy) in relation to the Service Level Agreement (SLA) terms, while balancing the risk of a sudden shift from the observed distribution (the service is sensitive to the amount of text per image).&lt;/p&gt;

&lt;p&gt;Currently, we deal with 330 million requests per month, and we have estimated that next year, more Adevinta marketplaces will onboard a Text in Image service, resulting in a 400% growth.&lt;/p&gt;

&lt;h2 id=&quot;results-and-impact-transforming-text-in-image-service-with-paddleocr&quot;&gt;Results and impact: Transforming Text in Image service with PaddleOCR&lt;/h2&gt;

&lt;p&gt;The new API resulted in an improved latency 7.5x compared to the FOTS-based solution, while providing a 7% cost reduction in serving. Also, since the new API being 12x cheaper than a typical external solution, such as GCP OCR, we received positive feedback from our users about both the speed and the accuracy of the Text in Image 2.0.&lt;/p&gt;

&lt;h2 id=&quot;key-takeaways-enhancing-ocr-with-paddleocr&quot;&gt;Key Takeaways: Enhancing OCR with PaddleOCR&lt;/h2&gt;

&lt;p&gt;As a computer vision team working for an international company serving millions of people every day, we aimed to improve our OCR API for text extraction from classified ads. After testing numerous frameworks, we built an image simulator in order to find the algorithm matching the needs of our users. The selected framework, PaddleOCR, went through our internal review and revamp. (There were challenges along the way and you can read more about them in &lt;a href=&quot;/works/deep-dive-in-paddleocr-inference&quot;&gt;&lt;strong&gt;Article 2: Deep Dive in PaddleOCR inference&lt;/strong&gt;&lt;/a&gt;). Now, we’re pleased to say we’re providing a more accurate, faster and cheaper API using the PaddleOCR framework.&lt;/p&gt;

&lt;footer&gt;
  &lt;p&gt;Exported from &lt;a href=&quot;https://medium.com&quot;&gt;Medium&lt;/a&gt; on June 06,
    2023.&lt;/p&gt;
  &lt;p&gt;&lt;a href=&quot;https://medium.com/adevinta-tech-blog/text-in-image-2-0-improving-ocr-service-with-paddleocr-61614c886f93&quot;&gt;This article was orignally co-authored by Cognition team members, special credits to Joaquin Cabezas&lt;/a&gt;&lt;/p&gt;
&lt;/footer&gt;
&lt;script type=&quot;text/javascript&quot; src=&quot;//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-584ec4ce89deed84&quot;&gt;&lt;/script&gt;

&lt;div class=&quot;addthis_inline_share_toolbox&quot;&gt;&lt;/div&gt;

</description>
        <pubDate>Mon, 06 Mar 2023 00:00:00 +0100</pubDate>
        <link>http://urszulaczerwinska.github.io/works/text-in-image-2-0-improving-ocr-service-with-paddleocr</link>
        <guid isPermaLink="true">http://urszulaczerwinska.github.io/works/text-in-image-2-0-improving-ocr-service-with-paddleocr</guid>
        
        <category>data science</category>
        
        <category>OCR</category>
        
        <category>deep learning</category>
        
        <category>computer vision</category>
        
        <category>machine learning</category>
        
        <category>API</category>
        
        
        <category>works</category>
        
      </item>
    
      <item>
        <title>Named Entity Recognition Tool by Cour de Cassation</title>
        <description>&lt;!DOCTYPE HTML&gt;
&lt;html lang=&quot;en-US&quot;&gt;
    &lt;head&gt;
        &lt;meta charset=&quot;UTF-8&quot;&gt;
        &lt;meta http-equiv=&quot;refresh&quot; content=&quot;0; url=https://github.com/Cour-de-cassation/moteurNER&quot;&gt;
        &lt;meta name=&quot;description&quot; content=&quot;Redirecting to the Named Entity Recognition (NER) tool repository by Cour de Cassation. It is a page about building deep learning NLP application for French justice&quot;&gt;
        &lt;script type=&quot;text/javascript&quot;&gt;
            window.location.href = &quot;https://github.com/Cour-de-cassation/moteurNER&quot;
        &lt;/script&gt;
        &lt;title&gt;Redirecting to Named Entity Recognition Tool by Cour de Cassation, building NER, NLP deep learning applications for French Supreme Court.&lt;/title&gt;
    &lt;/head&gt;
    &lt;body&gt;
        &lt;p&gt;You are being redirected to the Named Entity Recognition tool repository by Cour de Cassation. If you are not redirected automatically, &lt;a rel=&quot;canonical&quot; href=&apos;https://github.com/Cour-de-cassation/moteurNER&apos;&gt;click here to proceed to the repository.&lt;/a&gt;&lt;/p&gt;
    &lt;/body&gt;

      &lt;!-- Go to www.addthis.com/dashboard to customize your tools --&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-584ec4ce89deed84&quot;&gt;&lt;/script&gt;
&lt;!-- Go to www.addthis.com/dashboard to customize your tools --&gt; &lt;div class=&quot;addthis_inline_share_toolbox&quot;&gt;&lt;/div&gt;

&lt;/html&gt;</description>
        <pubDate>Wed, 01 Sep 2021 00:00:00 +0200</pubDate>
        <link>http://urszulaczerwinska.github.io/works/ner_cc</link>
        <guid isPermaLink="true">http://urszulaczerwinska.github.io/works/ner_cc</guid>
        
        <category>Machine Learning</category>
        
        <category>Python</category>
        
        <category>Deep Learning</category>
        
        <category>NER</category>
        
        <category>NLP</category>
        
        <category>Justice</category>
        
        <category>Flair</category>
        
        <category>featured</category>
        
        
        <category>works</category>
        
      </item>
    
      <item>
        <title>Push the limits of machine learning explainability</title>
        <description>&lt;head&gt;
  &lt;meta
    name=&quot;description&quot;
    content=&quot;Explore the power of SHAP in enhancing model interpretability in data science and machine learning. This guide provides detailed insights and practical examples for data professionals.&quot;
  /&gt;
&lt;/head&gt;

&lt;section&gt;
  &lt;h1&gt;
    Summary - A Comprehensive Guide to SHAP: Enhancing Machine Learning
    Interpretability
  &lt;/h1&gt;
  &lt;p&gt;
    This article is a guide to the advanced and lesser-known features of the
    python SHAP library. It is based on an example of tabular data
    classification.
  &lt;/p&gt;
  &lt;p&gt;
    But first, let’s talk about the motivation and interest in explainability at
    Saegus that motivated and financed my explorations.
  &lt;/p&gt;

  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img src=&quot;/images/engineering.jpg&quot; alt=&quot;&quot; width=&quot;70%&quot;
  /&gt;&lt;/span&gt;

  &lt;h3&gt;The Theory Behind Explainability in AI and Machine Learning&lt;/h3&gt;
  &lt;p&gt;
    The explainability of algorithms is taking more and more place in the
    discussions about Data Science. We know that algorithms are powerful, we
    know that they can assist us in many tasks: price prediction, document
    classification, video recommendation.
  &lt;/p&gt;
  &lt;p&gt;
    From now on, more and more questions are being asked about this
    prediction:&lt;br /&gt;- Is it ethical?&lt;br /&gt;- Is it affected by bias?&lt;br /&gt;- Is
    it used for the right reasons?
  &lt;/p&gt;
  &lt;p&gt;
    In many domains such as medicine, banking or insurance, algorithms can be
    used if, and only if, it is possible to trace and explain (or better,
    interpret) the decisions of these algorithms.
  &lt;/p&gt;
  &lt;h4&gt;Key Terminology in Machine Learning Interpretability and SHAP&lt;/h4&gt;
  &lt;p&gt;In this article we would like to distinguish the terms:&lt;/p&gt;
  &lt;p&gt;
    &lt;strong&gt;Explainability&lt;/strong&gt;: possibility to explain from a technical
    point of view the prediction of an algorithm.
  &lt;/p&gt;
  &lt;p&gt;
    &lt;strong&gt;Interpretability&lt;/strong&gt;: the ability to explain or provide meaning
    in terms that are understandable by a human being.
  &lt;/p&gt;
  &lt;p&gt;
    &lt;strong&gt;Transparency&lt;/strong&gt;: a model is considered transparent if it is
    understandable on its own.
  &lt;/p&gt;
  &lt;h4&gt;Why Explainability Matters in Data Science ?&lt;/h4&gt;
  &lt;p&gt;
    Interpretability helps to ensure impartiality in decision-making, i.e. to
    detect and therefore correct biases in the training data set. In addition,
    it facilitates robustness by highlighting potential adverse disturbances
    that could change the prediction. It can also act as an assurance that only
    significant features infer the outcome.
  &lt;/p&gt;
  &lt;p&gt;
    Sometimes, it would be more advisable to abandon the machine learning
    approach, and use deterministic algorithms based on rules justified by
    industry knowledge or legislation [1].
  &lt;/p&gt;
  &lt;p&gt;
    Nevertheless, it is too tempting to access the capabilities of machine
    learning algorithms that can offer high accuracy. We can talk about the
    trade-off between accuracy and explainability. This trade-off consists in
    discarding more complex models such as neural networks for simpler
    algorithms that can be explained.
  &lt;/p&gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img src=&quot;/images/model-interpret.png&quot; alt=&quot;&quot; /&gt;&lt;em
      &gt;As described in [2] relation between interpretability and accuracy of the
      model. For some models improvements can be made towards a more
      interpretable or more relevant model.&lt;/em
    &gt;&lt;/span
  &gt;
  &lt;p&gt;
    To achieve these goals, a new field has emerged: XAI (Explainable Artificial
    Intelligence), which aims to produce algorithms that are both powerful and
    explainable.
  &lt;/p&gt;
  &lt;p&gt;
    Many frameworks have been proposed to help explain non-transparent
    algorithms. A very good presentation of these methods can be found in the
    Cloudera white paper [3].
  &lt;/p&gt;
  &lt;p&gt;
    In this article we will deal with one of the most used frameworks: SHAP.
  &lt;/p&gt;
  &lt;h4&gt;Exploring the Audience for Explainable AI in Data Science&lt;/h4&gt;
  &lt;p&gt;
    Different profiles interested in expainability or interpretability have been
    identified:
  &lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;
      Business expert/model user — in order to trust the model, understand the
      causality of the prediction
    &lt;/li&gt;
    &lt;li&gt;
      Regulatory bodies to certify compliance with the legislation, auditing
    &lt;/li&gt;
    &lt;li&gt;
      Managers and executive board to assess regulatory compliance, understand
      enterprise AI applications
    &lt;/li&gt;
    &lt;li&gt;
      Users impacted by model decisions in order to understand the situation,
      verify decisions
    &lt;/li&gt;
    &lt;li&gt;
      Data scientist, developer, PO to ensure/improve product performance, find
      new features, explain functioning/predictions to superiors
    &lt;/li&gt;
  &lt;/ul&gt;
  &lt;p&gt;
    In order to make explainability accessible to people with low technical
    skills, first of all, the creator: a data scientist/developer must be
    comfortable with the tools of explainability.
  &lt;/p&gt;
  &lt;p&gt;
    The data scientist will use them above all to understand and improve his
    model and then to communicate with his superiors and regulatory bodies.&lt;br /&gt;Recently,
    explainability tools have become more and more accessible.
  &lt;/p&gt;
  &lt;p&gt;
    For example,
    &lt;a
      href=&quot;https://www.dataiku.com/&quot;
      data-href=&quot;https://www.dataiku.com/&quot;
      class=&quot;markup--anchor markup--p-anchor&quot;
      rel=&quot;noopener&quot;
      target=&quot;_blank&quot;
      &gt;Dataiku &lt;/a
    &gt;— ML’s platform — has added in its latest version 7.0 published on March 2,
    2020 explainability tools: Shapley values and “The Individual Conditional
    Expectation” (ICE).
  &lt;/p&gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img src=&quot;/images/dataiku.png&quot; alt=&quot;&quot; /&gt;&lt;em
      &gt;Dataiku prediction studio&lt;/em
    &gt;&lt;/span
  &gt;
  &lt;p&gt;
    &lt;a href=&quot;https://azure.microsoft.com/en-us/services/machine-learning/&quot;
      &gt;Azure ML&lt;/a
    &gt;
    proposes its own version of Shap and alternative tools adding interactive
    &lt;a
      href=&quot;https://docs.microsoft.com/en-us/azure/machine-learning/how-to-machine-learning-interpretability-aml&quot;
      &gt;dashboards&lt;/a
    &gt;.
  &lt;/p&gt;

  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img src=&quot;/images/azure.png&quot; alt=&quot;&quot; /&gt;&lt;em
      &gt;Azure ML interpretability dashboard&lt;/em
    &gt;&lt;/span
  &gt;

  &lt;p&gt;
    There are also open-source webapps such as this one described in the medium
    article [4] that facilitate the exploration of the SHAP library.
  &lt;/p&gt;
  &lt;div&gt;
    &lt;a
      href=&quot;https://towardsdatascience.com/understand-the-machine-learning-blackbox-with-ml-interpreter-7b0f9a2d8e9f&quot;
      &gt;&lt;strong
        &gt;Understand the machine learning Blackbox with ML interpreter&lt;/strong
      &gt;&lt;br /&gt;&lt;em class=&quot;markup--em markup--mixtapeEmbed-em&quot;
        &gt;There are dangers in having models running the world and making
        decisions from hiring to criminal justice&lt;/em
      &gt;towardsdatascience.com&lt;/a
    &gt;&lt;a
      href=&quot;https://towardsdatascience.com/understand-the-machine-learning-blackbox-with-ml-interpreter-7b0f9a2d8e9f&quot;
    &gt;&lt;/a&gt;
  &lt;/div&gt;
  &lt;div&gt;&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;
  &lt;p&gt;
    These tools, very interesting to get a quick overview of interpretation, do
    not necessarily give an understanding of the full potential of the SHAP
    library. Few allow to explore interaction values or to use different
    background or display sets.
  &lt;/p&gt;
  &lt;p&gt;
    I investigated the SHAP framework and I present you my remarks and the usage
    of less known features, available in the official version of the library in
    open source. I also propose some interactive visualizations easy to
    integrate in your projects.
  &lt;/p&gt;
  &lt;h3&gt;Step-by-Step Guide: Using SHAP for Machine Learning Models&lt;/h3&gt;
  &lt;p&gt;
    Most data scientists have already heard of the SHAP framework.&lt;br /&gt;In this
    post, we won’t explain in detail how the calculations behind the library are
    done. Many resources are available online such as the SHAP documentation
    [5], publications by authors of the library [6,7], the great book
    “Interpretable Machine Learning” [8] and multiple medium articles [9,10,11].
  &lt;/p&gt;
  &lt;p&gt;
    In summary, Shapley’s values calculate the importance of a feature by
    comparing what a model predicts with and without this feature. However,
    since the order in which a model sees the features can affect its
    predictions, this is done in all possible ways, so that the features are
    compared fairly. This approach is inspired by game theory.
  &lt;/p&gt;
  &lt;p&gt;
    Having worked with many clients, for example in the banking and insurance
    sectors, one can see that their data scientists are struggling to exploit
    the full potential of SHAP. They don’t know how this tool could really be
    useful for understanding a model and how to use it to go beyond simply
    extracting the importance of features.
  &lt;/p&gt;
  &lt;blockquote&gt;The devil is in the detail&lt;/blockquote&gt;
  &lt;p&gt;
    SHAP comes with a set of visualizations that are quite complex and not
    always intuitive, even for a data scientist.
  &lt;/p&gt;
  &lt;p&gt;
    On top of that, there are several technical nuances to be able to use SHAP
    with your data. Francesco Porchetti’s blog article [12] expresses some of
    these frustrations by exploring the SHAP,
    &lt;a href=&quot;https://github.com/marcotcr/lime&quot;&gt;LIME&lt;/a&gt;,
    &lt;a
      href=&quot;https://github.com/SauceCat/PDPbox&quot;
      data-href=&quot;https://github.com/SauceCat/PDPbox&quot;
      &gt;PDPbox &lt;/a
    &gt;(PDP and ICE) and
    &lt;a
      href=&quot;https://eli5.readthedocs.io/en/latest/autodocs/sklearn.html#module-eli5.sklearn.permutation_importance&quot;
      &gt;ELI5 &lt;/a
    &gt;libraries.
  &lt;/p&gt;
  &lt;p&gt;
    &lt;strong
      &gt;&lt;em
        &gt;At Saegus, I worked on a course which aims to give more clarity to the
        SHAP framework and to facilitate the use of this tool.&lt;/em
      &gt;&lt;/strong
    &gt;
  &lt;/p&gt;
  &lt;p&gt;
    In this post I would like to share with you some observations collected
    during that process.
  &lt;/p&gt;
  &lt;p&gt;
    SHAP is used to explain an existing model. Taking a binary classification
    case built with a sklearn model. We train, tune and test our model. Then we
    can use our data and the model to create an additional SHAP model that
    explains our classification model.
  &lt;/p&gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/shap.png&quot;
      alt=&quot;SHAP plot demonstrating deep learning AI model interpretability in data science.&quot;
    /&gt;&lt;em&gt;Image source: SHAP github&lt;/em&gt;&lt;/span
  &gt;
  &lt;h4&gt;Vocabulary&lt;/h4&gt;
  &lt;p&gt;
    It is important to understand all the bricks that make up a SHAP
    explanation.
  &lt;/p&gt;
  &lt;p&gt;
    Often, by using default values for parameters, the complexity of the choices
    we make remains obscure.
  &lt;/p&gt;
  &lt;p&gt;
    &lt;strong&gt;global explanations&lt;br /&gt;&lt;/strong&gt;explanations of how the model
    works from a general point of view
  &lt;/p&gt;
  &lt;p&gt;
    &lt;strong&gt;local explanations&lt;/strong&gt;&lt;br /&gt;explanations of the model for a
    sample (a data point)
  &lt;/p&gt;
  &lt;p&gt;
    &lt;strong&gt;explainer &lt;/strong&gt;(shap.explainer_type(params))&lt;br /&gt;type of
    explainability algorithm to be chosen according to the model used.
  &lt;/p&gt;
  &lt;p&gt;
    The parameters are different for each type of model. Usually, the model and
    training data must be provided, at a minimum.
  &lt;/p&gt;
  &lt;p&gt;
    &lt;strong&gt;base value&lt;/strong&gt; (explainer.expected_value)&lt;br /&gt;&lt;em
      &gt;E(y_hat)&lt;/em
    &gt;
    is “the value that would be predicted if we didn’t know any features of the
    current output” is the &lt;em&gt;mean(y_hat)&lt;/em&gt; prediction for the training data
    set or the background set. We can call it “reference value”, it’s a scalar
    (&lt;em&gt;n&lt;/em&gt;).
  &lt;/p&gt;
  &lt;p&gt;
    It’s important to choose your background set carefully — if we have the
    unbalanced training set this will result in a base value placed among the
    majority of samples. This can also be a desired effect: for example if for a
    bank loan we want to answer the question: “how is the customer in question
    different from customers who have been approved for the loan” or “how is my
    false positive different from the true positives”.
  &lt;/p&gt;
  &lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt; &lt;span class=&quot;c1&quot;&gt;# equilibrated case background = X.sample(1000) #X is
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;equilibrated&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# background used in explainer defines base value explainer =
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;shap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;TreeExplainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xgb_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model_output&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;raw&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shap_values&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;explainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;shap_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# background used in the plot, the points that are
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;visible&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;on&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;summary_plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shap_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;feature_names&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt; &lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt; &lt;span class=&quot;c1&quot;&gt;#
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;base&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shifted&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;class1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;class1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,:]&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#X is equilibrated # background
&lt;/span&gt;  &lt;span class=&quot;k&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;1 &lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;used&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;explainer&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;defines&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;base&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;explainer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;shap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;TreeExplainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xgb_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;class1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model_output&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;raw&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shap_values&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;explainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;shap_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# points from class 0 is used in the plot, the points
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;that&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;are&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;visible&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;on&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;summary_plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shap_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;class0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,:],&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;feature_names&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img src=&quot;/images/background-shap.png&quot; alt=&quot;&quot; /&gt;&lt;em
      &gt;Selecting the background dataset changes the question answered by
      shap values.&lt;/em
    &gt;&lt;/span
  &gt;
  &lt;p&gt;
    &lt;strong&gt;SHAPley values&lt;/strong&gt; (explainer.shap_values(x))&lt;br /&gt;the average
    contribution of each feature to each prediction for each sample based on all
    possible features. It is a (&lt;em&gt;n,m&lt;/em&gt;) &lt;em&gt;n &lt;/em&gt;— samples,
    &lt;em&gt;m &lt;/em&gt;— features matrix that represents the contribution of each
    feature to each sample.
  &lt;/p&gt;
  &lt;p&gt;
    &lt;strong&gt;output value&lt;/strong&gt; (for a sample)&lt;br /&gt;the value predicted by the
    algorithm (the probability, logit or raw output values of the model)
  &lt;/p&gt;
  &lt;p&gt;
    &lt;strong&gt;display features&lt;/strong&gt; (&lt;em&gt;n &lt;/em&gt;x &lt;em&gt;m&lt;/em&gt;)&lt;br /&gt;a matrix of
    original values — before transformation/encoding/engineering of features
    etc. — that can be provided to some graphs to improve interpretation. Often
    overlooked and essential for interpretation.
  &lt;/p&gt;
  &lt;p&gt;____&lt;/p&gt;
  &lt;p&gt;&lt;strong&gt;SHAPley values&lt;/strong&gt;&lt;/p&gt;
  &lt;p&gt;
    Shapley values remain the central element. Once we realize that this is
    simply a matrix with the same dimensions as our input data and that we can
    analyze it in different ways to explain the model and not only. We can
    reduce its dimensions, we can cluster it, we can use it to create new
    features. An interesting exploration described in the article [12] aims at
    improving anomaly detection using auto encoders and SHAP. The SHAP library
    proposes a rich but not exchaustive exploration through visualizations.
  &lt;/p&gt;
  &lt;h4&gt;Visualizing SHAP: Enhancing Interpretability in Deep Learning Models&lt;/h4&gt;
  &lt;p&gt;
    The SHAP library offers different visualizations. A good explanation on how
    to read the colors of the summary plot can be found in this medium article
    [14].
  &lt;/p&gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img src=&quot;/images/shap-global.png&quot; alt=&quot;&quot; /&gt;&lt;em
      &gt;A summary of graphical visualizations to analyze global explanations&lt;/em
    &gt;&lt;/span
  &gt;
  &lt;p&gt;
    &lt;strong&gt;The summary plot&lt;/strong&gt; shows the most important features and the
    magnitude of their impact on the model. It can take several graphical forms
    and for the models explained by TreeExplainer we can also observe the&lt;strong
      &gt;&lt;em&gt; interaction values &lt;/em&gt;&lt;/strong
    &gt;using the “compact dot” with shap_interaction_values in input.
  &lt;/p&gt;
  &lt;p&gt;
    &lt;strong&gt;The dependency plot&lt;/strong&gt; allows to analyze the features two by
    two by suggesting a possibility to observe the interactions. The scatter
    plot represents a dependency between a feature(x) and the shapley values (y)
    colored by a second feature(hue).
  &lt;/p&gt;
  &lt;p&gt;
    On a personal note, I find that an observation of a three-factor
    relationship at the same time is not intuitive for the human brain (at least
    mine). I also doubt that an observation of dependency by observing colours
    can be scientifically accurate. Shap can give us an interaction relationship
    that is calculated as a correlation between the shapley values of the first
    feature and the values of the second feature. If possible (for
    TreeExplainer) it makes more sense to use the shapley interaction values to
    observe interactions.
  &lt;/p&gt;
  &lt;figure&gt;
    &lt;script src=&quot;https://gist.github.com/UrszulaCzerwinska/b7853f5f0b209f2f89e2a3590dd6f329.js&quot;&gt;&lt;/script&gt;
    &lt;figcaption class=&quot;imageCaption&quot;&gt;
      Snippet code to reproduce my dependence plot variant.
    &lt;/figcaption&gt;
  &lt;/figure&gt;
  &lt;p&gt;
    I propose an interactive variant of dependency plot that allows to observe
    the relationship between a feature(x), the shapley values (y) and the
    prediction (histogram colors). What seems important to me in this version is
    the possibility to display on the graph the original values (Income in k
    USD) instead of the normalized space used by the model.
  &lt;/p&gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/my-dep-plot.png&quot;
      alt=&quot;Histogram visualizing SHAP interaction values in a data science model analysis.&quot;
    /&gt;&lt;em&gt;My variant of dependence plot&lt;/em&gt;&lt;/span
  &gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/shap-local.png&quot;
      alt=&quot;Histogram visualizing SHAP interaction values in a data science model analysis.&quot;
    /&gt;&lt;em
      &gt;a summary of graphical visualizations to analyze local explanations&lt;/em
    &gt;&lt;/span
  &gt;
  &lt;p&gt;
    There are three alternatives for the visualization of explanations of a
    sample: force plot, decision plot and waterfall plot.
  &lt;/p&gt;
  &lt;p&gt;
    For a sample, these three representations are redundant, they represent the
    information in a very similar way. At the same time, some elements of these
    graphs are complementary. By putting the three side by side, I have the
    impression to understand the result in a more intuitive way. The force plot
    is good to see where the “output value” fits in relation to the “base
    value”. We also see which features have a positive (red) or negative (blue)
    impact on the prediction and the magnitude of this impact. The water plot
    also allows us to see the amplitude and the nature of the impact of a
    feature with its quantification. It also allows to see the order of
    importance of the features and the values taken by each feature for the
    studied sample. The Decision plot makes it possible to observe the amplitude
    of each change, “a trajectory” taken by a samplefor the values of the
    displayed features.
  &lt;/p&gt;
  &lt;p&gt;
    By using force plot and decision plot we can represent several samples at
    the same time.
  &lt;/p&gt;
  &lt;p&gt;
    The force plot for a set of samples can be compared to the last level of a
    dendrogram. The samples are grouped by similarity or by selected feature. In
    my opinion, this graph is difficult to read for a random sample. It is much
    more meaningful if we represent the contrasting cases or with a hypothesis
    behind.
  &lt;/p&gt;
  &lt;p&gt;
    The decision plot, for a set of samples, quickly becomes cumbersome if we
    select too many samples. It is very useful to observe a ‘trajectory
    deviation’ or ‘diverging/converging trajectories’ of a limited group of
    samples.
  &lt;/p&gt;
  &lt;h4&gt;
    Explainers in SHAP: Understanding Different Model Interpretability
    Approaches
  &lt;/h4&gt;
  &lt;p&gt;
    Explainers are the models used to calculate shapley values. The diagram
    below shows different types of Explainers.
  &lt;/p&gt;
  &lt;p&gt;
    The choice of Explainers depends mainly on the selected learning model. For
    linear models, the “Linear Explainer” is used, for decision trees and “set”
    type models — “TreeExplainer”. “Kernel Explainer” is slower than the above
    mentioned explainers.
  &lt;/p&gt;
  &lt;p&gt;
    In addition the “Tree Explainer” allows to display the interaction values
    (see next section). It also allows to transform the model output into
    probabilities or logloss, which is useful for a better understanding of the
    model or to compare several models.
  &lt;/p&gt;
  &lt;p&gt;
    The Kernel Explainer creates a model that substitutes the closest to our
    model. Kernel Explainer can be used to explain neural networks. For deep
    learning models, there are the Deep Explainer and the Grandient Explainer.
    For this paper we have not investigated the explainability of neural
    networks.
  &lt;/p&gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/explainer.png&quot;
      alt=&quot;Force plot visualization of SHAP values enhancing transparency in AI model predictions; Waterfall plot depicting SHAP value contributions to machine learning predictions.&quot;
    /&gt;&lt;em&gt;a summary of Explainer types in the SHAP library&lt;/em&gt;&lt;/span
  &gt;
  &lt;h4&gt;Shapley values of interactions&lt;/h4&gt;
  &lt;p&gt;
    One of the properties that allows to go further in the analysis of a model
    that can be explained with the “Tree Explainer” is the calculation of
    shapley values of interactions.
  &lt;/p&gt;
  &lt;p&gt;
    These values make it possible to quantify the impact of an interaction
    between two features on the prediction for each sample. As the matrix of
    shapley values has two dimensions (samples x features), the interactions are
    a tensor with three dimensions (samples x features x features).
  &lt;/p&gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/interactions-1.png&quot;
      alt=&quot;Interactive SHAP dependence plot showing feature impact in deep learning models.&quot;
    /&gt;&lt;em
      &gt;a summary of graphical visualizations to analyze local explanations&lt;/em
    &gt;&lt;/span
  &gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/interactions-2.png&quot;
      alt=&quot;Interactive SHAP dependence plot showing feature impact in deep learning models.&quot;
    /&gt;&lt;em
      &gt;a summary of graphical visualizations to analyze local explanations&lt;/em
    &gt;&lt;/span
  &gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/interactions-3.png&quot;
      alt=&quot;Interactive SHAP dependence plot showing feature impact in deep learning models.&quot;
    /&gt;&lt;em
      &gt;a summary of graphical visualizations to analyze local explanations&lt;/em
    &gt;&lt;/span
  &gt;

  &lt;p&gt;
    &lt;strong
      &gt;Here’s how interaction values help interpret a binary classification
      model.&lt;/strong
    &gt;
  &lt;/p&gt;
  &lt;p&gt;
    I used a Kaggle [15] dataset that represents a client base and the binary
    dependent feature: did the client accept the personal loan? NO/YES (0/1).
  &lt;/p&gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/table.png&quot;
      alt=&quot;A summary of SHAP graphical
      visualizations to analyze local explanations of machine learning model&quot;
    /&gt;&lt;em
      &gt;a summary of graphical visualizations to analyze local explanations&lt;/em
    &gt;&lt;/span
  &gt;

  &lt;p&gt;
    I’ve trained several models, including an xgboost model that we treated with
    the Tree Explainer.
  &lt;/p&gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/sklearn-model.png&quot;
      alt=&quot;A summary of SHAP graphical
        visualizations to analyze local explanations of machine learning model&quot;
    /&gt;&lt;em
      &gt;a summary of graphical visualizations to analyze local explanations&lt;/em
    &gt;&lt;/span
  &gt;

  &lt;p&gt;The background dataset was balanced and represented 40% of the dataset.&lt;/p&gt;
  &lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt; &lt;span class=&quot;c1&quot;&gt;# xgb - traned model # X_background - background
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;explainer_raw&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;TreeExplainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xgb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;model_output&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;raw&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;feature_perturbation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;tree_path_dependent&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# project
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;point&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;background&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datasetshap_values&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;explainer_raw&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;shap_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# obtain interaction values
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;shap_interaction_values&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;explainer_raw&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;shap_interaction_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;# dimensions shap_values.shape &amp;gt;&amp;gt;&amp;gt;(2543, 16) shap_interaction_values.shape
&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2543&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;summary_plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shap_interaction_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;plot_type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;compact_dot&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/summary-plot.png&quot;
      alt=&quot;Summary plot showcasing SHAP values and feature interactions in machine learning models.&quot;
    /&gt;&lt;em&gt;Summary plot with interactions&lt;/em&gt;&lt;/span
  &gt;

  &lt;p&gt;To better explore interactions, a heatmap can be very useful.&lt;/p&gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/histogram.png&quot;
      alt=&quot;Heatmap illustrating SHAP values and feature interactions in machine learning models.&quot;
    /&gt;&lt;em&gt;Histogram of interaction values&lt;/em&gt;&lt;/span
  &gt;
  &lt;p&gt;
    In the Summary_plot one can observe the importance of features and the
    importance the interactions. The interactions appear in double which
    confuses a little the reading.
  &lt;/p&gt;
  &lt;p&gt;
    In the histogram, we observe directly the interactions. The strongests of
    them of being: Income-Education, Income — Family, Income — CCAvg and
    Family-Education, Income-Age.
  &lt;/p&gt;
  &lt;p&gt;
    Then I investigated the interactions two by two.&lt;br /&gt;To understand the
    difference between a dependency_plot and a dependency_plot of interactions
    here are the two:
  &lt;/p&gt;

  &lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt; &lt;span class=&quot;c1&quot;&gt;# dependence_plot classique shap.dependence_plot(&quot;Age&quot;,
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;shap_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;features&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;display_features&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_background_display&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;interaction_index&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Income&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/dep-plot.png&quot;
      alt=&quot;SHAP dependency plot illustrating interaction effects in machine learning algorithm&quot;
  /&gt;&lt;/span&gt;
  &lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt; &lt;span class=&quot;c1&quot;&gt;# dependence_plot des interactions
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;shap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;dependence_plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Age&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Income&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shap_interaction_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;features&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;X_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;display_features&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_background_display&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/interaction-dep-plot.png&quot;
      alt=&quot;SHAP dependency plot illustrating interaction effects in machine learning algorithm&quot;
  /&gt;&lt;/span&gt;

  &lt;p&gt;
    Even when using the ‘display_features’ parameter, the Age and Income values
    are displayed in the transformed space.
  &lt;/p&gt;
  &lt;p&gt;
    For this reasons I offer an interactive version, which displays the
    non-transformed values.
  &lt;/p&gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/interactive-dep-plot.png&quot;
      alt=&quot;SHAP dependency plot illustrating interaction effects in machine learning algorithm&quot;
  /&gt;&lt;/span&gt;

  &lt;p&gt;And here is the code to reproduce this plot:&lt;/p&gt;
  &lt;figure&gt;
    &lt;script src=&quot;https://gist.github.com/UrszulaCzerwinska/b78e85c6e1795d949321209cbe41587a.js&quot;&gt;&lt;/script&gt;
  &lt;/figure&gt;
  &lt;p&gt;Here we have the strongest interactions:&lt;/p&gt;
  &lt;p&gt;Income — Education&lt;/p&gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/income-education.png&quot;
      alt=&quot;Shapley interaction plot for better visualization of features interaction in machine learning model&quot;
  /&gt;&lt;/span&gt;

  &lt;p&gt;
    In this graph, we notice that with an Education level 1 (undergrad), low
    income (under 100 k USD) is an encouraging factor to take a credit, and high
    income (over 120 k USD) is an inhibiting interaction.&lt;br /&gt;For individuals
    with Education 2 &amp;amp; 3 (graduated &amp;amp; advanced/professional), the
    interaction effect is slightly lower and opposite to that for Education ==
    1.
  &lt;/p&gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/income-family.png&quot;
      alt=&quot;Shapley interaction plot for better visualization of features interaction in machine learning model&quot;
  /&gt;&lt;/span&gt;

  &lt;p&gt;
    For the features “Family” and “number of people” in the household, the
    interaction is positive when income is low (below USD 100k) and the family
    has 1–2 members. For higher incomes (&amp;gt; 120 k USD), for family with 1–2
    members has a negative effect. The opposite is true for families of 3–4
    people.
  &lt;/p&gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/income-ccavg.png&quot;
      alt=&quot;Shapley interaction plot for better visualization of features interaction in machine learning model&quot;
  /&gt;&lt;/span&gt;

  &lt;p&gt;
    The interaction between income and credit card average spending is more
    complex. For low income (&amp;lt;100 k USD) and low CCAvg (&amp;lt;4 k USD) the
    interaction has a negative effect, for income between 50 and 110 k USD and
    CCAvg 2–6 k USD the effect is strongly positive, this could define a
    potential target for credit canvassing along these two axes. For high
    incomes (&amp;gt; 120 k USD), the low CCAvg has a positive impact on the
    prediction of class 1, high CCAvg has a small negative effect on the
    predictions, the medium CCAvg has a stronger negative impact.
  &lt;/p&gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/family-education.png&quot;
      alt=&quot;Shapley interaction plot for better visualization of features interaction in machine learning model&quot;
  /&gt;&lt;/span&gt;
  &lt;p&gt;
    The interaction between two features is a little less readible. For a family
    of 1 and 2 members with “undergrad” education, the interaction has a
    negative impact. For a family of 3–4 members the effect is the opposite.
  &lt;/p&gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/income-age.png&quot;
      alt=&quot;Shapley interaction plot for better visualization of features interaction in machine learning model&quot;
  /&gt;&lt;/span&gt;

  &lt;p&gt;
    For low incomes (&amp;lt; 70k USD), impact changes linearly with age, the higher
    the age, the more the impact varies positively. For high incomes (&amp;gt;120k
    USD), the interaction impact is lower, at middle age (~40 years) the impact
    is slightly positive, at low age the impact is negative and for age &amp;gt;45
    the impact is neutral.
  &lt;/p&gt;
  &lt;p&gt;
    These findings would be more complicated to interpret if the values of the
    features had not corresponded to original values. For instance, speaking of
    age or income in negative in units. Therefore, representing explanations in
    an understandable dimension facilitates interpretation.
  &lt;/p&gt;
  &lt;h4&gt;Comparing Models: How SHAP Improves Machine Learning Interpretability&lt;/h4&gt;
  &lt;p&gt;
    In some situations, we may want to compare the predictions of different
    models for the same samples. Understand why one model classifies the sample
    correctly and the other one does not.&lt;br /&gt;To start, we can display the
    summary plots for each model, look at the importance of features and the
    shapley value distributions. This gives a first general idea.
  &lt;/p&gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/tree-explainer.png&quot;
      alt=&quot;SHAP tree explainer illustrating how to compare the predictions of different models for the same samples&quot;
  /&gt;&lt;/span&gt;

  &lt;p&gt;
    Decision plot allows to compare on the same graph the predictions of
    different models for the same sample.&lt;br /&gt;You just have to create an object
    that simulates multiclass classification.
  &lt;/p&gt;
  &lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt; &lt;span class=&quot;c1&quot;&gt;# we have tree models : xgb, gbt, rf # for each model
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;we&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;an&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;explainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_explainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;probabilities &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_probs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shapley&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#xgb xgb_explainer =
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;shap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;TreeExplainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xgb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model_output&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;probability&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;feature_perturbation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;interventional&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xgb_values&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;xgb_explainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;shap_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xgb_probs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;xgb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;predict_proba&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pipeline_trans&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xgb_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;xgb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pipeline_trans&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# rf rf_explainer =
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;shap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;TreeExplainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model_output&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;probability&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;feature_perturbation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;interventional&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rf_values&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;rf_explainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;shap_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rf_probs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;rf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;predict_proba&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pipeline_trans&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rf_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;rf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pipeline_trans&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# gbt gbt_explainer =
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;shap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;TreeExplainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gbt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model_output&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;probability&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;feature_perturbation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;interventional&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gbt_values&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;gbt_explainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;shap_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gbt_probs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;gbt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;predict_proba&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pipeline_trans&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gbt_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;gbt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pipeline_trans&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;######## # we make a list
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;explaners&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;base_values&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xgb_explainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;expected_value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;rf_explainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;expected_value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gbt_explainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;expected_value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shap_values&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xgb_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rf_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gbt_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xgb_probs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rf_probs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gbt_probs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;xgb&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;rf&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;gbt&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# index of a
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Plot idx = 100 shap.multioutput_decision_plot(base_values,
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;shap_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;feature_names&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;to_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;legend_labels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;legend_location&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;lower right&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/model-comparison.png&quot;
      alt=&quot;Comparative analysis of machine learning models using SHAP values for data science insights.&quot;
  /&gt;&lt;/span&gt;

  &lt;p&gt;
    The only difficulty consists in checking the dimensions of shapley values
    because for some models, shapley values are calculated for each class, in
    the case of the binary classification (class 0 and 1), while for others we
    obtain a single matrix which corresponds to class 1. In our example, we
    select a second matrix (index 1) for random forest.
  &lt;/p&gt;
  &lt;h4&gt;
    Running Simulations with SHAP: Enhancing Model Predictions in Data Science
  &lt;/h4&gt;
  &lt;p&gt;
    By default SHAP does not contain functions that make it easier to answer the
    “What if?” question. “What if I could earn an extra 10K USD a year, would my
    credit be extended?”&lt;br /&gt;Nevertheless, it is possible to run the
    simulations by varying a feature and calculating hypothetical shapley
    values.
  &lt;/p&gt;
  &lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt; &lt;span class=&quot;n&quot;&gt;explainer_margin_i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;shap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;TreeExplainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xgb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model_output&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;raw&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;feature_perturbation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;interventional&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rg&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;202&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;R&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;hypothetical_shap_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hypothetical_predictions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_r&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
  &lt;span class=&quot;nf&quot;&gt;simulate_with_shap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Income&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;explainer_margin_i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;pipeline_trans&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pipeline_trans&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
  &lt;p&gt;
    I created a `simulate_with_shap` function that simulates different values of
    the feature and calculates the hypothetical shapley values.
  &lt;/p&gt;
  &lt;figure&gt;
    &lt;script src=&quot;https://gist.github.com/UrszulaCzerwinska/c324a8ec8e37ebe4758f4affe52456be.js&quot;&gt;&lt;/script&gt;
  &lt;/figure&gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/simulation.png&quot;
      alt=&quot;The SHAP simulation allows to see how we could change the prediction using new values and what the shapley values would be for these&quot;
  /&gt;&lt;/span&gt;

  &lt;p&gt;
    This simulation allows us to see for the selected sample, if we freeze all
    the features apart from Income, how we could change the prediction and what
    the shapley values would be for these new values.
  &lt;/p&gt;
  &lt;p&gt;
    It is possible to simulate the changes ‘feature by feature’, it would be
    interesting to be able to make several changes simultaneously.
  &lt;/p&gt;
  &lt;h3&gt;
    Future Trends in Data Science: The Growing Role of SHAP and Interpretability
  &lt;/h3&gt;
  &lt;p&gt;
    AI algorithms are taking up more and more space in our lives. The
    explanability of predictions is an important topic for data scientists,
    decision-makers and individuals who are impacted by predictions.
  &lt;/p&gt;
  &lt;p&gt;
    Several frameworks have been proposed in order to transform non-explainable
    models into explainable ones. One of the best known and most widely used
    frameworks is SHAP.
  &lt;/p&gt;
  &lt;p&gt;
    Despite very good documentation, it is not clear how to exploit all its
    features in depth.
  &lt;/p&gt;
  &lt;p&gt;
    I have proposed some simple graphical enhancements and tried to demonstrate
    the usefulness of less known and not understood features in most standard
    uses of SHAP.
  &lt;/p&gt;
  &lt;h3&gt;Acknowledgements&lt;/h3&gt;
  &lt;p&gt;
    I would like to thank the Saegus DATA team who participated in this work
    with good advice, in particular Manager Fréderic Brajon and Senior
    Consultant Manager Clément Moutard.
  &lt;/p&gt;
  &lt;h3&gt;Bibliography&lt;/h3&gt;
  &lt;p&gt;
    [1] Stop Explaining Black Box Machine Learning Models for High Stakes
    Decisions and Use Interpretable Models Instead; Cynthia Rudin
    &lt;a
      href=&quot;https://arxiv.org/pdf/1811.10154.pdf&quot;
      data-href=&quot;https://arxiv.org/pdf/1811.10154.pdf&quot;
      target=&quot;_blank&quot;
      &gt;https://arxiv.org/pdf/1811.10154.pdf&lt;/a
    &gt;
  &lt;/p&gt;
  &lt;p&gt;
    [2] Explainable Artificial Intelligence (XAI): Concepts, Taxonomies,
    Opportunities and Challenges toward Responsible AI; Arrietaa et al.
    &lt;a
      href=&quot;https://arxiv.org/pdf/1910.10045.pdf&quot;
      data-href=&quot;https://arxiv.org/pdf/1910.10045.pdf&quot;
      target=&quot;_blank&quot;
      &gt;https://arxiv.org/pdf/1910.10045.pdf&lt;/a
    &gt;
  &lt;/p&gt;
  &lt;p&gt;
    [3] Cloudera Fast Forward Interpretability:
    &lt;a
      href=&quot;https://ff06-2020.fastforwardlabs.com/?utm_campaign=Data_Elixir&amp;amp;utm_source=Data_Elixir_282&quot;
      data-href=&quot;https://ff06-2020.fastforwardlabs.com/?utm_campaign=Data_Elixir&amp;amp;utm_source=Data_Elixir_282&quot;
      target=&quot;_blank&quot;
      &gt;https://ff06-2020.fastforwardlabs.com/?utm_campaign=Data_Elixir&amp;amp;utm_source=Data_Elixir_282&lt;/a
    &gt;
  &lt;/p&gt;
  &lt;p&gt;
    [4]
    &lt;a
      href=&quot;https://towardsdatascience.com/understand-the-machine-learning-blackbox-with-ml-interpreter-7b0f9a2d8e9f&quot;
      data-href=&quot;https://towardsdatascience.com/understand-the-machine-learning-blackbox-with-ml-interpreter-7b0f9a2d8e9f&quot;
      target=&quot;_blank&quot;
      &gt;https://towardsdatascience.com/understand-the-machine-learning-blackbox-with-ml-interpreter-7b0f9a2d8e9f&lt;/a
    &gt;
  &lt;/p&gt;
  &lt;p&gt;
    [5]
    &lt;a
      href=&quot;https://github.com/slundberg/shap&quot;
      data-href=&quot;https://github.com/slundberg/shap&quot;
      target=&quot;_blank&quot;
      &gt;https://github.com/slundberg/shap&lt;/a
    &gt;
  &lt;/p&gt;
  &lt;p&gt;
    [6]
    &lt;a
      href=&quot;http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions&quot;
      data-href=&quot;http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions&quot;
      target=&quot;_blank&quot;
      &gt;http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions&lt;/a
    &gt;
  &lt;/p&gt;
  &lt;p&gt;
    [7]
    &lt;a
      href=&quot;https://www.nature.com/articles/s42256-019-0138-9&quot;
      data-href=&quot;https://www.nature.com/articles/s42256-019-0138-9&quot;
      target=&quot;_blank&quot;
      &gt;https://www.nature.com/articles/s42256-019-0138-9&lt;/a
    &gt;
  &lt;/p&gt;
  &lt;p&gt;
    [8]
    &lt;a
      href=&quot;https://christophm.github.io/interpretable-ml-book/&quot;
      data-href=&quot;https://christophm.github.io/interpretable-ml-book/&quot;
      target=&quot;_blank&quot;
      &gt;https://christophm.github.io/interpretable-ml-book/&lt;/a
    &gt;
  &lt;/p&gt;
  &lt;p&gt;
    [9]
    &lt;a
      href=&quot;https://towardsdatascience.com/shap-explained-the-way-i-wish-someone-explained-it-to-me-ab81cc69ef30&quot;
      data-href=&quot;https://towardsdatascience.com/shap-explained-the-way-i-wish-someone-explained-it-to-me-ab81cc69ef30&quot;
      target=&quot;_blank&quot;
      &gt;https://towardsdatascience.com/shap-explained-the-way-i-wish-someone-explained-it-to-me-ab81cc69ef30&lt;/a
    &gt;
  &lt;/p&gt;
  &lt;p&gt;
    [10]
    &lt;a
      href=&quot;https://medium.com/@gabrieltseng/interpreting-complex-models-with-shap-values-1c187db6ec83&quot;
      data-href=&quot;https://medium.com/@gabrieltseng/interpreting-complex-models-with-shap-values-1c187db6ec83&quot;
      target=&quot;_blank&quot;
      &gt;https://medium.com/@gabrieltseng/interpreting-complex-models-with-shap-values-1c187db6ec83&lt;/a
    &gt;
  &lt;/p&gt;
  &lt;p&gt;
    [11]
    &lt;a
      href=&quot;https://medium.com/@stanleyg1/a-detailed-walk-through-of-shap-example-for-interpretable-machine-learning-d265c693ac22&quot;
      data-href=&quot;https://medium.com/@stanleyg1/a-detailed-walk-through-of-shap-example-for-interpretable-machine-learning-d265c693ac22&quot;
      target=&quot;_blank&quot;
      &gt;https://medium.com/@stanleyg1/a-detailed-walk-through-of-shap-example-for-interpretable-machine-learning-d265c693ac22&lt;/a
    &gt;
  &lt;/p&gt;
  &lt;p&gt;
    [12]
    &lt;a
      href=&quot;https://francescopochetti.com/whitening-a-black-box-how-to-interpret-a-ml-model/&quot;
      data-href=&quot;https://francescopochetti.com/whitening-a-black-box-how-to-interpret-a-ml-model/&quot;
      target=&quot;_blank&quot;
      &gt;https://francescopochetti.com/whitening-a-black-box-how-to-interpret-a-ml-model/&lt;/a
    &gt;
  &lt;/p&gt;
  &lt;p&gt;
    [13] Explaining Anomalies Detected by Autoencoders Using SHAP; Antwarg et
    al.
    &lt;a
      href=&quot;https://arxiv.org/pdf/1903.02407.pdf&quot;
      data-href=&quot;https://arxiv.org/pdf/1903.02407.pdf&quot;
      target=&quot;_blank&quot;
      &gt;https://arxiv.org/pdf/1903.02407.pdf&lt;/a
    &gt;
  &lt;/p&gt;
  &lt;p&gt;
    [14]
    &lt;a
      href=&quot;https://medium.com/fiddlerlabs/case-study-explaining-credit-modeling-predictions-with-shap-2a7b3f86ec12&quot;
      data-href=&quot;https://medium.com/fiddlerlabs/case-study-explaining-credit-modeling-predictions-with-shap-2a7b3f86ec12&quot;
      target=&quot;_blank&quot;
      &gt;https://medium.com/fiddlerlabs/case-study-explaining-credit-modeling-predictions-with-shap-2a7b3f86ec12&lt;/a
    &gt;
  &lt;/p&gt;
  &lt;p&gt;
    [15]
    &lt;a
      href=&quot;https://www.kaggle.com/itsmesunil/bank-loan-modelling&quot;
      data-href=&quot;https://www.kaggle.com/itsmesunil/bank-loan-modelling&quot;
      target=&quot;_blank&quot;
      &gt;https://www.kaggle.com/itsmesunil/bank-loan-modelling&lt;/a
    &gt;
  &lt;/p&gt;

  &lt;footer&gt;
    This blog post was originally published with
    &lt;a
      href=&quot;https://medium.com/swlh&quot;
      alt=&quot;Get smarter at building your thing. Follow to join The Startup’s +8 million monthly readers &amp; +778K followers.&quot;
      &gt;The Startup&lt;/a
    &gt;
    at
    &lt;a
      href=&quot;https://medium.com/swlh/push-the-limits-of-explainability-an-ultimate-guide-to-shap-library-a110af566a02&quot;
      alt=&quot;Discover how to push the boundaries of explainability in data science and machine learning. This comprehensive guide to SHAP (SHapley Additive exPlanations) covers everything from deep learning algorithms to model interpretability, making complex AI models more transparent and trustworthy. Ideal for data science professionals and enthusiasts looking to enhance their understanding of machine learning interpretability with cutting-edge tools and techniques.&quot;
      &gt;Medium&lt;/a
    &gt;. &lt;br /&gt;&lt;br /&gt;
  &lt;/footer&gt;
  &lt;script
    type=&quot;text/javascript&quot;
    src=&quot;//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-584ec4ce89deed84&quot;
  &gt;&lt;/script&gt;
  &lt;div class=&quot;addthis_inline_share_toolbox&quot;&gt;&lt;/div&gt;
&lt;/section&gt;
</description>
        <pubDate>Sun, 01 Mar 2020 00:00:00 +0100</pubDate>
        <link>http://urszulaczerwinska.github.io/works/interpretability-shap</link>
        <guid isPermaLink="true">http://urszulaczerwinska.github.io/works/interpretability-shap</guid>
        
        <category>data science</category>
        
        <category>interpretability</category>
        
        <category>XAI</category>
        
        <category>machine learning</category>
        
        <category>featured</category>
        
        <category>explainable AI</category>
        
        <category>Python</category>
        
        
        <category>works</category>
        
      </item>
    
  </channel>
</rss>
