<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Urszula Czerwinska | Data Scientist &amp; Deep Learning Engineer</title>
    <description>Explore the journey of Urszula Czerwinska from PhD to Data Science, featuring insights on Data Science projects, Machine Learning, and Deep Learning. Discover how to become a Data Scientist or Machine Learning Engineer.</description>
    <link>http://urszulaczerwinska.github.io/</link>
    <atom:link href="http://urszulaczerwinska.github.io/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Fri, 16 Aug 2024 16:43:13 +0200</pubDate>
    <lastBuildDate>Fri, 16 Aug 2024 16:43:13 +0200</lastBuildDate>
    <generator>Jekyll v4.3.3</generator>
    
      <item>
        <title>Named Entity Recognition Tool by Cour de Cassation</title>
        <description>&lt;!DOCTYPE HTML&gt;
&lt;html lang=&quot;en-US&quot;&gt;
    &lt;head&gt;
        &lt;meta charset=&quot;UTF-8&quot;&gt;
        &lt;meta http-equiv=&quot;refresh&quot; content=&quot;0; url=https://github.com/Cour-de-cassation/moteurNER&quot;&gt;
        &lt;meta name=&quot;description&quot; content=&quot;Redirecting to the Named Entity Recognition (NER) tool repository by Cour de Cassation. It is a page about building deep learning NLP application for French justice&quot;&gt;
        &lt;script type=&quot;text/javascript&quot;&gt;
            window.location.href = &quot;https://github.com/Cour-de-cassation/moteurNER&quot;
        &lt;/script&gt;
        &lt;title&gt;Redirecting to Named Entity Recognition Tool by Cour de Cassation, building NER, NLP deep learning applications for French Supreme Court.&lt;/title&gt;
    &lt;/head&gt;
    &lt;body&gt;
        &lt;p&gt;You are being redirected to the Named Entity Recognition tool repository by Cour de Cassation. If you are not redirected automatically, &lt;a rel=&quot;canonical&quot; href=&apos;https://github.com/Cour-de-cassation/moteurNER&apos;&gt;click here to proceed to the repository.&lt;/a&gt;&lt;/p&gt;
    &lt;/body&gt;

      &lt;!-- Go to www.addthis.com/dashboard to customize your tools --&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-584ec4ce89deed84&quot;&gt;&lt;/script&gt;
&lt;!-- Go to www.addthis.com/dashboard to customize your tools --&gt; &lt;div class=&quot;addthis_inline_share_toolbox&quot;&gt;&lt;/div&gt;

&lt;/html&gt;</description>
        <pubDate>Wed, 01 Sep 2021 00:00:00 +0200</pubDate>
        <link>http://urszulaczerwinska.github.io/works/ner_cc</link>
        <guid isPermaLink="true">http://urszulaczerwinska.github.io/works/ner_cc</guid>
        
        <category>Machine Learning</category>
        
        <category>Python</category>
        
        <category>Deep Learning</category>
        
        <category>NER</category>
        
        <category>NLP</category>
        
        <category>Justice</category>
        
        <category>Flair</category>
        
        <category>featured</category>
        
        
        <category>works</category>
        
      </item>
    
      <item>
        <title>Push the limits of machine learning explainability</title>
        <description>&lt;head&gt;
  &lt;meta
    name=&quot;description&quot;
    content=&quot;Explore the power of SHAP in enhancing model interpretability in data science and machine learning. This guide provides detailed insights and practical examples for data professionals.&quot;
  /&gt;
&lt;/head&gt;

&lt;section&gt;
  &lt;h1&gt;
    Summary - A Comprehensive Guide to SHAP: Enhancing Machine Learning
    Interpretability
  &lt;/h1&gt;
  &lt;p&gt;
    This article is a guide to the advanced and lesser-known features of the
    python SHAP library. It is based on an example of tabular data
    classification.
  &lt;/p&gt;
  &lt;p&gt;
    But first, let’s talk about the motivation and interest in explainability at
    Saegus that motivated and financed my explorations.
  &lt;/p&gt;

  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img src=&quot;/images/engineering.jpg&quot; alt=&quot;&quot; width=&quot;70%&quot;
  /&gt;&lt;/span&gt;

  &lt;h3&gt;The Theory Behind Explainability in AI and Machine Learning&lt;/h3&gt;
  &lt;p&gt;
    The explainability of algorithms is taking more and more place in the
    discussions about Data Science. We know that algorithms are powerful, we
    know that they can assist us in many tasks: price prediction, document
    classification, video recommendation.
  &lt;/p&gt;
  &lt;p&gt;
    From now on, more and more questions are being asked about this
    prediction:&lt;br /&gt;- Is it ethical?&lt;br /&gt;- Is it affected by bias?&lt;br /&gt;- Is
    it used for the right reasons?
  &lt;/p&gt;
  &lt;p&gt;
    In many domains such as medicine, banking or insurance, algorithms can be
    used if, and only if, it is possible to trace and explain (or better,
    interpret) the decisions of these algorithms.
  &lt;/p&gt;
  &lt;h4&gt;Key Terminology in Machine Learning Interpretability and SHAP&lt;/h4&gt;
  &lt;p&gt;In this article we would like to distinguish the terms:&lt;/p&gt;
  &lt;p&gt;
    &lt;strong&gt;Explainability&lt;/strong&gt;: possibility to explain from a technical
    point of view the prediction of an algorithm.
  &lt;/p&gt;
  &lt;p&gt;
    &lt;strong&gt;Interpretability&lt;/strong&gt;: the ability to explain or provide meaning
    in terms that are understandable by a human being.
  &lt;/p&gt;
  &lt;p&gt;
    &lt;strong&gt;Transparency&lt;/strong&gt;: a model is considered transparent if it is
    understandable on its own.
  &lt;/p&gt;
  &lt;h4&gt;Why Explainability Matters in Data Science ?&lt;/h4&gt;
  &lt;p&gt;
    Interpretability helps to ensure impartiality in decision-making, i.e. to
    detect and therefore correct biases in the training data set. In addition,
    it facilitates robustness by highlighting potential adverse disturbances
    that could change the prediction. It can also act as an assurance that only
    significant features infer the outcome.
  &lt;/p&gt;
  &lt;p&gt;
    Sometimes, it would be more advisable to abandon the machine learning
    approach, and use deterministic algorithms based on rules justified by
    industry knowledge or legislation [1].
  &lt;/p&gt;
  &lt;p&gt;
    Nevertheless, it is too tempting to access the capabilities of machine
    learning algorithms that can offer high accuracy. We can talk about the
    trade-off between accuracy and explainability. This trade-off consists in
    discarding more complex models such as neural networks for simpler
    algorithms that can be explained.
  &lt;/p&gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img src=&quot;/images/model-interpret.png&quot; alt=&quot;&quot; /&gt;&lt;em
      &gt;As described in [2] relation between interpretability and accuracy of the
      model. For some models improvements can be made towards a more
      interpretable or more relevant model.&lt;/em
    &gt;&lt;/span
  &gt;
  &lt;p&gt;
    To achieve these goals, a new field has emerged: XAI (Explainable Artificial
    Intelligence), which aims to produce algorithms that are both powerful and
    explainable.
  &lt;/p&gt;
  &lt;p&gt;
    Many frameworks have been proposed to help explain non-transparent
    algorithms. A very good presentation of these methods can be found in the
    Cloudera white paper [3].
  &lt;/p&gt;
  &lt;p&gt;
    In this article we will deal with one of the most used frameworks: SHAP.
  &lt;/p&gt;
  &lt;h4&gt;Exploring the Audience for Explainable AI in Data Science&lt;/h4&gt;
  &lt;p&gt;
    Different profiles interested in expainability or interpretability have been
    identified:
  &lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;
      Business expert/model user — in order to trust the model, understand the
      causality of the prediction
    &lt;/li&gt;
    &lt;li&gt;
      Regulatory bodies to certify compliance with the legislation, auditing
    &lt;/li&gt;
    &lt;li&gt;
      Managers and executive board to assess regulatory compliance, understand
      enterprise AI applications
    &lt;/li&gt;
    &lt;li&gt;
      Users impacted by model decisions in order to understand the situation,
      verify decisions
    &lt;/li&gt;
    &lt;li&gt;
      Data scientist, developer, PO to ensure/improve product performance, find
      new features, explain functioning/predictions to superiors
    &lt;/li&gt;
  &lt;/ul&gt;
  &lt;p&gt;
    In order to make explainability accessible to people with low technical
    skills, first of all, the creator: a data scientist/developer must be
    comfortable with the tools of explainability.
  &lt;/p&gt;
  &lt;p&gt;
    The data scientist will use them above all to understand and improve his
    model and then to communicate with his superiors and regulatory bodies.&lt;br /&gt;Recently,
    explainability tools have become more and more accessible.
  &lt;/p&gt;
  &lt;p&gt;
    For example,
    &lt;a
      href=&quot;https://www.dataiku.com/&quot;
      data-href=&quot;https://www.dataiku.com/&quot;
      class=&quot;markup--anchor markup--p-anchor&quot;
      rel=&quot;noopener&quot;
      target=&quot;_blank&quot;
      &gt;Dataiku &lt;/a
    &gt;— ML’s platform — has added in its latest version 7.0 published on March 2,
    2020 explainability tools: Shapley values and “The Individual Conditional
    Expectation” (ICE).
  &lt;/p&gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img src=&quot;/images/dataiku.png&quot; alt=&quot;&quot; /&gt;&lt;em
      &gt;Dataiku prediction studio&lt;/em
    &gt;&lt;/span
  &gt;
  &lt;p&gt;
    &lt;a href=&quot;https://azure.microsoft.com/en-us/services/machine-learning/&quot;
      &gt;Azure ML&lt;/a
    &gt;
    proposes its own version of Shap and alternative tools adding interactive
    &lt;a
      href=&quot;https://docs.microsoft.com/en-us/azure/machine-learning/how-to-machine-learning-interpretability-aml&quot;
      &gt;dashboards&lt;/a
    &gt;.
  &lt;/p&gt;

  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img src=&quot;/images/azure.png&quot; alt=&quot;&quot; /&gt;&lt;em
      &gt;Azure ML interpretability dashboard&lt;/em
    &gt;&lt;/span
  &gt;

  &lt;p&gt;
    There are also open-source webapps such as this one described in the medium
    article [4] that facilitate the exploration of the SHAP library.
  &lt;/p&gt;
  &lt;div&gt;
    &lt;a
      href=&quot;https://towardsdatascience.com/understand-the-machine-learning-blackbox-with-ml-interpreter-7b0f9a2d8e9f&quot;
      &gt;&lt;strong
        &gt;Understand the machine learning Blackbox with ML interpreter&lt;/strong
      &gt;&lt;br /&gt;&lt;em class=&quot;markup--em markup--mixtapeEmbed-em&quot;
        &gt;There are dangers in having models running the world and making
        decisions from hiring to criminal justice&lt;/em
      &gt;towardsdatascience.com&lt;/a
    &gt;&lt;a
      href=&quot;https://towardsdatascience.com/understand-the-machine-learning-blackbox-with-ml-interpreter-7b0f9a2d8e9f&quot;
    &gt;&lt;/a&gt;
  &lt;/div&gt;
  &lt;div&gt;&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;
  &lt;p&gt;
    These tools, very interesting to get a quick overview of interpretation, do
    not necessarily give an understanding of the full potential of the SHAP
    library. Few allow to explore interaction values or to use different
    background or display sets.
  &lt;/p&gt;
  &lt;p&gt;
    I investigated the SHAP framework and I present you my remarks and the usage
    of less known features, available in the official version of the library in
    open source. I also propose some interactive visualizations easy to
    integrate in your projects.
  &lt;/p&gt;
  &lt;h3&gt;Step-by-Step Guide: Using SHAP for Machine Learning Models&lt;/h3&gt;
  &lt;p&gt;
    Most data scientists have already heard of the SHAP framework.&lt;br /&gt;In this
    post, we won’t explain in detail how the calculations behind the library are
    done. Many resources are available online such as the SHAP documentation
    [5], publications by authors of the library [6,7], the great book
    “Interpretable Machine Learning” [8] and multiple medium articles [9,10,11].
  &lt;/p&gt;
  &lt;p&gt;
    In summary, Shapley’s values calculate the importance of a feature by
    comparing what a model predicts with and without this feature. However,
    since the order in which a model sees the features can affect its
    predictions, this is done in all possible ways, so that the features are
    compared fairly. This approach is inspired by game theory.
  &lt;/p&gt;
  &lt;p&gt;
    Having worked with many clients, for example in the banking and insurance
    sectors, one can see that their data scientists are struggling to exploit
    the full potential of SHAP. They don’t know how this tool could really be
    useful for understanding a model and how to use it to go beyond simply
    extracting the importance of features.
  &lt;/p&gt;
  &lt;blockquote&gt;The devil is in the detail&lt;/blockquote&gt;
  &lt;p&gt;
    SHAP comes with a set of visualizations that are quite complex and not
    always intuitive, even for a data scientist.
  &lt;/p&gt;
  &lt;p&gt;
    On top of that, there are several technical nuances to be able to use SHAP
    with your data. Francesco Porchetti’s blog article [12] expresses some of
    these frustrations by exploring the SHAP,
    &lt;a href=&quot;https://github.com/marcotcr/lime&quot;&gt;LIME&lt;/a&gt;,
    &lt;a
      href=&quot;https://github.com/SauceCat/PDPbox&quot;
      data-href=&quot;https://github.com/SauceCat/PDPbox&quot;
      &gt;PDPbox &lt;/a
    &gt;(PDP and ICE) and
    &lt;a
      href=&quot;https://eli5.readthedocs.io/en/latest/autodocs/sklearn.html#module-eli5.sklearn.permutation_importance&quot;
      &gt;ELI5 &lt;/a
    &gt;libraries.
  &lt;/p&gt;
  &lt;p&gt;
    &lt;strong
      &gt;&lt;em
        &gt;At Saegus, I worked on a course which aims to give more clarity to the
        SHAP framework and to facilitate the use of this tool.&lt;/em
      &gt;&lt;/strong
    &gt;
  &lt;/p&gt;
  &lt;p&gt;
    In this post I would like to share with you some observations collected
    during that process.
  &lt;/p&gt;
  &lt;p&gt;
    SHAP is used to explain an existing model. Taking a binary classification
    case built with a sklearn model. We train, tune and test our model. Then we
    can use our data and the model to create an additional SHAP model that
    explains our classification model.
  &lt;/p&gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/shap.png&quot;
      alt=&quot;SHAP plot demonstrating deep learning AI model interpretability in data science.&quot;
    /&gt;&lt;em&gt;Image source: SHAP github&lt;/em&gt;&lt;/span
  &gt;
  &lt;h4&gt;Vocabulary&lt;/h4&gt;
  &lt;p&gt;
    It is important to understand all the bricks that make up a SHAP
    explanation.
  &lt;/p&gt;
  &lt;p&gt;
    Often, by using default values for parameters, the complexity of the choices
    we make remains obscure.
  &lt;/p&gt;
  &lt;p&gt;
    &lt;strong&gt;global explanations&lt;br /&gt;&lt;/strong&gt;explanations of how the model
    works from a general point of view
  &lt;/p&gt;
  &lt;p&gt;
    &lt;strong&gt;local explanations&lt;/strong&gt;&lt;br /&gt;explanations of the model for a
    sample (a data point)
  &lt;/p&gt;
  &lt;p&gt;
    &lt;strong&gt;explainer &lt;/strong&gt;(shap.explainer_type(params))&lt;br /&gt;type of
    explainability algorithm to be chosen according to the model used.
  &lt;/p&gt;
  &lt;p&gt;
    The parameters are different for each type of model. Usually, the model and
    training data must be provided, at a minimum.
  &lt;/p&gt;
  &lt;p&gt;
    &lt;strong&gt;base value&lt;/strong&gt; (explainer.expected_value)&lt;br /&gt;&lt;em
      &gt;E(y_hat)&lt;/em
    &gt;
    is “the value that would be predicted if we didn’t know any features of the
    current output” is the &lt;em&gt;mean(y_hat)&lt;/em&gt; prediction for the training data
    set or the background set. We can call it “reference value”, it’s a scalar
    (&lt;em&gt;n&lt;/em&gt;).
  &lt;/p&gt;
  &lt;p&gt;
    It’s important to choose your background set carefully — if we have the
    unbalanced training set this will result in a base value placed among the
    majority of samples. This can also be a desired effect: for example if for a
    bank loan we want to answer the question: “how is the customer in question
    different from customers who have been approved for the loan” or “how is my
    false positive different from the true positives”.
  &lt;/p&gt;
  &lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt; &lt;span class=&quot;c1&quot;&gt;# equilibrated case background = X.sample(1000) #X is
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;equilibrated&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# background used in explainer defines base value explainer =
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;shap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;TreeExplainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xgb_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model_output&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;raw&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shap_values&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;explainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;shap_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# background used in the plot, the points that are
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;visible&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;on&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;summary_plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shap_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;feature_names&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt; &lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt; &lt;span class=&quot;c1&quot;&gt;#
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;base&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shifted&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;class1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;class1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,:]&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#X is equilibrated # background
&lt;/span&gt;  &lt;span class=&quot;k&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;1 &lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;used&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;explainer&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;defines&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;base&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;explainer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;shap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;TreeExplainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xgb_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;class1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model_output&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;raw&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shap_values&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;explainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;shap_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# points from class 0 is used in the plot, the points
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;that&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;are&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;visible&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;on&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;summary_plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shap_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;class0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,:],&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;feature_names&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img src=&quot;/images/background-shap.png&quot; alt=&quot;&quot; /&gt;&lt;em
      &gt;Selecting the background dataset changes the question answered by
      shap values.&lt;/em
    &gt;&lt;/span
  &gt;
  &lt;p&gt;
    &lt;strong&gt;SHAPley values&lt;/strong&gt; (explainer.shap_values(x))&lt;br /&gt;the average
    contribution of each feature to each prediction for each sample based on all
    possible features. It is a (&lt;em&gt;n,m&lt;/em&gt;) &lt;em&gt;n &lt;/em&gt;— samples,
    &lt;em&gt;m &lt;/em&gt;— features matrix that represents the contribution of each
    feature to each sample.
  &lt;/p&gt;
  &lt;p&gt;
    &lt;strong&gt;output value&lt;/strong&gt; (for a sample)&lt;br /&gt;the value predicted by the
    algorithm (the probability, logit or raw output values of the model)
  &lt;/p&gt;
  &lt;p&gt;
    &lt;strong&gt;display features&lt;/strong&gt; (&lt;em&gt;n &lt;/em&gt;x &lt;em&gt;m&lt;/em&gt;)&lt;br /&gt;a matrix of
    original values — before transformation/encoding/engineering of features
    etc. — that can be provided to some graphs to improve interpretation. Often
    overlooked and essential for interpretation.
  &lt;/p&gt;
  &lt;p&gt;____&lt;/p&gt;
  &lt;p&gt;&lt;strong&gt;SHAPley values&lt;/strong&gt;&lt;/p&gt;
  &lt;p&gt;
    Shapley values remain the central element. Once we realize that this is
    simply a matrix with the same dimensions as our input data and that we can
    analyze it in different ways to explain the model and not only. We can
    reduce its dimensions, we can cluster it, we can use it to create new
    features. An interesting exploration described in the article [12] aims at
    improving anomaly detection using auto encoders and SHAP. The SHAP library
    proposes a rich but not exchaustive exploration through visualizations.
  &lt;/p&gt;
  &lt;h4&gt;Visualizing SHAP: Enhancing Interpretability in Deep Learning Models&lt;/h4&gt;
  &lt;p&gt;
    The SHAP library offers different visualizations. A good explanation on how
    to read the colors of the summary plot can be found in this medium article
    [14].
  &lt;/p&gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img src=&quot;/images/shap-global.png&quot; alt=&quot;&quot; /&gt;&lt;em
      &gt;A summary of graphical visualizations to analyze global explanations&lt;/em
    &gt;&lt;/span
  &gt;
  &lt;p&gt;
    &lt;strong&gt;The summary plot&lt;/strong&gt; shows the most important features and the
    magnitude of their impact on the model. It can take several graphical forms
    and for the models explained by TreeExplainer we can also observe the&lt;strong
      &gt;&lt;em&gt; interaction values &lt;/em&gt;&lt;/strong
    &gt;using the “compact dot” with shap_interaction_values in input.
  &lt;/p&gt;
  &lt;p&gt;
    &lt;strong&gt;The dependency plot&lt;/strong&gt; allows to analyze the features two by
    two by suggesting a possibility to observe the interactions. The scatter
    plot represents a dependency between a feature(x) and the shapley values (y)
    colored by a second feature(hue).
  &lt;/p&gt;
  &lt;p&gt;
    On a personal note, I find that an observation of a three-factor
    relationship at the same time is not intuitive for the human brain (at least
    mine). I also doubt that an observation of dependency by observing colours
    can be scientifically accurate. Shap can give us an interaction relationship
    that is calculated as a correlation between the shapley values of the first
    feature and the values of the second feature. If possible (for
    TreeExplainer) it makes more sense to use the shapley interaction values to
    observe interactions.
  &lt;/p&gt;
  &lt;figure&gt;
    &lt;script src=&quot;https://gist.github.com/UrszulaCzerwinska/b7853f5f0b209f2f89e2a3590dd6f329.js&quot;&gt;&lt;/script&gt;
    &lt;figcaption class=&quot;imageCaption&quot;&gt;
      Snippet code to reproduce my dependence plot variant.
    &lt;/figcaption&gt;
  &lt;/figure&gt;
  &lt;p&gt;
    I propose an interactive variant of dependency plot that allows to observe
    the relationship between a feature(x), the shapley values (y) and the
    prediction (histogram colors). What seems important to me in this version is
    the possibility to display on the graph the original values (Income in k
    USD) instead of the normalized space used by the model.
  &lt;/p&gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/my-dep-plot.png&quot;
      alt=&quot;Histogram visualizing SHAP interaction values in a data science model analysis.&quot;
    /&gt;&lt;em&gt;My variant of dependence plot&lt;/em&gt;&lt;/span
  &gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/shap-local.png&quot;
      alt=&quot;Histogram visualizing SHAP interaction values in a data science model analysis.&quot;
    /&gt;&lt;em
      &gt;a summary of graphical visualizations to analyze local explanations&lt;/em
    &gt;&lt;/span
  &gt;
  &lt;p&gt;
    There are three alternatives for the visualization of explanations of a
    sample: force plot, decision plot and waterfall plot.
  &lt;/p&gt;
  &lt;p&gt;
    For a sample, these three representations are redundant, they represent the
    information in a very similar way. At the same time, some elements of these
    graphs are complementary. By putting the three side by side, I have the
    impression to understand the result in a more intuitive way. The force plot
    is good to see where the “output value” fits in relation to the “base
    value”. We also see which features have a positive (red) or negative (blue)
    impact on the prediction and the magnitude of this impact. The water plot
    also allows us to see the amplitude and the nature of the impact of a
    feature with its quantification. It also allows to see the order of
    importance of the features and the values taken by each feature for the
    studied sample. The Decision plot makes it possible to observe the amplitude
    of each change, “a trajectory” taken by a samplefor the values of the
    displayed features.
  &lt;/p&gt;
  &lt;p&gt;
    By using force plot and decision plot we can represent several samples at
    the same time.
  &lt;/p&gt;
  &lt;p&gt;
    The force plot for a set of samples can be compared to the last level of a
    dendrogram. The samples are grouped by similarity or by selected feature. In
    my opinion, this graph is difficult to read for a random sample. It is much
    more meaningful if we represent the contrasting cases or with a hypothesis
    behind.
  &lt;/p&gt;
  &lt;p&gt;
    The decision plot, for a set of samples, quickly becomes cumbersome if we
    select too many samples. It is very useful to observe a ‘trajectory
    deviation’ or ‘diverging/converging trajectories’ of a limited group of
    samples.
  &lt;/p&gt;
  &lt;h4&gt;
    Explainers in SHAP: Understanding Different Model Interpretability
    Approaches
  &lt;/h4&gt;
  &lt;p&gt;
    Explainers are the models used to calculate shapley values. The diagram
    below shows different types of Explainers.
  &lt;/p&gt;
  &lt;p&gt;
    The choice of Explainers depends mainly on the selected learning model. For
    linear models, the “Linear Explainer” is used, for decision trees and “set”
    type models — “TreeExplainer”. “Kernel Explainer” is slower than the above
    mentioned explainers.
  &lt;/p&gt;
  &lt;p&gt;
    In addition the “Tree Explainer” allows to display the interaction values
    (see next section). It also allows to transform the model output into
    probabilities or logloss, which is useful for a better understanding of the
    model or to compare several models.
  &lt;/p&gt;
  &lt;p&gt;
    The Kernel Explainer creates a model that substitutes the closest to our
    model. Kernel Explainer can be used to explain neural networks. For deep
    learning models, there are the Deep Explainer and the Grandient Explainer.
    For this paper we have not investigated the explainability of neural
    networks.
  &lt;/p&gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/explainer.png&quot;
      alt=&quot;Force plot visualization of SHAP values enhancing transparency in AI model predictions; Waterfall plot depicting SHAP value contributions to machine learning predictions.&quot;
    /&gt;&lt;em&gt;a summary of Explainer types in the SHAP library&lt;/em&gt;&lt;/span
  &gt;
  &lt;h4&gt;Shapley values of interactions&lt;/h4&gt;
  &lt;p&gt;
    One of the properties that allows to go further in the analysis of a model
    that can be explained with the “Tree Explainer” is the calculation of
    shapley values of interactions.
  &lt;/p&gt;
  &lt;p&gt;
    These values make it possible to quantify the impact of an interaction
    between two features on the prediction for each sample. As the matrix of
    shapley values has two dimensions (samples x features), the interactions are
    a tensor with three dimensions (samples x features x features).
  &lt;/p&gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/interactions-1.png&quot;
      alt=&quot;Interactive SHAP dependence plot showing feature impact in deep learning models.&quot;
    /&gt;&lt;em
      &gt;a summary of graphical visualizations to analyze local explanations&lt;/em
    &gt;&lt;/span
  &gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/interactions-2.png&quot;
      alt=&quot;Interactive SHAP dependence plot showing feature impact in deep learning models.&quot;
    /&gt;&lt;em
      &gt;a summary of graphical visualizations to analyze local explanations&lt;/em
    &gt;&lt;/span
  &gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/interactions-3.png&quot;
      alt=&quot;Interactive SHAP dependence plot showing feature impact in deep learning models.&quot;
    /&gt;&lt;em
      &gt;a summary of graphical visualizations to analyze local explanations&lt;/em
    &gt;&lt;/span
  &gt;

  &lt;p&gt;
    &lt;strong
      &gt;Here’s how interaction values help interpret a binary classification
      model.&lt;/strong
    &gt;
  &lt;/p&gt;
  &lt;p&gt;
    I used a Kaggle [15] dataset that represents a client base and the binary
    dependent feature: did the client accept the personal loan? NO/YES (0/1).
  &lt;/p&gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/table.png&quot;
      alt=&quot;A summary of SHAP graphical
      visualizations to analyze local explanations of machine learning model&quot;
    /&gt;&lt;em
      &gt;a summary of graphical visualizations to analyze local explanations&lt;/em
    &gt;&lt;/span
  &gt;

  &lt;p&gt;
    I’ve trained several models, including an xgboost model that we treated with
    the Tree Explainer.
  &lt;/p&gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/sklearn-model.png&quot;
      alt=&quot;A summary of SHAP graphical
        visualizations to analyze local explanations of machine learning model&quot;
    /&gt;&lt;em
      &gt;a summary of graphical visualizations to analyze local explanations&lt;/em
    &gt;&lt;/span
  &gt;

  &lt;p&gt;The background dataset was balanced and represented 40% of the dataset.&lt;/p&gt;
  &lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt; &lt;span class=&quot;c1&quot;&gt;# xgb - traned model # X_background - background
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;explainer_raw&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;TreeExplainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xgb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;model_output&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;raw&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;feature_perturbation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;tree_path_dependent&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# project
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;point&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;background&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datasetshap_values&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;explainer_raw&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;shap_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# obtain interaction values
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;shap_interaction_values&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;explainer_raw&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;shap_interaction_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;# dimensions shap_values.shape &amp;gt;&amp;gt;&amp;gt;(2543, 16) shap_interaction_values.shape
&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2543&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;summary_plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shap_interaction_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;plot_type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;compact_dot&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/summary-plot.png&quot;
      alt=&quot;Summary plot showcasing SHAP values and feature interactions in machine learning models.&quot;
    /&gt;&lt;em&gt;Summary plot with interactions&lt;/em&gt;&lt;/span
  &gt;

  &lt;p&gt;To better explore interactions, a heatmap can be very useful.&lt;/p&gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/histogram.png&quot;
      alt=&quot;Heatmap illustrating SHAP values and feature interactions in machine learning models.&quot;
    /&gt;&lt;em&gt;Histogram of interaction values&lt;/em&gt;&lt;/span
  &gt;
  &lt;p&gt;
    In the Summary_plot one can observe the importance of features and the
    importance the interactions. The interactions appear in double which
    confuses a little the reading.
  &lt;/p&gt;
  &lt;p&gt;
    In the histogram, we observe directly the interactions. The strongests of
    them of being: Income-Education, Income — Family, Income — CCAvg and
    Family-Education, Income-Age.
  &lt;/p&gt;
  &lt;p&gt;
    Then I investigated the interactions two by two.&lt;br /&gt;To understand the
    difference between a dependency_plot and a dependency_plot of interactions
    here are the two:
  &lt;/p&gt;

  &lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt; &lt;span class=&quot;c1&quot;&gt;# dependence_plot classique shap.dependence_plot(&quot;Age&quot;,
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;shap_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;features&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;display_features&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_background_display&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;interaction_index&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Income&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/dep-plot.png&quot;
      alt=&quot;SHAP dependency plot illustrating interaction effects in machine learning algorithm&quot;
  /&gt;&lt;/span&gt;
  &lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt; &lt;span class=&quot;c1&quot;&gt;# dependence_plot des interactions
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;shap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;dependence_plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Age&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Income&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shap_interaction_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;features&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;X_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;display_features&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_background_display&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/interaction-dep-plot.png&quot;
      alt=&quot;SHAP dependency plot illustrating interaction effects in machine learning algorithm&quot;
  /&gt;&lt;/span&gt;

  &lt;p&gt;
    Even when using the ‘display_features’ parameter, the Age and Income values
    are displayed in the transformed space.
  &lt;/p&gt;
  &lt;p&gt;
    For this reasons I offer an interactive version, which displays the
    non-transformed values.
  &lt;/p&gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/interactive-dep-plot.png&quot;
      alt=&quot;SHAP dependency plot illustrating interaction effects in machine learning algorithm&quot;
  /&gt;&lt;/span&gt;

  &lt;p&gt;And here is the code to reproduce this plot:&lt;/p&gt;
  &lt;figure&gt;
    &lt;script src=&quot;https://gist.github.com/UrszulaCzerwinska/b78e85c6e1795d949321209cbe41587a.js&quot;&gt;&lt;/script&gt;
  &lt;/figure&gt;
  &lt;p&gt;Here we have the strongest interactions:&lt;/p&gt;
  &lt;p&gt;Income — Education&lt;/p&gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/income-education.png&quot;
      alt=&quot;Shapley interaction plot for better visualization of features interaction in machine learning model&quot;
  /&gt;&lt;/span&gt;

  &lt;p&gt;
    In this graph, we notice that with an Education level 1 (undergrad), low
    income (under 100 k USD) is an encouraging factor to take a credit, and high
    income (over 120 k USD) is an inhibiting interaction.&lt;br /&gt;For individuals
    with Education 2 &amp;amp; 3 (graduated &amp;amp; advanced/professional), the
    interaction effect is slightly lower and opposite to that for Education ==
    1.
  &lt;/p&gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/income-family.png&quot;
      alt=&quot;Shapley interaction plot for better visualization of features interaction in machine learning model&quot;
  /&gt;&lt;/span&gt;

  &lt;p&gt;
    For the features “Family” and “number of people” in the household, the
    interaction is positive when income is low (below USD 100k) and the family
    has 1–2 members. For higher incomes (&amp;gt; 120 k USD), for family with 1–2
    members has a negative effect. The opposite is true for families of 3–4
    people.
  &lt;/p&gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/income-ccavg.png&quot;
      alt=&quot;Shapley interaction plot for better visualization of features interaction in machine learning model&quot;
  /&gt;&lt;/span&gt;

  &lt;p&gt;
    The interaction between income and credit card average spending is more
    complex. For low income (&amp;lt;100 k USD) and low CCAvg (&amp;lt;4 k USD) the
    interaction has a negative effect, for income between 50 and 110 k USD and
    CCAvg 2–6 k USD the effect is strongly positive, this could define a
    potential target for credit canvassing along these two axes. For high
    incomes (&amp;gt; 120 k USD), the low CCAvg has a positive impact on the
    prediction of class 1, high CCAvg has a small negative effect on the
    predictions, the medium CCAvg has a stronger negative impact.
  &lt;/p&gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/family-education.png&quot;
      alt=&quot;Shapley interaction plot for better visualization of features interaction in machine learning model&quot;
  /&gt;&lt;/span&gt;
  &lt;p&gt;
    The interaction between two features is a little less readible. For a family
    of 1 and 2 members with “undergrad” education, the interaction has a
    negative impact. For a family of 3–4 members the effect is the opposite.
  &lt;/p&gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/income-age.png&quot;
      alt=&quot;Shapley interaction plot for better visualization of features interaction in machine learning model&quot;
  /&gt;&lt;/span&gt;

  &lt;p&gt;
    For low incomes (&amp;lt; 70k USD), impact changes linearly with age, the higher
    the age, the more the impact varies positively. For high incomes (&amp;gt;120k
    USD), the interaction impact is lower, at middle age (~40 years) the impact
    is slightly positive, at low age the impact is negative and for age &amp;gt;45
    the impact is neutral.
  &lt;/p&gt;
  &lt;p&gt;
    These findings would be more complicated to interpret if the values of the
    features had not corresponded to original values. For instance, speaking of
    age or income in negative in units. Therefore, representing explanations in
    an understandable dimension facilitates interpretation.
  &lt;/p&gt;
  &lt;h4&gt;Comparing Models: How SHAP Improves Machine Learning Interpretability&lt;/h4&gt;
  &lt;p&gt;
    In some situations, we may want to compare the predictions of different
    models for the same samples. Understand why one model classifies the sample
    correctly and the other one does not.&lt;br /&gt;To start, we can display the
    summary plots for each model, look at the importance of features and the
    shapley value distributions. This gives a first general idea.
  &lt;/p&gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/tree-explainer.png&quot;
      alt=&quot;SHAP tree explainer illustrating how to compare the predictions of different models for the same samples&quot;
  /&gt;&lt;/span&gt;

  &lt;p&gt;
    Decision plot allows to compare on the same graph the predictions of
    different models for the same sample.&lt;br /&gt;You just have to create an object
    that simulates multiclass classification.
  &lt;/p&gt;
  &lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt; &lt;span class=&quot;c1&quot;&gt;# we have tree models : xgb, gbt, rf # for each model
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;we&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;an&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;explainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_explainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;probabilities &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_probs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shapley&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#xgb xgb_explainer =
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;shap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;TreeExplainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xgb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model_output&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;probability&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;feature_perturbation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;interventional&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xgb_values&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;xgb_explainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;shap_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xgb_probs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;xgb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;predict_proba&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pipeline_trans&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xgb_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;xgb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pipeline_trans&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# rf rf_explainer =
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;shap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;TreeExplainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model_output&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;probability&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;feature_perturbation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;interventional&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rf_values&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;rf_explainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;shap_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rf_probs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;rf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;predict_proba&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pipeline_trans&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rf_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;rf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pipeline_trans&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# gbt gbt_explainer =
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;shap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;TreeExplainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gbt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model_output&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;probability&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;feature_perturbation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;interventional&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gbt_values&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;gbt_explainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;shap_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gbt_probs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;gbt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;predict_proba&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pipeline_trans&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gbt_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;gbt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pipeline_trans&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;######## # we make a list
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;explaners&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;base_values&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xgb_explainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;expected_value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;rf_explainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;expected_value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gbt_explainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;expected_value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shap_values&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xgb_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rf_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gbt_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xgb_probs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rf_probs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gbt_probs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;xgb&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;rf&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;gbt&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# index of a
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Plot idx = 100 shap.multioutput_decision_plot(base_values,
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;shap_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;feature_names&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;to_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;legend_labels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;legend_location&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;lower right&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/model-comparison.png&quot;
      alt=&quot;Comparative analysis of machine learning models using SHAP values for data science insights.&quot;
  /&gt;&lt;/span&gt;

  &lt;p&gt;
    The only difficulty consists in checking the dimensions of shapley values
    because for some models, shapley values are calculated for each class, in
    the case of the binary classification (class 0 and 1), while for others we
    obtain a single matrix which corresponds to class 1. In our example, we
    select a second matrix (index 1) for random forest.
  &lt;/p&gt;
  &lt;h4&gt;
    Running Simulations with SHAP: Enhancing Model Predictions in Data Science
  &lt;/h4&gt;
  &lt;p&gt;
    By default SHAP does not contain functions that make it easier to answer the
    “What if?” question. “What if I could earn an extra 10K USD a year, would my
    credit be extended?”&lt;br /&gt;Nevertheless, it is possible to run the
    simulations by varying a feature and calculating hypothetical shapley
    values.
  &lt;/p&gt;
  &lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt; &lt;span class=&quot;n&quot;&gt;explainer_margin_i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;shap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;TreeExplainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xgb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model_output&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;raw&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;feature_perturbation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;interventional&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rg&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;202&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;R&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;hypothetical_shap_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hypothetical_predictions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_r&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
  &lt;span class=&quot;nf&quot;&gt;simulate_with_shap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Income&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;explainer_margin_i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;pipeline_trans&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pipeline_trans&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
  &lt;p&gt;
    I created a `simulate_with_shap` function that simulates different values of
    the feature and calculates the hypothetical shapley values.
  &lt;/p&gt;
  &lt;figure&gt;
    &lt;script src=&quot;https://gist.github.com/UrszulaCzerwinska/c324a8ec8e37ebe4758f4affe52456be.js&quot;&gt;&lt;/script&gt;
  &lt;/figure&gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/simulation.png&quot;
      alt=&quot;The SHAP simulation allows to see how we could change the prediction using new values and what the shapley values would be for these&quot;
  /&gt;&lt;/span&gt;

  &lt;p&gt;
    This simulation allows us to see for the selected sample, if we freeze all
    the features apart from Income, how we could change the prediction and what
    the shapley values would be for these new values.
  &lt;/p&gt;
  &lt;p&gt;
    It is possible to simulate the changes ‘feature by feature’, it would be
    interesting to be able to make several changes simultaneously.
  &lt;/p&gt;
  &lt;h3&gt;
    Future Trends in Data Science: The Growing Role of SHAP and Interpretability
  &lt;/h3&gt;
  &lt;p&gt;
    AI algorithms are taking up more and more space in our lives. The
    explanability of predictions is an important topic for data scientists,
    decision-makers and individuals who are impacted by predictions.
  &lt;/p&gt;
  &lt;p&gt;
    Several frameworks have been proposed in order to transform non-explainable
    models into explainable ones. One of the best known and most widely used
    frameworks is SHAP.
  &lt;/p&gt;
  &lt;p&gt;
    Despite very good documentation, it is not clear how to exploit all its
    features in depth.
  &lt;/p&gt;
  &lt;p&gt;
    I have proposed some simple graphical enhancements and tried to demonstrate
    the usefulness of less known and not understood features in most standard
    uses of SHAP.
  &lt;/p&gt;
  &lt;h3&gt;Acknowledgements&lt;/h3&gt;
  &lt;p&gt;
    I would like to thank the Saegus DATA team who participated in this work
    with good advice, in particular Manager Fréderic Brajon and Senior
    Consultant Manager Clément Moutard.
  &lt;/p&gt;
  &lt;h3&gt;Bibliography&lt;/h3&gt;
  &lt;p&gt;
    [1] Stop Explaining Black Box Machine Learning Models for High Stakes
    Decisions and Use Interpretable Models Instead; Cynthia Rudin
    &lt;a
      href=&quot;https://arxiv.org/pdf/1811.10154.pdf&quot;
      data-href=&quot;https://arxiv.org/pdf/1811.10154.pdf&quot;
      target=&quot;_blank&quot;
      &gt;https://arxiv.org/pdf/1811.10154.pdf&lt;/a
    &gt;
  &lt;/p&gt;
  &lt;p&gt;
    [2] Explainable Artificial Intelligence (XAI): Concepts, Taxonomies,
    Opportunities and Challenges toward Responsible AI; Arrietaa et al.
    &lt;a
      href=&quot;https://arxiv.org/pdf/1910.10045.pdf&quot;
      data-href=&quot;https://arxiv.org/pdf/1910.10045.pdf&quot;
      target=&quot;_blank&quot;
      &gt;https://arxiv.org/pdf/1910.10045.pdf&lt;/a
    &gt;
  &lt;/p&gt;
  &lt;p&gt;
    [3] Cloudera Fast Forward Interpretability:
    &lt;a
      href=&quot;https://ff06-2020.fastforwardlabs.com/?utm_campaign=Data_Elixir&amp;amp;utm_source=Data_Elixir_282&quot;
      data-href=&quot;https://ff06-2020.fastforwardlabs.com/?utm_campaign=Data_Elixir&amp;amp;utm_source=Data_Elixir_282&quot;
      target=&quot;_blank&quot;
      &gt;https://ff06-2020.fastforwardlabs.com/?utm_campaign=Data_Elixir&amp;amp;utm_source=Data_Elixir_282&lt;/a
    &gt;
  &lt;/p&gt;
  &lt;p&gt;
    [4]
    &lt;a
      href=&quot;https://towardsdatascience.com/understand-the-machine-learning-blackbox-with-ml-interpreter-7b0f9a2d8e9f&quot;
      data-href=&quot;https://towardsdatascience.com/understand-the-machine-learning-blackbox-with-ml-interpreter-7b0f9a2d8e9f&quot;
      target=&quot;_blank&quot;
      &gt;https://towardsdatascience.com/understand-the-machine-learning-blackbox-with-ml-interpreter-7b0f9a2d8e9f&lt;/a
    &gt;
  &lt;/p&gt;
  &lt;p&gt;
    [5]
    &lt;a
      href=&quot;https://github.com/slundberg/shap&quot;
      data-href=&quot;https://github.com/slundberg/shap&quot;
      target=&quot;_blank&quot;
      &gt;https://github.com/slundberg/shap&lt;/a
    &gt;
  &lt;/p&gt;
  &lt;p&gt;
    [6]
    &lt;a
      href=&quot;http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions&quot;
      data-href=&quot;http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions&quot;
      target=&quot;_blank&quot;
      &gt;http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions&lt;/a
    &gt;
  &lt;/p&gt;
  &lt;p&gt;
    [7]
    &lt;a
      href=&quot;https://www.nature.com/articles/s42256-019-0138-9&quot;
      data-href=&quot;https://www.nature.com/articles/s42256-019-0138-9&quot;
      target=&quot;_blank&quot;
      &gt;https://www.nature.com/articles/s42256-019-0138-9&lt;/a
    &gt;
  &lt;/p&gt;
  &lt;p&gt;
    [8]
    &lt;a
      href=&quot;https://christophm.github.io/interpretable-ml-book/&quot;
      data-href=&quot;https://christophm.github.io/interpretable-ml-book/&quot;
      target=&quot;_blank&quot;
      &gt;https://christophm.github.io/interpretable-ml-book/&lt;/a
    &gt;
  &lt;/p&gt;
  &lt;p&gt;
    [9]
    &lt;a
      href=&quot;https://towardsdatascience.com/shap-explained-the-way-i-wish-someone-explained-it-to-me-ab81cc69ef30&quot;
      data-href=&quot;https://towardsdatascience.com/shap-explained-the-way-i-wish-someone-explained-it-to-me-ab81cc69ef30&quot;
      target=&quot;_blank&quot;
      &gt;https://towardsdatascience.com/shap-explained-the-way-i-wish-someone-explained-it-to-me-ab81cc69ef30&lt;/a
    &gt;
  &lt;/p&gt;
  &lt;p&gt;
    [10]
    &lt;a
      href=&quot;https://medium.com/@gabrieltseng/interpreting-complex-models-with-shap-values-1c187db6ec83&quot;
      data-href=&quot;https://medium.com/@gabrieltseng/interpreting-complex-models-with-shap-values-1c187db6ec83&quot;
      target=&quot;_blank&quot;
      &gt;https://medium.com/@gabrieltseng/interpreting-complex-models-with-shap-values-1c187db6ec83&lt;/a
    &gt;
  &lt;/p&gt;
  &lt;p&gt;
    [11]
    &lt;a
      href=&quot;https://medium.com/@stanleyg1/a-detailed-walk-through-of-shap-example-for-interpretable-machine-learning-d265c693ac22&quot;
      data-href=&quot;https://medium.com/@stanleyg1/a-detailed-walk-through-of-shap-example-for-interpretable-machine-learning-d265c693ac22&quot;
      target=&quot;_blank&quot;
      &gt;https://medium.com/@stanleyg1/a-detailed-walk-through-of-shap-example-for-interpretable-machine-learning-d265c693ac22&lt;/a
    &gt;
  &lt;/p&gt;
  &lt;p&gt;
    [12]
    &lt;a
      href=&quot;https://francescopochetti.com/whitening-a-black-box-how-to-interpret-a-ml-model/&quot;
      data-href=&quot;https://francescopochetti.com/whitening-a-black-box-how-to-interpret-a-ml-model/&quot;
      target=&quot;_blank&quot;
      &gt;https://francescopochetti.com/whitening-a-black-box-how-to-interpret-a-ml-model/&lt;/a
    &gt;
  &lt;/p&gt;
  &lt;p&gt;
    [13] Explaining Anomalies Detected by Autoencoders Using SHAP; Antwarg et
    al.
    &lt;a
      href=&quot;https://arxiv.org/pdf/1903.02407.pdf&quot;
      data-href=&quot;https://arxiv.org/pdf/1903.02407.pdf&quot;
      target=&quot;_blank&quot;
      &gt;https://arxiv.org/pdf/1903.02407.pdf&lt;/a
    &gt;
  &lt;/p&gt;
  &lt;p&gt;
    [14]
    &lt;a
      href=&quot;https://medium.com/fiddlerlabs/case-study-explaining-credit-modeling-predictions-with-shap-2a7b3f86ec12&quot;
      data-href=&quot;https://medium.com/fiddlerlabs/case-study-explaining-credit-modeling-predictions-with-shap-2a7b3f86ec12&quot;
      target=&quot;_blank&quot;
      &gt;https://medium.com/fiddlerlabs/case-study-explaining-credit-modeling-predictions-with-shap-2a7b3f86ec12&lt;/a
    &gt;
  &lt;/p&gt;
  &lt;p&gt;
    [15]
    &lt;a
      href=&quot;https://www.kaggle.com/itsmesunil/bank-loan-modelling&quot;
      data-href=&quot;https://www.kaggle.com/itsmesunil/bank-loan-modelling&quot;
      target=&quot;_blank&quot;
      &gt;https://www.kaggle.com/itsmesunil/bank-loan-modelling&lt;/a
    &gt;
  &lt;/p&gt;

  &lt;footer&gt;
    This blog post was originally published with
    &lt;a
      href=&quot;https://medium.com/swlh&quot;
      alt=&quot;Get smarter at building your thing. Follow to join The Startup’s +8 million monthly readers &amp; +778K followers.&quot;
      &gt;The Startup&lt;/a
    &gt;
    at
    &lt;a
      href=&quot;https://medium.com/swlh/push-the-limits-of-explainability-an-ultimate-guide-to-shap-library-a110af566a02&quot;
      alt=&quot;Discover how to push the boundaries of explainability in data science and machine learning. This comprehensive guide to SHAP (SHapley Additive exPlanations) covers everything from deep learning algorithms to model interpretability, making complex AI models more transparent and trustworthy. Ideal for data science professionals and enthusiasts looking to enhance their understanding of machine learning interpretability with cutting-edge tools and techniques.&quot;
      &gt;Medium&lt;/a
    &gt;. &lt;br /&gt;&lt;br /&gt;
  &lt;/footer&gt;
  &lt;script
    type=&quot;text/javascript&quot;
    src=&quot;//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-584ec4ce89deed84&quot;
  &gt;&lt;/script&gt;
  &lt;div class=&quot;addthis_inline_share_toolbox&quot;&gt;&lt;/div&gt;
&lt;/section&gt;
</description>
        <pubDate>Sun, 01 Mar 2020 00:00:00 +0100</pubDate>
        <link>http://urszulaczerwinska.github.io/works/interpretability-shap</link>
        <guid isPermaLink="true">http://urszulaczerwinska.github.io/works/interpretability-shap</guid>
        
        <category>data science</category>
        
        <category>interpretability</category>
        
        <category>XAI</category>
        
        <category>machine learning</category>
        
        <category>featured</category>
        
        <category>explainable AI</category>
        
        
        <category>works</category>
        
      </item>
    
      <item>
        <title>Mastering Named Entity Recognition (NER) in Data Science</title>
        <description>&lt;h2 id=&quot;context-of-development-of-a-keyword-extraction-application-using-nlp-language-model&quot;&gt;Context of development of a keyword extraction application using NLP language model&lt;/h2&gt;

&lt;p&gt;Named Entity Recognition, often abbreviated as NER, has gained traction as a critical tool for extracting meaningful insights from text data. Whether you’re diving into data science projects or exploring the cutting edge of AI applied to language, understanding how to utilize NER is essential. In this post, I’ll walk you through a practical example of using SpaCy, a go-to library for NLP, to detect keywords from Medium articles. But first, let’s explore why NER is becoming a must-have skill in the data science and engineering toolbox.&lt;/p&gt;

&lt;p&gt;Inspired by a solution developed for a customer in the Pharmaceutical industry, we presented at the &lt;a href=&quot;https://paris.egg.dataiku.com/&quot;&gt;EGG PARIS 2019&lt;/a&gt; conference an application based on NLP (Natural Language Processing) and developed on a &lt;a href=&quot;https://www.dataiku.com/&quot;&gt;Dataiku&lt;/a&gt; &lt;a href=&quot;https://www.dataiku.com/dss/&quot;&gt;DSS&lt;/a&gt; environment.&lt;/p&gt;

&lt;p&gt;More precisely, we trained a deep learning model to recognize the keywords of a blog article, precisely from &lt;a href=&quot;https://medium.com/&quot;&gt;Medium blogging platform&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;By &lt;strong&gt;automatically generate tags and/or keywords&lt;/strong&gt;, this approach enables personalized content recommendations, improving user experience by aligning content with reader expectations. The method holds significant potential, particularly for automated text analysis of complex documents, including scientific papers and legal texts.&lt;/p&gt;

&lt;p&gt;To showcase its functionality, we integrated a voice command feature using &lt;a href=&quot;https://azure.microsoft.com/en-us/services/cognitive-services/&quot;&gt;Azure’s cognitive services API&lt;/a&gt;. The &lt;em&gt;speech to text&lt;/em&gt; module translates spoken queries into text, which is then processed by the algorithm. The output is a recommendation of articles, classified by relevance according to the field of research.&lt;/p&gt;

&lt;p&gt;In this article, I’ll walk you through our approach to creating the underlying NLP model.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/zg0pTe-GyF0&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;sub&gt;&lt;sup&gt;[To view the comments, please enable subtitles] A video that illustrates our web application created for the EGG Dataiku 2019 conference&lt;/sup&gt;&lt;/sub&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;why-extract-keywords-from-medium-blog-articles-with-ai-&quot;&gt;Why Extract Keywords from Medium Blog Articles with AI ?&lt;/h2&gt;

&lt;p&gt;Medium has two categorization systems: &lt;strong&gt;tags&lt;/strong&gt; and &lt;strong&gt;topics&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Topics&lt;/strong&gt; are predefined by the platform and correspond to broad categories like data science or machine learning. Authors have no control over these.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Tags&lt;/strong&gt;, on the other hand, are keywords selected by the author, with a maximum of five tags per article. These tags help increase the visibility of the article but often may not accurately reflect the content. For instance, tags like “TECHNOLOGY,” “MINDFULNESS,” or “LIFE LESSONS” might make an article easier to find but can complicate the reader’s search for specific content.&lt;/p&gt;

&lt;p&gt;Our approach aims to improve this by automatically tagging articles, increasing their relevance. With these “new tags” or “keywords,” searching for articles becomes more efficient.&lt;/p&gt;

&lt;p&gt;Going further, this method could be used to build a recommendation system that suggests related articles based on the one you’re currently reading or aligned with your reading habits.&lt;/p&gt;

&lt;h2 id=&quot;the-ner-named-entity-recognition-approach&quot;&gt;The NER (Named Entity Recognition) approach&lt;/h2&gt;

&lt;p&gt;Using the NER (Named Entity Recognition) approach, we can extract entities across various categories. Several pre-trained models, like &lt;a href=&quot;https://github.com/explosion/spacy-models/releases/tag/en_core_web_md-2.2.0&quot;&gt;en_core_web_md&lt;/a&gt; can recognize entities like people, places, dates, etc.&lt;/p&gt;

&lt;p&gt;For example, in the sentence &lt;em&gt;“I think Barack Obama met founder of Facebook at occasion of a release of a new NLP algorithm.”&lt;/em&gt;, the en_core_web_md model detects “Facebook” and “Barack Obama” as entities.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/UrszulaCzerwinska/11a8fab0cc4c936b67e374e2b55e0fa0.js&quot;&gt;&lt;/script&gt;

&lt;div&gt;&lt;span class=&quot;image fit&quot;&gt;&lt;img src=&quot;/images/NER_img1.png&quot; alt=&quot;NER process using SpaCy in data science&quot; /&gt;&lt;/span&gt;&lt;/div&gt;
&lt;p align=&quot;center&quot;&gt;&lt;sub&gt;&lt;sup&gt;Dependency graph: result of line 9 (# 1)&lt;/sup&gt;&lt;/sub&gt;&lt;/p&gt;

&lt;div&gt;&lt;span class=&quot;image fit&quot;&gt;&lt;img src=&quot;/images/NER_img2.png&quot; alt=&quot;NER process using SpaCy in data science&quot; /&gt;&lt;/span&gt;&lt;/div&gt;
&lt;p align=&quot;center&quot;&gt;&lt;sub&gt;&lt;sup&gt;Entity detection: result of line 10 (# 2)&lt;/sup&gt;&lt;/sub&gt;&lt;/p&gt;

&lt;p&gt;With some annotated data, we trained the algorithm to detect this new entity type.&lt;/p&gt;

&lt;p&gt;The concept is straightforward: an article tagged with “Data Science,” “AI,” “Machine Learning,” or “Python” might still cover vastly different technologies. Our algorithm is designed to detect specific technologies mentioned in the article, such as GANs, reinforcement learning, or Python libraries, while still recognizing places, organizations, and people.&lt;/p&gt;

&lt;p&gt;During training, the model learns to identify keywords without prior knowledge. For example, it might recognize “random forest” as a topic, even if it wasn’t in the training data. By analyzing other algorithms discussed in articles, the NER model can identify phrase patterns that indicate a specific topic.&lt;/p&gt;

&lt;h2 id=&quot;the-machine-learning-language-model-behind&quot;&gt;The machine learning language model behind&lt;/h2&gt;

&lt;h3 id=&quot;spacy-framework-for-nlp&quot;&gt;SpaCy Framework for NLP&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://spacy.io/&quot;&gt;SpaCy&lt;/a&gt; is an open-source library tailored for advanced natural language processing in Python. It’s built for production use and helps create applications that process large volumes of text. SpaCy can be used to build information extraction systems, natural language understanding systems, or text preprocessing pipelines for deep learning. Among its features are tokenization, parts-of-speech (PoS) tagging, text classification, and named entity recognition.&lt;/p&gt;

&lt;p&gt;SpaCy offers an efficient, statistical system for NER in Python. Beyond the default entities, SpaCy allows us to add custom classes to the NER model and train it with new examples.&lt;/p&gt;

&lt;p&gt;SpaCy’s NER model is based on &lt;strong&gt;Convolutional Neural Networks (CNNs)&lt;/strong&gt;. For those interested, more details on how SpaCy’s NER model works can be found in the video below:&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/sqDHBH9IjRU&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/p&gt;

&lt;h3 id=&quot;training-data&quot;&gt;Training data&lt;/h3&gt;

&lt;p&gt;To train our model to recognize tech keywords, we scraped some Medium articles through &lt;strong&gt;web scraping&lt;/strong&gt;.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/UrszulaCzerwinska/db0aa37b1cb10ec94205d847f63ddc4f.js&quot;&gt;&lt;/script&gt;

&lt;div&gt;&lt;span class=&quot;image fit&quot;&gt;&lt;img src=&quot;/images/NER_img3.png&quot; alt=&quot;Table showing training data for language model&quot; /&gt;&lt;/span&gt;&lt;/div&gt;
&lt;p align=&quot;center&quot;&gt;&lt;sub&gt;&lt;sup&gt;An extract from the table containing the contents of the medium articles&lt;/sup&gt;&lt;/sub&gt;&lt;/p&gt;

&lt;p&gt;The text of each article was split into sentences for easier annotation.&lt;/p&gt;

&lt;p&gt;For NER annotation, there are tools like &lt;strong&gt;Prodigy&lt;/strong&gt;, but we opted for a simple spreadsheet where we manually marked the entities in dedicated columns.&lt;/p&gt;

&lt;div&gt;&lt;span class=&quot;image fit&quot;&gt;&lt;img src=&quot;/images/NER_img4.png&quot; alt=&quot;Table showing training data for language model and first predictions&quot; /&gt;&lt;/span&gt;&lt;/div&gt;

&lt;p&gt;With around twenty articles (~600 sentences), our model began to show promising performance, achieving over 0.78 accuracy on the test set. We separated the train and test data to evaluate the model effectively.&lt;/p&gt;

&lt;div&gt;&lt;span class=&quot;image fit&quot;&gt;&lt;img src=&quot;/images/NER_img5.png&quot; alt=&quot;Table showing training data for language model and first predictions&quot; /&gt;&lt;/span&gt;&lt;/div&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;TRAIN_DATA_ALL&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_table&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;apply&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;mark_targets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;ORG&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;PERSON&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;LOC&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;TOPIC&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;GPE&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;DATE&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;EVENT&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;WORK_OF_ART&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;sents&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;ORG&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;PERSON&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;LOC&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;TOPIC&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;GPE&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;DATE&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;EVENT&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;WORK_OF_ART&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;div&gt;&lt;span class=&quot;image fit&quot;&gt;&lt;img src=&quot;/images/NER_img6.png&quot; alt=&quot;Table showing training data for language model and first predictions&quot; /&gt;&lt;/span&gt;&lt;/div&gt;

&lt;p&gt;We fine-tuned the algorithm by adjusting parameters like the number of iterations, dropout rate, learning rate, and batch size.&lt;/p&gt;

&lt;h3 id=&quot;the-nlp-model-assesment&quot;&gt;The NLP model assesment&lt;/h3&gt;

&lt;p&gt;In addition to the model’s loss metric, we implemented precision, recall, and F1 score to measure performance more accurately.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/UrszulaCzerwinska/c23ce9e0edffe6f9790a2bbf8f018a4b.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;After training on the annotated data, the best model’s performance on our test set was quite impressive, especially considering the modest training data size (~3000 sentences).&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;precision :  0.9588053949903661
recall :  0.9211764705882353
f1_score :  0.9396221959858323

It is is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy.

TOPIC Python
TOPIC NumPy
TOPIC SciPy&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;In the &lt;strong&gt;Flow&lt;/strong&gt; on DSS, the process can be summarized by the graph:&lt;/p&gt;

&lt;div&gt;&lt;span class=&quot;image fit&quot;&gt;&lt;img src=&quot;/images/NER_img7.png&quot; alt=&quot;&quot; /&gt;&lt;/span&gt;&lt;/div&gt;
&lt;p align=&quot;center&quot;&gt;&lt;sub&gt;&lt;sup&gt;Flow on Dataiku&apos;s DSS platform: the annotated dataset is divided into train and test, the model learned on the train data is evaluated on the train and test batches.&lt;/sup&gt;&lt;/sub&gt;&lt;/p&gt;

&lt;p&gt;Returning to our Barack Obama example, our algorithm now detects the NLP algorithm entity as a TOPIC, in addition to the ORG (organization), LOC (location), GPE (geopolitical entity), and DATE categories.&lt;/p&gt;

&lt;p&gt;We have succeeded! 🚀&lt;/p&gt;

&lt;p&gt;The next step involves incorporating the model into our recommendation system, enhancing the customization of articles offered to users based on detected topics.&lt;/p&gt;

&lt;div&gt;&lt;span class=&quot;image fit&quot;&gt;&lt;img src=&quot;/images/NER_img8.png&quot; alt=&quot;&quot; /&gt;&lt;/span&gt;&lt;/div&gt;

&lt;p&gt;The finalized model can be compiled as an independent python library (instructions here) and installed with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pip&lt;/code&gt;. This is very practical for deploying the model in another environment and for production setup.&lt;/p&gt;

&lt;div&gt;&lt;span class=&quot;image fit&quot;&gt;&lt;img src=&quot;/images/NER_img9.jpg&quot; alt=&quot;&quot; /&gt;&lt;/span&gt;&lt;/div&gt;

&lt;h2 id=&quot;exploitation-of-the-model&quot;&gt;Exploitation of the model&lt;/h2&gt;

&lt;h3 id=&quot;analysis-of-an-article-medium&quot;&gt;Analysis of an article Medium&lt;/h3&gt;

&lt;p&gt;In our mini webapp, presented at the EGG, it is possible to display the most frequent entities of a Medium article.&lt;/p&gt;

&lt;p&gt;Thus, for the article: &lt;a href=&quot;https://towardsdatascience.com/cat-dog-or-elon-musk-145658489730&quot;&gt;https://towardsdatascience.com/cat-dog-or-elon-musk-145658489730&lt;/a&gt;, the most frequent entities were: model, MobileNet, Transfer learning, network, Python. We also detected people: Elon Musk, Marshal McLuhan and organizations: Google, Google Brain.&lt;/p&gt;

&lt;div&gt;&lt;span class=&quot;image fit&quot;&gt;&lt;img src=&quot;/images/NER_img10.png&quot; alt=&quot;&quot; /&gt;&lt;/span&gt;&lt;/div&gt;

&lt;p&gt;Inspired by &lt;a href=&quot;https://towardsdatascience.com/@bramblexu&quot;&gt;Xu LIANG’s&lt;/a&gt; &lt;a href=&quot;https://towardsdatascience.com/textrank-for-keyword-extraction-by-python-c0bae21bcec0&quot;&gt;post&lt;/a&gt;, we also used his way of representing the relationship between words in the form of a graph of linguistic dependencies. Unlike in his method, we did not use TextRank or TFIDF to detect keywords but we only applied our pre-trained NER model.&lt;/p&gt;

&lt;p&gt;Then, like &lt;a href=&quot;https://towardsdatascience.com/@bramblexu&quot;&gt;Xu LIANG&lt;/a&gt;, we used the capacity of Parts-of-Speech (PoS) Tagging, inherited by our model from the original model (&lt;a href=&quot;https://github.com/explosion/spacy-models/releases/tag/en_core_web_md-2.2.0&quot;&gt;en_core_web_md&lt;/a&gt;), to link the entities together with the edges, which forms the graph below.&lt;/p&gt;

&lt;div&gt;&lt;span class=&quot;image fit&quot;&gt;&lt;img src=&quot;/images/NER_img11.png&quot; alt=&quot;&quot; /&gt;&lt;/span&gt;&lt;/div&gt;
&lt;p&gt;&lt;sub&gt;&lt;sup&gt;The graph of dependencies between the entities detected in the article “Cat, Dog, or Elon Musk?”&lt;/sup&gt;&lt;/sub&gt;&lt;/p&gt;

&lt;p&gt;Thus, we get a graph where the keywords are placed around their category: Tech topic, Person and Organization.&lt;/p&gt;

&lt;p&gt;This gives a quick overview of the content of a Medium article.&lt;/p&gt;

&lt;p&gt;Here is how to get the graph from a Medium article url link:&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/UrszulaCzerwinska/d1d77f0bf8bd089103994eb3883db28f.js&quot;&gt;&lt;/script&gt;

&lt;h2 id=&quot;to-go-further&quot;&gt;To go further&lt;/h2&gt;
&lt;p&gt;Our Saegus Showroom including the functional webapp is coming soon. Feel free to follow our page &lt;a href=&quot;https://medium.com/data-by-saegus&quot;&gt;https://medium.com/data-by-saegus&lt;/a&gt; to be kept informed.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The project we have outlined here can easily be transposed into the various fields of industry: technical, legal and medical documents. It could be very interesting to analyse the civil, criminal and law… with this approach for a better efficiency in the research that all legal professionals do.&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt;

&lt;p&gt;To conclude, by recognizing topics within Medium articles, this solution represents a significant leap forward in content personalization. Whether for individual readers or professionals seeking articles on specific subjects, automatic keyword extraction offers a tailored experience. This model’s ability to classify articles based on finely-tuned NER allows for precise, relevant recommendations, improving overall user satisfaction and engagement.&lt;/p&gt;

&lt;p&gt;We invite you to explore this exciting field and consider how such technology could be adapted to your specific needs.&lt;/p&gt;

&lt;h2 id=&quot;disclaimer&quot;&gt;&lt;em&gt;Disclaimer&lt;/em&gt;&lt;/h2&gt;

&lt;p&gt;This article is a result of a teamwork realized at &lt;a href=&quot;http://saegus.com/fr/&quot;&gt;Saegus&lt;/a&gt;. Published originally in French at &lt;a href=&quot;https://medium.com/data-by-saegus/ner-medium-articles-saegus-7ffec0f3188c&quot;&gt;Medium&lt;/a&gt;.&lt;/p&gt;

&lt;section&gt;
&lt;!-- Go to www.addthis.com/dashboard to customize your tools --&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-584ec4ce89deed84&quot;&gt;&lt;/script&gt;
&lt;!-- Go to www.addthis.com/dashboard to customize your tools --&gt; &lt;div class=&quot;addthis_inline_share_toolbox&quot;&gt;&lt;/div&gt;
&lt;section&gt;

&lt;/section&gt;&lt;/section&gt;
</description>
        <pubDate>Mon, 11 Nov 2019 00:00:00 +0100</pubDate>
        <link>http://urszulaczerwinska.github.io/works/egg_ner</link>
        <guid isPermaLink="true">http://urszulaczerwinska.github.io/works/egg_ner</guid>
        
        <category>NLP</category>
        
        <category>Python</category>
        
        <category>featured</category>
        
        <category>machine learning</category>
        
        <category>NER</category>
        
        <category>language models</category>
        
        
        <category>works</category>
        
      </item>
    
      <item>
        <title>Women in Healthcare Analytics and Data Science (WiHADS)</title>
        <description>&lt;html&gt;
  &lt;head&gt;
    &lt;meta
      name=&quot;description&quot;
      content=&quot;Urszula Czerwinska orgnaiser of a meetup WiHADS | Join WiHADS, a pioneering community promoting women in data science, healthcare analytics, and deep learning AI. Explore opportunities to lead data science projects, engage with deep learning models, and shape the future of AI in healthcare.&quot;
    /&gt;
  &lt;/head&gt;

  &lt;body&gt;&lt;/body&gt;
  &lt;section&gt;
    &lt;h2&gt;
      Introduction: Empowering Women in Data Science and AI for Healthcare
    &lt;/h2&gt;
    &lt;p&gt;
      In today’s rapidly evolving world, data science is driving innovation
      across industries, particularly in healthcare where AI and deep learning
      models are transforming patient outcomes. The
      &lt;strong&gt;Women in Healthcare Analytics and Data Science (WiHADS)&lt;/strong&gt;
      initiative is leading the charge, empowering women in data science to
      spearhead crucial data science projects that are revolutionizing the
      healthcare sector.
    &lt;/p&gt;
    &lt;h2&gt;The Beginnings of WiHADS: A Movement in Data Science and Healthcare&lt;/h2&gt;
    &lt;p&gt;
      WiHADS began in 2017, spearheaded by
      &lt;a href=&quot;https://www.linkedin.com/in/sameh-m-8575a3179/&quot;&gt;Sameh Megrhi&lt;/a&gt;,
      with a vision to amplify the voices of women in data science and
      healthcare analytics.&lt;br /&gt;
    &lt;/p&gt;

    &lt;blockquote&gt;
      &lt;p&gt;
        The initiative was created to address the underrepresentation of women
        in fields like deep learning models, data science and engineering, and
        AI in healthcare. By highlighting the significant contributions of
        women, WiHADS aims to inspire more women to enter and thrive in these
        critical sectors.
      &lt;/p&gt;
      &lt;p&gt;
        Since its inception, WiHADS has brought together a healthcare data
        science community. The platform provides a space where women can share
        their experiences and insights, fostering a collaborative environment
        that encourages innovation in data science and machine learning. These
        meetups are not just about discussions—they are about solving real-world
        challenges using deep learning algorithms and data science
        methodologies.
      &lt;/p&gt;
    &lt;/blockquote&gt;
    &lt;h2&gt;WiHADS Meetups: A Hub for Innovation in Data Science and Healthcare&lt;/h2&gt;
    &lt;p&gt;
      WiHADS meetups have become a central platform for women in data science to
      exchange ideas, network, and discuss the latest trends in data science and
      machine learning. Since 2017, WiHADS has organized four major meetups in
      Paris, each focusing on the intersection of data science and healthcare.
      These events have garnered attention within the data science community and
      beyond, as they provide invaluable insights into how data science and deep
      learning AI are transforming the healthcare industry.
    &lt;/p&gt;
    &lt;blockquote&gt;
      &lt;b
        &gt;Join us at
        &lt;a
          href=&quot;https://www.meetup.com/fr-FR/Healthcare-Analytics-Data-Science/&quot;
          alt=&quot;The WiHADS meetups are place to exchange about woman in Data Science, Machine Learning and AI&quot;
          &gt;our meetup webpage&lt;/a
        &gt;&lt;/b
      &gt;
    &lt;/blockquote&gt;

    &lt;h2&gt;Key Components of WiHADS Meetups:&lt;/h2&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;strong&gt;Speakers:&lt;/strong&gt; The meetups feature accomplished women from
        top companies like
        &lt;a
          alt=&quot;IQVIA, a global leader in clinical research and healthcare data, drives innovation to enhance patient and public health. By integrating data, insights, technology, and expertise, IQVIA empowers clients and partners to advance and market cutting-edge therapies.&quot;
          href=&quot;https://www.iqvia.com/fr-fr/locations/france&quot;
          &gt;IQvia&lt;/a
        &gt;,
        &lt;a
          alt=&quot;Doctolib a leader in AI for healthcare.&quot;
          href=&quot;https://about.doctolib.fr/?utm_button=header&amp;utm_website=doctolib_career%2F&quot;
          &gt;Doctolib&lt;/a
        &gt;, and
        &lt;a
          alt=&quot;Democratizing expert cardiac care through medical-grade AI and cloud technology&quot;
          href=&quot;https://cardiologs.com/&quot;
          &gt;Cardiologs&lt;/a
        &gt;, who share their expertise on data science projects, deep learning
        models, and their applications in healthcare. These sessions provide
        attendees with a deeper understanding of how data science is applied in
        real-world healthcare settings.
      &lt;/li&gt;

      &lt;li&gt;
        &lt;strong&gt;Opportunities:&lt;/strong&gt; The events offer a unique chance for
        women in data science to connect, collaborate, and learn from each
        other. This networking is crucial for building a strong data science and
        engineering network that can support professional growth and innovation.
      &lt;/li&gt;

      &lt;li&gt;
        &lt;strong&gt;Roundtable Discussions:&lt;/strong&gt; After the presentations,
        participants engage in roundtable discussions, where they explore the
        ethical implications of AI in healthcare, the challenges of implementing
        deep learning algorithms in medical practice, and the future of data
        science in healthcare.
      &lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;
      WiHADS is committed to fostering a supportive environment where women in
      data science can thrive. The meetups are free to attend, thanks to the
      generous support of sponsors like
      &lt;a
        alt=&quot;Doctolib a leader in AI for healthcare.&quot;
        href=&quot;https://about.doctolib.fr/?utm_button=header&amp;utm_website=doctolib_career%2F&quot;
        &gt;Doctolib&lt;/a
      &gt;,
      &lt;a
        alt=&quot;IQVIA, a global leader in clinical research and healthcare data, drives innovation to enhance patient and public health. By integrating data, insights, technology, and expertise, IQVIA empowers clients and partners to advance and market cutting-edge therapies.&quot;
        href=&quot;https://www.iqvia.com/fr-fr/locations/france&quot;
        &gt;IQvia&lt;/a
      &gt;, and
      &lt;a
        alt=&quot;Saegus is supporting businesses in their transition to a combined intelligence model, where artificial intelligence serves humanity&quot;
        href=&quot;https://www.saegus.com/&quot;
        &gt;Saegus&lt;/a
      &gt;, who believe in the importance of promoting diversity in data science
      and AI.
    &lt;/p&gt;

    &lt;h2&gt;The Vision Behind WiHADS: Promoting Women in Data Science&lt;/h2&gt;
    &lt;p&gt;
      The driving force behind WiHADS is the belief that women have a critical
      role to play in the future of data science and healthcare. With a
      background in systems biology and extensive experience in machine learning
      within academia and the pharmaceutical industry, I joined WiHADS in 2018
      to promote the visibility of women in these fields. My goal was to build a
      platform where women could not only showcase their achievements in data
      science and AI in healthcare but also mentor and support one another.
      Working alongside
      &lt;a
        alt=&quot;Data scientist Sameh Megrhi is one of leaders in Machine Learning&quot;
        href=&quot;https://www.linkedin.com/in/sameh-m-8575a3179/&quot;
        &gt;Sameh Megrhi&lt;/a
      &gt;,&lt;a
        alt=&quot;Imen Helali working in Healthcare and Pharmaceutics is highly motivated promote Healthcare and Data Science fields&quot;
        href=&quot;https://www.linkedin.com/in/imen-helali-phd-a6a4222b/&quot;
      &gt;
        Imen Helali&lt;/a
      &gt;
      and
      &lt;a
        atl=&quot;Young data science professional Avani Tanna is promoting woman in tech importance&quot;
        href=&quot;https://www.linkedin.com/in/avani-tanna-b20b8a77/&quot;
        &gt;Avani Tanna&lt;/a
      &gt;, we crafted a unique meetup format that balances expert presentations
      with interactive, community-driven discussions. Our vision is to create a
      vibrant healthcare data science community where women can gain the skills
      and confidence needed to lead impactful data science projects.
    &lt;/p&gt;
    &lt;h2&gt;The Impact of WiHADS on the Healthcare Data Science Community&lt;/h2&gt;
    &lt;p&gt;
      WiHADS has had a profound impact on the healthcare data science community
      by providing a platform that not only highlights the achievements of women
      but also encourages ongoing education and professional development. The
      meetups have sparked important conversations about the future of data
      science in healthcare and the role of deep learning AI in improving
      patient care. Through WiHADS, we have seen an increase in the number of
      women pursuing careers in data science and machine learning, particularly
      in the healthcare sector. By offering opportunities for mentorship,
      networking, and skill development, WiHADS is helping to close the gender
      gap in data science and ensure that women are well-represented in the
      future of AI in healthcare.
    &lt;/p&gt;
    &lt;h2&gt;
      Get Involved with WiHADS: Join the Movement in Data Science and Healthcare
    &lt;/h2&gt;
    &lt;p&gt;
      Whether you are an experienced data scientist or just beginning your
      journey in data science and machine learning, WiHADS offers a welcoming
      community where you can grow, learn, and contribute. There are many ways
      to get involved with WiHADS and become part of this exciting movement.
    &lt;/p&gt;
    &lt;h2&gt;Ways to Participate:&lt;/h2&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;strong&gt;Attend a Meetup:&lt;/strong&gt;Join us at our next event to connect
        with fellow professionals and learn about the latest developments in
        data science and deep learning models for healthcare. Stay updated on
        upcoming events by visiting our
        &lt;a
          alt=&quot;Join WiHADS, a pioneering community promoting women in data science, healthcare analytics, and deep learning AI. Explore opportunities to lead data science projects, engage with deep learning models, and shape the future of AI in healthcare.&quot;
          href=&quot;https://www.meetup.com/fr-FR/Healthcare-Analytics-Data-Science/&quot;
          &gt;Meetup page&lt;/a
        &gt;.
      &lt;/li&gt;

      &lt;li&gt;
        &lt;strong&gt;Become a Speaker:&lt;/strong&gt; Share your knowledge and experience
        in data science projects or deep learning algorithms by speaking at one
        of our meetups. We are always looking for new voices to contribute to
        our discussions on AI in healthcare.
      &lt;/li&gt;

      &lt;li&gt;
        &lt;strong&gt;Sponsor an Event:&lt;/strong&gt; Help us continue to provide free,
        high-quality events by sponsoring a WiHADS meetup. Your support will
        enable us to reach more women in data science and expand our impact in
        the healthcare industry.
      &lt;/li&gt;
    &lt;/ul&gt;

    &lt;h2&gt;Conclusion: Shaping the Future of Data Science in Healthcare&lt;/h2&gt;
    &lt;p&gt;
      As we look to the future, data science and AI will continue to shape
      healthcare, and WiHADS is committed to ensuring that women in tech are at
      the forefront of this transformation. By joining us, you’re becoming part
      of a powerful movement dedicated to advancing deep learning algorithms and
      data science in healthcare. Together, we can lead the way in pioneering
      solutions that enhance patient care and drive innovation

      &lt;p&gt;&lt;strong&gt;Contact Us:&lt;/strong&gt; If you&apos;re interested in hosting a meetup or
      sponsoring an event, please reach out to us. We look forward to welcoming
      you to the WiHADS community.&lt;/p&gt;
    &lt;/p&gt;
    &lt;span class=&quot;image fit&quot;
      &gt;&lt;img
        src=&quot;/images/wihads3.jpeg&quot;
        alt=&quot;Group of women leaders in healthcare analytics and data science at a WiHADS meetup, fostering innovation in data science, deep learning AI, and healthcare.&quot;
        width=&quot;70%&quot;
    /&gt;&lt;/span&gt;

    &lt;!-- Go to www.addthis.com/dashboard to customize your tools --&gt;
    &lt;script
      type=&quot;text/javascript&quot;
      src=&quot;//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-584ec4ce89deed84&quot;
    &gt;&lt;/script&gt;
    &lt;!-- Go to www.addthis.com/dashboard to customize your tools --&gt;
    &lt;div class=&quot;addthis_inline_share_toolbox&quot;&gt;&lt;/div&gt;
  &lt;/section&gt;
&lt;/html&gt;
</description>
        <pubDate>Tue, 01 Oct 2019 00:00:00 +0200</pubDate>
        <link>http://urszulaczerwinska.github.io/works/women-healthcare-data-science-analytics-wihads</link>
        <guid isPermaLink="true">http://urszulaczerwinska.github.io/works/women-healthcare-data-science-analytics-wihads</guid>
        
        <category>data science</category>
        
        <category>events</category>
        
        <category>healthcare</category>
        
        <category>networking</category>
        
        
        <category>works</category>
        
      </item>
    
      <item>
        <title>Language gap between academia and business</title>
        <description>&lt;!DOCTYPE html&gt;
&lt;html&gt;
  &lt;head&gt;
    &lt;meta
      name=&quot;description&quot;
      content=&quot;An insider view on the language gap between academia and business, and how to bridge it from a data scientist perspective. | Transitioning from academia to industry.&quot;
    /&gt;
  &lt;/head&gt;

  &lt;body&gt;
    &lt;style&gt;
      .highlightme { background-color:#ccffe5; }
    &lt;/style&gt;

    &lt;section&gt;
      &lt;h2&gt;
        Transitioning from Academia to Industry: A Guide for Aspiring Data
        Scientists
      &lt;/h2&gt;
      &lt;p&gt;
        &lt;em
          &gt;Have you ever struggled to communicate your ideas effectively to
          business professionals when transitioning from academia to industry?
          If you&apos;re coming from an academic background and aiming for a career
          in the private sector, you may find the jargon and communication
          styles quite different.&lt;/em
        &gt;
      &lt;/p&gt;
      &lt;h3&gt;Understanding the Language Gap Between Academia and Business&lt;/h3&gt;
      &lt;p&gt;
        Academia and the private sector often speak different languages. In
        academia, the focus is on innovation and originality, leading to the use
        of terms like
        &lt;b&gt;novel&lt;/b&gt;, &lt;b&gt;insight&lt;/b&gt;, &lt;b&gt;paradigm&lt;/b&gt;, &lt;b&gt;robust&lt;/b&gt;, and
        &lt;b&gt;elucidate&lt;/b&gt;. Research often involves complex landscapes and
        frameworks, with a tendency to use words like &lt;b&gt;putative&lt;/b&gt;,
        &lt;b&gt;respectively&lt;/b&gt;, and &lt;b&gt;potentially&lt;/b&gt; to handle uncertainty and
        criticism.
      &lt;/p&gt;

      &lt;p&gt;
        On the other hand, the business world values clarity and efficiency.
        Terms such as &lt;b&gt;ballpark&lt;/b&gt;, &lt;b&gt;itemize&lt;/b&gt;, &lt;b&gt;prioritize&lt;/b&gt;,
        &lt;b&gt;agile&lt;/b&gt;, &lt;b&gt;incentivize&lt;/b&gt;, &lt;b&gt;proactive&lt;/b&gt;, and
        &lt;b&gt;empowerment&lt;/b&gt; reflect the need for quick solutions and strategic
        thinking. Business professionals focus on value, profit, and material
        gains, often using expressions like &lt;b&gt;push the envelope&lt;/b&gt; and
        &lt;b&gt;added value&lt;/b&gt;.
      &lt;/p&gt;
      &lt;div&gt;
        &lt;span class=&quot;image fit&quot;
          &gt;&lt;img
            src=&quot;/images/boss.jpg&quot;
            alt=&quot;Transitioning from academia to industry can be challenging but rewarding&quot;
        /&gt;&lt;/span&gt;
      &lt;/div&gt;

      &lt;h2&gt;The Critical Attitude Shift&lt;/h2&gt;

      &lt;p&gt;
        Academics are trained to be critical and challenge ideas rigorously.
        However, in the business world, being overly critical can be
        counterproductive. In business, you&apos;ll often hear terms like
        &lt;b&gt;opportunities for improvement&lt;/b&gt; rather than &lt;i&gt;pitfalls&lt;/i&gt;. Issues
        are described as &lt;b&gt;in development&lt;/b&gt; or &lt;b&gt;with growth potential&lt;/b&gt;,
        and even terminations are referred to as &lt;b&gt;restructuring&lt;/b&gt;. Adapting
        to this language and attitude is crucial for fitting into the corporate
        environment.
      &lt;/p&gt;

      &lt;h2&gt;Adapting to Corporate Culture&lt;/h2&gt;

      &lt;p&gt;
        Business professionals often prioritize efficiency and profit. Words
        like
        &lt;b&gt;ballpark&lt;/b&gt;, &lt;b&gt;itemize&lt;/b&gt;, &lt;b&gt;agile&lt;/b&gt;, and
        &lt;b&gt;takeaways&lt;/b&gt; emphasize task management and speed. Expressions such
        as &lt;b&gt;incentivize&lt;/b&gt;, &lt;b&gt;proactive&lt;/b&gt;, and &lt;b&gt;repurpose&lt;/b&gt; highlight
        the dynamic nature of corporate work. When discussing strategy, terms
        like &lt;b&gt;empowerment&lt;/b&gt; and &lt;b&gt;leverage&lt;/b&gt; are common, reflecting a
        focus on strategic impact and financial value.
      &lt;/p&gt;

      &lt;blockquote&gt;
        The difference in wording reflects different attitudes and methods used
        in business and research. However, the skills gained in research can be
        valuable in the business world.
        &lt;span class=&quot;highlightme&quot;
          &gt;&lt;b
            &gt;Mastering the art of communication can facilitate a smooth
            transition from academia to industry.&lt;/b
          &gt;&lt;/span
        &gt;
        Effective communication is crucial for team success and overall company
        performance.
      &lt;/blockquote&gt;

      &lt;p&gt;
        In academia, you often manage a project from start to finish,
        communicating with peers who are already familiar with your topic. In
        contrast, the corporate world involves working within a larger process,
        requiring clear communication and documentation. Corporate language
        often emphasizes organization and management.
      &lt;/p&gt;

      &lt;h2&gt;Crafting a Business-Ready CV&lt;/h2&gt;

      &lt;p&gt;
        When transitioning to a corporate role, your CV needs to shift from the
        academic format. Instead of a detailed list of projects and
        publications, focus on achievements and contributions that align with
        business needs. Follow
        &lt;a
          alt=&quot;Google&apos;s recommendations for tech jobs&quot;
          href=&quot;https://www.businessinsider.fr/us/google-exec-gives-key-to-perfect-resume-2014-9&quot;
          &gt;Google&apos;s recommendations&lt;/a
        &gt;
        for tech jobs, using a business tone and highlighting impact. For
        example, replace academic jargon with phrases like &apos;Achieved Z&apos; or
        &apos;Managed a team of N people&apos;.
      &lt;/p&gt;

      &lt;p&gt;
        In the business world, the emphasis is less on publications and more on
        the skills and logistics involved in achieving results. Break down your
        academic achievements into understandable and valuable skills for
        potential employers.
      &lt;/p&gt;

      &lt;div&gt;
        &lt;span class=&quot;image fit&quot;
          &gt;&lt;img
            src=&quot;/images/work.jpg&quot;
            alt=&quot;Effective CV writing is crucial for transitioning from academia to industry&quot;
        /&gt;&lt;/span&gt;
      &lt;/div&gt;

      &lt;h2&gt;Presenting Your Research&lt;/h2&gt;

      &lt;p&gt;
        When discussing your research, practice explaining it to non-academic
        audiences. Focus on the big picture and avoid technical jargon. Use
        everyday language and make parallels with business or everyday life.
        Consider preparing an
        &lt;a
          href=&quot;http://thepostdocway.com/content/elevator-pitches-scientists-what-when-where-and-how&quot;
          &gt;elevator pitch&lt;/a
        &gt;
        or reviewing
        &lt;a href=&quot;https://vimeo.com/threeminutethesis&quot;&gt;3MT&lt;/a&gt; videos for
        inspiration.
      &lt;/p&gt;

      &lt;h2&gt;Effective Presentations&lt;/h2&gt;

      &lt;p&gt;
        In the corporate world, presentations are crucial for persuading and
        informing. Unlike academic presentations, business presentations require
        a strong focus on the form and visual appeal. Ensure your slides are
        well-designed and that your presentation highlights the big picture
        before delving into details.
      &lt;/p&gt;

      &lt;h2&gt;Final Thoughts&lt;/h2&gt;

      &lt;p&gt;
        Although there is a significant gap between academia and business,
        bridging this divide can be rewarding. Adapting to business language and
        practices will open new doors and enhance your career opportunities.
        Engage with industry events, practice your communication skills, and be
        prepared for interviews to ensure a successful transition.
      &lt;/p&gt;

      &lt;div&gt;
        &lt;span class=&quot;image fit&quot;
          &gt;&lt;img
            src=&quot;/images/raising.jpg&quot;
            alt=&quot;Successfully transitioning from academia to industry opens new career opportunities&quot;
        /&gt;&lt;/span&gt;
      &lt;/div&gt;
	  &lt;p&gt;Also published on &lt;a alt=&quot;Discover more stories about Deep Learnig, Machine Learning and Data Science by Urszula Czerwinska&quot; href=&quot;https://medium.com/@ulalaparis/language-gap-between-academia-and-business-ef534ce71d10&quot;&gt;Medium&lt;/a&gt;&lt;/p&gt;
    &lt;/section&gt;
  &lt;/body&gt;
  &lt;!-- Go to www.addthis.com/dashboard to customize your tools --&gt;
  &lt;script
    type=&quot;text/javascript&quot;
    src=&quot;//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-584ec4ce89deed84&quot;
  &gt;&lt;/script&gt;

  &lt;!-- Go to www.addthis.com/dashboard to customize your tools --&gt;
  &lt;div class=&quot;addthis_inline_share_toolbox&quot;&gt;&lt;/div&gt;
&lt;/html&gt;
</description>
        <pubDate>Sat, 19 Jan 2019 00:00:00 +0100</pubDate>
        <link>http://urszulaczerwinska.github.io/thoughts/businesslang</link>
        <guid isPermaLink="true">http://urszulaczerwinska.github.io/thoughts/businesslang</guid>
        
        
        <category>thoughts</category>
        
      </item>
    
      <item>
        <title>PhD Thesis</title>
        <description>&lt;!DOCTYPE HTML&gt;
&lt;html lang=&quot;en-US&quot;&gt;
    &lt;head&gt;
        &lt;meta charset=&quot;UTF-8&quot;&gt;
        &lt;meta http-equiv=&quot;refresh&quot; content=&quot;1; url=&quot;&gt;
        &lt;script type=&quot;text/javascript&quot;&gt;
            window.location.href = &quot;https://urszulaczerwinska.github.io/UCzPhDThesis/&quot;
        &lt;/script&gt;
        &lt;title&gt;Page Redirection&lt;/title&gt;
    &lt;/head&gt;
    &lt;body&gt;
        &lt;!-- Note: don&apos;t tell people to `click` the link, just tell them that it is a link. --&gt;
        If you are not redirected automatically, follow this &lt;a href=&apos;https://urszulaczerwinska.github.io/UCzPhDThesis/&apos;&gt;link to example&lt;/a&gt;.
          &lt;!-- Go to www.addthis.com/dashboard to customize your tools --&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-584ec4ce89deed84&quot;&gt;&lt;/script&gt; 
&lt;!-- Go to www.addthis.com/dashboard to customize your tools --&gt; &lt;div class=&quot;addthis_inline_share_toolbox&quot;&gt;&lt;/div&gt;
 
    &lt;/body&gt;
&lt;/html&gt;</description>
        <pubDate>Thu, 02 Aug 2018 00:00:00 +0200</pubDate>
        <link>http://urszulaczerwinska.github.io/works/PhDThesis.html</link>
        <guid isPermaLink="true">http://urszulaczerwinska.github.io/works/PhDThesis.html</guid>
        
        <category>Machine Learning</category>
        
        <category>R</category>
        
        <category>Data Science</category>
        
        <category>biomedical applications</category>
        
        
        <category>works</category>
        
      </item>
    
      <item>
        <title>DeconICA</title>
        <description>&lt;!DOCTYPE HTML&gt;
&lt;html lang=&quot;en-US&quot;&gt;
    &lt;head&gt;
        &lt;meta charset=&quot;UTF-8&quot;&gt;
        &lt;meta http-equiv=&quot;refresh&quot; content=&quot;1; url=&quot;&gt;
        &lt;script type=&quot;text/javascript&quot;&gt;
            window.location.href = &quot;https://urszulaczerwinska.github.io/DeconICA/&quot;
        &lt;/script&gt;
        &lt;title&gt;Page Redirection&lt;/title&gt;
    &lt;/head&gt;
    &lt;body&gt;
        &lt;!-- Note: don&apos;t tell people to `click` the link, just tell them that it is a link. --&gt;
        If you are not redirected automatically, follow this &lt;a href=&apos;https://urszulaczerwinska.github.io/DeconICA/&apos;&gt;link to example&lt;/a&gt;.
          &lt;!-- Go to www.addthis.com/dashboard to customize your tools --&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-584ec4ce89deed84&quot;&gt;&lt;/script&gt; 
&lt;!-- Go to www.addthis.com/dashboard to customize your tools --&gt; &lt;div class=&quot;addthis_inline_share_toolbox&quot;&gt;&lt;/div&gt;
 
    &lt;/body&gt;
&lt;/html&gt;</description>
        <pubDate>Tue, 22 May 2018 00:00:00 +0200</pubDate>
        <link>http://urszulaczerwinska.github.io/works/deconica</link>
        <guid isPermaLink="true">http://urszulaczerwinska.github.io/works/deconica</guid>
        
        <category>Machine Learning</category>
        
        <category>R</category>
        
        <category>Data Science</category>
        
        <category>Deconvolution</category>
        
        
        <category>works</category>
        
      </item>
    
      <item>
        <title>Big Dive</title>
        <description>&lt;!DOCTYPE html&gt;
&lt;html&gt;
  &lt;head&gt;
    &lt;meta
      name=&quot;description&quot;
      content=&quot;Join the Big Dive course to gain hands-on experience with big data, distributed computing, and advanced data visualization techniques. Perfect for those transitioning from academia to industry, this course offers practical projects, expert insights, and valuable networking opportunities in the data science field.&quot;
    /&gt;
  &lt;/head&gt;
  &lt;body&gt;
    &lt;style&gt;
      .scalable {
          overflow: hidden;
      }

      .scalable iframe {
          height: 100%;
          left: 0;
          position: absolute;
          top: 0;
          width: 100%;
      }

      .scalable .scalable-content {
          height: 0;
          position: relative;
      }

      .scalable-16-9 .scalable-content {
          /* Percentage from 9 divided by 16 for ratio of 16:9. */
          padding-bottom: 56.25%;
      }

      iframe {display:block;margin:auto;width: 100%}
    &lt;/style&gt;
  &lt;/body&gt;
  &lt;section&gt;
    &lt;p&gt;
      &lt;i&gt;&lt;strong&gt;Big data - big unknown&lt;/strong&gt;&lt;/i&gt;
    &lt;/p&gt;
    &lt;h2&gt;When we play with data that are not so big...&lt;/h2&gt;
    &lt;p&gt;
      Experience with big data is among top skills of a data scientist profile.
      However, it is not trivial to learn big data tools without necessary
      infrastructure and proper tutoring. While pursuing my PhD in Data science
      dealing with medium size data frames (&lt; 1G), usually all can be processed
      on one machine. In order to get some taste of methods and challenges of
      Big Data, I completed
      &lt;a href=&quot;http://www.bigdive.eu/&quot; target=&quot;_blank&quot;&gt;Big Dive&lt;/a&gt; course
      organized by
      &lt;a href=&quot;https://www.top-ix.org/en/home-eng/&quot; target=&quot;_blank&quot;&gt;TOP-IX&lt;/a&gt;.
    &lt;/p&gt;
    &lt;h3&gt;... but we want to get a taste of &lt;strong&gt;BIG data&lt;/strong&gt;&lt;/h3&gt;
    &lt;p&gt;
      &lt;span class=&quot;image right&quot;
        &gt;&lt;img src=&quot;/images/companies.png&quot; alt=&quot;&quot; /&gt;&lt;em
          &gt;Image rights TOP-IX&lt;/em
        &gt;&lt;/span
      &gt;
      During the intense 5-week course I put in practice my Python - pandas
      skills and learned new libraries for &lt;b&gt;distributed computing&lt;/b&gt;, as well
      as &lt;i&gt;Py&lt;b&gt;Spark&lt;/b&gt;&lt;/i&gt; basics. We also spent a significant amount of
      time setting up the infrastructure on
      &lt;a href=&quot;https://aws.amazon.com/fr/?nc2=h_lg&quot; target=&quot;_blank&quot;
        &gt;amazon aws&lt;/a
      &gt;
      and &lt;a href=&quot;https://www.mongodb.com/&quot; target=&quot;_blank&quot;&gt;&lt;b&gt;MongoDB&lt;/b&gt;&lt;/a
      &gt;. Invited keynote speakers showed us important aspects and challenges of
      the data science for &lt;b&gt;business and science&lt;/b&gt;. A bonus was an
      introduction to beautiful &lt;b&gt;visualization D3.js&lt;/b&gt;. Finally, for me the
      most interesting and important part of the course was a
      &lt;b&gt;project&lt;/b&gt; realized for one of 3 companies:
      &lt;a
        href=&quot;http://www.ifc.org/wps/wcm/connect/corp_ext_content/ifc_external_corporate_site/home&quot;
        target=&quot;_blank&quot;
        &gt;IFC&lt;/a
      &gt;, &lt;a href=&quot;https://eduscopio.it/&quot; target=&quot;_blank&quot;&gt;Eduscopio&lt;/a&gt; and
      &lt;a href=&quot;https://www.tesisquare.com/en/&quot; target=&quot;_blank&quot;&gt;TesiSquare&lt;/a&gt;
      where in just a few days we put our best efforts to make sense of hundreds
      of gigabytes of data!
    &lt;/p&gt;

    &lt;h2&gt;Introduction to the Big Dive Course&lt;/h2&gt;
    &lt;p&gt;
      The Big Dive course is an intensive program designed to provide
      participants with hands-on experience in data science, with a particular
      focus on big data, distributed computing, and data visualization. Held
      over five weeks, this course serves as an essential stepping stone for
      those looking to transition from academia to industry or deepen their
      understanding of big data tools and techniques. This article will walk you
      through the key aspects of the course, its importance in the data science
      landscape, and the personal and professional growth opportunities it
      offers.
    &lt;/p&gt;
    &lt;h2&gt;The Growing Importance of Big Data&lt;/h2&gt;
    &lt;p&gt;
      In today&apos;s data-driven world, big data has become a cornerstone of
      decision-making processes across various industries. The ability to
      process and analyze large volumes of data efficiently is a top skill for
      any data scientist. However, working with big data comes with its own set
      of challenges, such as the need for specialized tools and infrastructure.
      The Big Dive course provides a comprehensive introduction to these
      challenges, helping participants understand the significance of big data
      and equipping them with the necessary skills to tackle it.
    &lt;/p&gt;
    &lt;h2&gt;Learning Big Data Tools and Infrastructure&lt;/h2&gt;
    &lt;p&gt;
      One of the key highlights of the Big Dive course is the hands-on training
      in big data tools and infrastructure. Participants work extensively with
      Python and Pandas for data manipulation and learn the basics of PySpark
      for distributed computing. Setting up infrastructure on platforms like AWS
      and MongoDB is another critical component of the course, giving
      participants a solid foundation in managing and processing large data
      sets.
    &lt;/p&gt;
    &lt;h2&gt;Learning Big Data Tools and Infrastructure&lt;/h2&gt;
    &lt;p&gt;
      One of the key highlights of the Big Dive course is the hands-on training
      in big data tools and infrastructure. Participants work extensively with
      &lt;strong&gt;Python&lt;/strong&gt; and &lt;strong&gt;Pandas&lt;/strong&gt; for data manipulation
      and learn the basics of PySpark for distributed computing. Setting up
      infrastructure on platforms like AWS and
      &lt;a
        alt=&quot;MongoDB is a no sql database, extensively used in Data Science&quot;
        href=&quot;https://www.mongodb.com/&quot;
        target=&quot;_blank&quot;
        &gt;&lt;b&gt;MongoDB&lt;/b&gt;&lt;/a
      &gt;
      is another critical component of the course, giving participants a solid
      foundation in managing and processing large data sets.
    &lt;/p&gt;
    &lt;h2&gt;Deep Dive into Data Science Projects&lt;/h2&gt;
    &lt;p&gt;
      &lt;span class=&quot;image left&quot;
        &gt;&lt;img
          src=&quot;/images/bigdive6-WELCOME_def.jpg&quot;
          alt=&quot;Data Science Bootcamp Big Dive mix all elements of data science&quot;
        /&gt;&lt;em&gt;Image rights TOP-IX&lt;/em&gt;&lt;/span
      &gt;
      The practical application of knowledge is at the heart of the Big Dive
      course. Participants engage in real-world projects that simulate the
      challenges faced by data scientists in the industry. For instance, one
      project involved analyzing hundreds of gigabytes of data for the
      &lt;a
        alt=&quot;IFC a member of the World Bank Group — is the largest global development institution focused on the private sector in emerging markets.&quot;
        href=&quot;http://www.ifc.org/wps/wcm/connect/corp_ext_content/ifc_external_corporate_site/home&quot;
        target=&quot;_blank&quot;
        &gt;International Finance Corporation (IFC)&lt;/a
      &gt;. These projects not only reinforce the technical skills learned during
      the course but also highlight the importance of applying data science in
      solving complex business and scientific problems.
    &lt;/p&gt;
    &lt;p&gt;&lt;/p&gt;

    &lt;h2&gt;The Role of Visualization in Data Science&lt;/h2&gt;
    &lt;p&gt;
      Data visualization is a crucial aspect of data science, allowing data
      scientists to present complex data in a more understandable and actionable
      format. The Big Dive course introduces participants to advanced data
      visualization techniques, with a particular focus on D3.js, a powerful
      JavaScript library for creating interactive graphs. The course emphasizes
      the importance of visualization in data science projects, helping
      participants understand how to effectively communicate their findings.
    &lt;/p&gt;

    &lt;h2&gt;Interactive Graphs with D3.js&lt;/h2&gt;
    &lt;p&gt;
      D3.js is known for its ability to create dynamic, interactive graphs that
      can bring data to life. However, mastering D3.js comes with a steep
      learning curve. Participants in the Big Dive course learn how to overcome
      these challenges, create effective data visualizations, and explore
      alternatives like RShiny for those who prefer less complex tools.
      Understanding the intricacies of D3.js not only enhances the visual appeal
      of data presentations but also improves the interpretability of the data
      itself.
    &lt;/p&gt;
    &lt;h2&gt;Data Science Bootcamps: Are They Worth It?&lt;/h2&gt;
    &lt;p&gt;
      Data science bootcamps, like the Big Dive, are becoming increasingly
      popular as a fast track to acquiring the skills needed in the industry.
      But are they worth the investment? This section compares different
      bootcamps, weighing the cost against the potential benefits. The Big Dive
      stands out for its comprehensive curriculum, real-world projects, and the
      networking opportunities it provides. Success stories from past
      participants highlight the value of bootcamps in accelerating career
      growth in data science.
    &lt;/p&gt;

    &lt;div class=&quot;scalable scalable-16-9&quot;&gt;
      &lt;div class=&quot;scalable-content&quot;&gt;
        &lt;iframe
          src=&quot;https://www.youtube.com/embed/y1Zj_pOx_iE&quot;
          allowfullscreen
        &gt;&lt;/iframe
        &gt;&lt;br /&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;em&gt;Big Dive Yearbook 2017&lt;/em&gt;
    &lt;h2&gt;Key Skills Acquired During the Big Dive&lt;/h2&gt;
    &lt;p&gt;
      Participants leave the Big Dive course with a wealth of new skills. These
      include distributed computing, which is essential for handling large
      datasets, and the basics of PySpark, a tool widely used in big data
      processing. The course also offers a refresher on machine learning and
      network science, with practical mini-projects that can be used as
      portfolio pieces. The importance of understanding the data science cycle
      and applying it to real-world scenarios is another critical takeaway from
      the course.
    &lt;/p&gt;
    &lt;h2&gt;Challenges Faced During the Course&lt;/h2&gt;
    &lt;p&gt;
      Learning new technologies and tools is never without its challenges. The
      Big Dive course participants face several hurdles, such as the steep
      learning curve of D3.js and the complexities of distributed computing with
      Dask. Additionally, handling large datasets often requires substantial
      computational resources, which can be a significant challenge. However,
      overcoming these challenges is a key part of the learning process,
      preparing participants for the demands of the industry.
    &lt;/p&gt;
    &lt;h2&gt;Networking Opportunities and Professional Growth&lt;/h2&gt;
    &lt;p&gt;
      One of the most valuable aspects of the Big Dive course is the networking
      opportunities it offers. Participants build connections with industry
      professionals, course instructors, and peers, all of whom can play a
      crucial role in their professional development. The course also provides
      resources for career development, such as a CV repository and job postings
      from partner companies. Leveraging these connections on platforms like
      LinkedIn can significantly boost one’s career in data science.
    &lt;/p&gt;

    &lt;h3&gt;Practical aspect of the big dive data science bootcamp&lt;/h3&gt;
    &lt;p&gt;
      The course took place in a group of 20 students, with
      STEM/linguistic/design background, mostly from Italy but there were also a
      few foreign folks like me. Our diversity helped a lot during the group
      project when each of us could bring different perspective and skillset.
    &lt;/p&gt;

    &lt;h3&gt;How to apply&lt;/h3&gt;
    &lt;p&gt;
      In order to apply, a candidate needs to send a video explaining why he or
      she is a good fit for the big dive as well as state level of her/his
      prerequisites.
    &lt;/p&gt;

    &lt;h3&gt;Detailed content of the big dive bootcamp&lt;/h3&gt;
    &lt;p&gt;
      If you want to read more about the content of the course you can check out
      &lt;a href=&quot;http://www.bigdive.eu/&quot; target=&quot;_blank&quot;&gt;Big Dive website&lt;/a&gt; as
      well as read their great
      &lt;a
        href=&quot;https://www.linkedin.com/in/christianracca/detail/recent-activity/posts/&quot;
        target=&quot;_blank&quot;
        &gt;posts on LinkedIn&lt;/a
      &gt;
      of
      &lt;a href=&quot;https://www.linkedin.com/in/christianracca/&quot; target=&quot;_blank&quot;
        &gt;Christian Racca&lt;/a
      &gt;,
      &lt;a href=&quot;https://www.facebook.com/bigdive.eu/&quot; target=&quot;_blank&quot;
        &gt;Facebook page&lt;/a
      &gt;
      and
      &lt;a href=&quot;https://twitter.com/bigdive_eu&quot; target=&quot;_blank&quot;
        &gt;twitter @bigdive_eu&lt;/a
      &gt;.
    &lt;/p&gt;

    &lt;h3&gt;Who are the participants of the data science bootcamp ?&lt;/h3&gt;
    &lt;p&gt;
      For most of the participants and for me it was the first time with Big
      data. I found the possibility to face the problems and tools I don&apos;t
      encounter in my everyday work. The content of the course was quite densely
      packed and I will need some time after the course to practice new skills
      and read all material. Thanks to great teachers like among others
      &lt;a href=&quot;https://www.linkedin.com/in/alexcomu/&quot; target=&quot;_blank&quot;
        &gt;Alex Comunian&lt;/a
      &gt;
      and
      &lt;a href=&quot;https://www.linkedin.com/in/abusedmedia/&quot; target=&quot;_blank&quot;
        &gt;Fabio Franchino&lt;/a
      &gt;
      classes were very interactive and easy to follow.
    &lt;/p&gt;
    &lt;h3&gt;Who are the instructors of the data science bootcamp ?&lt;/h3&gt;
    &lt;p&gt;
      The instructors of the Big Dive course bring a wealth of industry
      experience to the table. Their insights into the practical applications of
      data science, the importance of domain knowledge, and the latest trends in
      the field are invaluable. Participants benefit from their interactive
      teaching style and the real-world examples they provide. The course also
      emphasizes the importance of having a field expert guide the analysis,
      ensuring that the data science work is truly impactful in its application.
    &lt;/p&gt;

    &lt;h2&gt;The Future of Data Science and Engineering&lt;/h2&gt;
    &lt;p&gt;
      As data science and engineering continue to evolve, staying updated with
      the latest trends and technologies is crucial. The Big Dive course
      prepares participants for future challenges by introducing them to
      emerging fields like deep learning and artificial intelligence.
      Understanding these trends and how they will shape the future of data
      science is essential for anyone looking to stay competitive in the
      industry.
    &lt;/p&gt;

    &lt;h3&gt;Personal highlights&lt;/h3&gt;
    &lt;p&gt;
      &lt;li&gt;
        &lt;b&gt;D3.js&lt;/b&gt; is a very powerful tool for data visualization but its
        learning curve is quite steep. Learning jquery and communication with a
        remote data server, basics of web design are necessary to create a
        serious final product. So if you don&apos;t want to spend a significant
        amount of time learning all the toolkit better go for pre-set solutions
        like RShiny if you know R for instance.
      &lt;/li&gt;
      &lt;li&gt;
        &lt;b&gt;Dask&lt;/b&gt; library seems to be an interesting alternative to Spark for
        smaller projects, the big plus is that it is using pandas vocabulary. It
        can be also misleading as we encountered quite a lot of problems
        building a set up for our data, lots of ram memory seems to be necessary
        and understanding how dask is dealing with our objects differently than
        pandas is crucial for a successful application.
      &lt;/li&gt;
      &lt;li&gt;
        &lt;b&gt;Spark&lt;/b&gt; contains important differences in algorithms from R or
        scikit-learn implementations. Therefore testing locally and comparing to
        the distributed version is not a good idea and difference in performance
        can be significant. It is important to keep it in mind from the
        beginning of the project.
      &lt;/li&gt;
      &lt;li&gt;
        &lt;b&gt;Github&lt;/b&gt; has some really cool project management features like
        issues system and wiki. Usually, it is not extensively used in academia
        or only as a code repository while for us the management part was really
        useful and handy to organize work. If you are a student you can get a
        &lt;a href=&quot;https://education.github.com/pack&quot; target=&quot;_blank&quot;
          &gt;Student Developer Pack for free&lt;/a
        &gt;.
      &lt;/li&gt;
      &lt;li&gt;
        A &lt;b&gt;field expert&lt;/b&gt; is an important part of the team. His needs should
        guide the analysis, it helps to make data scientist work really useful
        in the domain of its application
      &lt;/li&gt;
      &lt;span class=&quot;image fit&quot;
        &gt;&lt;img src=&quot;/images/dataring.png&quot; alt=&quot;&quot; /&gt;&lt;img
          src=&quot;/images/copyrights.png&quot;
          alt=&quot;&quot;
      /&gt;&lt;/span&gt;

      &lt;li&gt;
        &lt;b&gt;Data ring&lt;/b&gt; - a tool proposed by top-ix and IFC helps to plan the
        project and communicate within data science team and with the business
        partner. You can find more information about in the comprehensive
        handbook
        &lt;a
          href=&quot;http://www.ifc.org/wps/wcm/connect/22ca3a7a-4ee6-444a-858e-374d88354d97/IFC+Data+HandBook+FINAL.pdf?MOD=AJPERES&quot;
          target=&quot;_blank&quot;
          &gt;DATA ANALYTICS AND DIGITAL FINANCIAL SERVICES&lt;/a
        &gt;
      &lt;/li&gt;
      &lt;li&gt;
        In my project, I worked with e-money transaction data and I found the
        problematic pretty interesting, it encouraged me to read finance and
        economy articles.
      &lt;/li&gt;
      &lt;span class=&quot;image fit&quot;
        &gt;&lt;img src=&quot;/images/final.png&quot; alt=&quot;&quot; /&gt;&lt;em
          &gt;Image rights TOP-IX&lt;/em
        &gt;&lt;/span
      &gt;
    &lt;/p&gt;

    &lt;h2&gt;Conclusion: Personal Reflections on the Big Dive&lt;/h2&gt;
    &lt;p&gt;
      The Big Dive course offers a unique and intense learning experience that
      is both challenging and rewarding. Participants gain valuable skills,
      build professional networks, and explore the world of big data in depth.
      While it’s impossible to become an expert in big data in just five weeks,
      the course provides a strong foundation and sparks a passion for continued
      learning in data science. For anyone considering a career in data science,
      the Big Dive is an excellent starting point.
    &lt;/p&gt;
    &lt;span class=&quot;image fit&quot;
      &gt;&lt;img src=&quot;/images/bigdive.jpg&quot; alt=&quot;&quot; /&gt;&lt;em
        &gt;Image rights TOP-IX&lt;/em
      &gt;&lt;/span
    &gt;
    &lt;h2&gt;Frequently Asked Questions (FAQs)&lt;/h2&gt;
    &lt;p&gt;&lt;strong&gt;1. What is the Big Dive Course About?&lt;/strong&gt;&lt;/p&gt;
    &lt;p&gt;
      The Big Dive course is a five-week intensive program designed to equip
      participants with practical skills in big data, distributed computing, and
      data visualization. It includes hands-on projects, industry insights, and
      networking opportunities.
    &lt;/p&gt;
    &lt;p&gt;&lt;strong&gt;2. Is a Data Science Bootcamp Worth It?&lt;/strong&gt;&lt;/p&gt;
    &lt;p&gt;
      Yes, data science bootcamps like the Big Dive are worth it for those
      looking to gain practical experience and industry connections quickly.
      They offer a fast track to acquiring in-demand skills and provide a solid
      foundation for a career in data science.
    &lt;/p&gt;

    &lt;p&gt;&lt;strong&gt;3. How Can I Transition from Academia to Industry?&lt;/strong&gt;&lt;/p&gt;
    &lt;p&gt;
      The Big Dive course is an excellent way to transition from academia to
      industry. It bridges the gap by offering real-world projects, networking
      opportunities, and exposure to industry-standard tools and practices.
    &lt;/p&gt;

    &lt;p&gt;&lt;strong&gt; 4. What Tools Should I Learn for Data Science?&lt;/strong&gt;&lt;/p&gt;
    &lt;p&gt;
      Key tools include Python, Pandas, PySpark, D3.js, and AWS. Understanding
      distributed computing, data visualization, and machine learning is also
      crucial for success in data science.
    &lt;/p&gt;

    &lt;p&gt;&lt;strong&gt; 5. How Important is Data Visualization?&lt;/strong&gt;&lt;/p&gt;
    &lt;p&gt;
      Data visualization is vital for effectively communicating complex data
      insights. Tools like D3.js are powerful for creating interactive and
      dynamic visualizations that make data more accessible and actionable.
    &lt;/p&gt;

    &lt;p&gt;&lt;strong&gt; 6. Where Can I Learn More About Data Science?&lt;/strong&gt;&lt;/p&gt;
    &lt;p&gt;
      Apart from bootcamps like Big Dive, you can learn more about data science
      through online courses, books, articles, and by joining data science
      communities. Continuing education is key to staying competitive in the
      field.
    &lt;/p&gt;

    &lt;h2&gt;Resources and Further Reading&lt;/h2&gt;
    &lt;p&gt;
      Continuing education is essential in the fast-paced field of data science.
      The Big Dive course provides participants with a list of recommended
      books, articles, and websites to further their learning. Joining data
      science communities and participating in online forums are also encouraged
      as ways to stay updated and connected with the industry. Resources like
      these ensure that participants continue to grow their skills long after
      the course has ended.
    &lt;/p&gt;
    &lt;h3&gt;Useful links&lt;/h3&gt;
    &lt;p&gt;
      &lt;li&gt;
        &lt;a href=&quot;http://www.bigdive.eu/&quot; target=&quot;_blank&quot;
          &gt;&lt;b&gt;Big Dive website&lt;/b&gt;: http://www.bigdive.eu/&lt;/a
        &gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;a
          href=&quot;https://www.linkedin.com/in/christianracca/detail/recent-activity/posts/&quot;
          target=&quot;_blank&quot;
          &gt;&lt;b&gt;posts on LinkedIn&lt;/b&gt;:
          https://www.linkedin.com/in/christianracca/detail/recent-activity/posts/&lt;/a
        &gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;a href=&quot;https://www.facebook.com/bigdive.eu/&quot; target=&quot;_blank&quot;
          &gt;&lt;b&gt;Facebook page&lt;/b&gt;: https://www.facebook.com/bigdive.eu/&lt;/a
        &gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;a href=&quot;https://twitter.com/bigdive_eu&quot; target=&quot;_blank&quot;
          &gt;&lt;b&gt;twitter&lt;/b&gt;: @bigdive_eu&lt;/a
        &gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;a
          href=&quot;http://www.kdnuggets.com/2016/04/data-science-comprehensive-guide-transition.html&quot;
          target=&quot;_blank&quot;
          &gt;post that inspired me to participate in Big DIVE : From Science to
          Data Science, a Comprehensive Guide for Transition&lt;/a
        &gt;
      &lt;/li&gt;
    &lt;/p&gt;

    &lt;h3&gt;Acknowledgements&lt;/h3&gt;
    &lt;p&gt;&lt;/p&gt;
    I would like to thank Amodsen Chiota and Pierre Girard for their support for
    my application. As well as my PhD supervisor Andrei Zinovyev for allowing to
    participate in the course during my PhD time. Funding was provided by “Ecole
    Doctorale Frontières du Vivant (FdV) – Programme Bettencourt.”

    &lt;!-- Go to www.addthis.com/dashboard to customize your tools --&gt;
    &lt;script
      type=&quot;text/javascript&quot;
      src=&quot;//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-584ec4ce89deed84&quot;
    &gt;&lt;/script&gt;
    &lt;!-- Go to www.addthis.com/dashboard to customize your tools --&gt;
    &lt;div class=&quot;addthis_inline_share_toolbox&quot;&gt;&lt;/div&gt;
  &lt;/section&gt;
&lt;/html&gt;
</description>
        <pubDate>Mon, 28 Aug 2017 00:00:00 +0200</pubDate>
        <link>http://urszulaczerwinska.github.io/thoughts/bigdive/</link>
        <guid isPermaLink="true">http://urszulaczerwinska.github.io/thoughts/bigdive/</guid>
        
        
        <category>thoughts</category>
        
      </item>
    
      <item>
        <title>Hi, I am Computational Biologist</title>
        <description>&lt;html&gt;
  &lt;head&gt;
    &lt;meta
      name=&quot;description&quot;
      content=&quot;Urszula Czerwinska. About me, data science, big data, analytics, PhD | Explore career opportunities in computational biology and bioinformatics. Gain insights into the skills needed, job market trends, and tips for breaking into this dynamic field. Perfect for scientists and data professionals looking to advance their careers in computational biology.&quot;
    /&gt;
  &lt;/head&gt;

  &lt;body&gt;
    &lt;style&gt;
      .scalable {
      	overflow: hidden;
      }

      .scalable iframe {
      	height: 100%;
      	left: 0;
      	position: absolute;
      	top: 0;
      	width: 100%;
      }

      .scalable .scalable-content {
      	height: 0;
      	position: relative;
      }

      .scalable-16-9 .scalable-content {
      	/* Percentage from 9 divided by 16 for ratio of 16:9. */
      	padding-bottom: 56.25%;
      }

      iframe {display:block;margin:auto;width: 100%}
    &lt;/style&gt;
  &lt;/body&gt;
  &lt;section&gt;
    &lt;p&gt;&lt;/p&gt;
    &lt;blockquote&gt;
      Hi, I am Computational Biologist, would you hire me when I graduate (from
      PhD)?
    &lt;/blockquote&gt;

    &lt;h3&gt;Thinking about a job&lt;/h3&gt;
    &lt;p&gt;
      I am in my second year of PhD in Computational Biology / Systems Biology
      at Cancer Institute in France. It is almost halfway to graduation here,
      where PhD needs to be completed in 3-4 years, so I reckon it is a right
      time to find out personally how are my chances of getting a job in the
      &lt;b&gt;data science&lt;/b&gt; market. So I asked the question
      &lt;i
        &gt;&lt;b
          &gt;Hi, I am Computational Biologist, would you hire me when I graduate
          (from PhD)?&lt;/b
        &gt;&lt;/i
      &gt;
      in many declinations to different companies. I would like to share with
      you my experience from four &lt;strong&gt;job fairs&lt;/strong&gt;, I attended in
      November in Paris.
    &lt;/p&gt;
    &lt;p&gt;Let me start from the end...&lt;/p&gt;

    &lt;h3&gt;To make the long story short - key messages (for busy ones)&lt;/h3&gt;
    &lt;ul&gt;
      &lt;li&gt;
        &lt;b&gt;scientific writing&lt;/b&gt; and the whole communication toolbox, can also
        be a big plus, some companies release internal white papers and writing
        skills are highly valued
      &lt;/li&gt;
      &lt;li&gt;
        there is a huge need for skills of computational biologists in diverse
        projects in industry - lots of start-up with crazy ideas, challenging
        projects, there world is big out there
      &lt;/li&gt;
      &lt;li&gt;
        show you interest in a specific sector of industry, even an on-line
        course can be a plus, attend some meet-ups or events to feel the general
        ambiance and hot topics of the sector
      &lt;/li&gt;
      &lt;li&gt;
        ask questions and listen to HR advice - some of HR, especially in
        start-ups are actually working on projects that may interest you; some
        of them are new hires and they are happy to give you advice on how to
        get into the company
      &lt;/li&gt;
      &lt;li&gt;
        &lt;b&gt;hiring procedures of companies&lt;/b&gt;, even with very similar profiles
        can be very different - take time to ask about hiring procedures, go to
        &lt;a href=&quot;https://www.glassdoor.fr/index.htm&quot; target=&quot;_blank&quot;
          &gt;glassdoor&lt;/a
        &gt;, reach company employees on LinkedIn
      &lt;/li&gt;
      &lt;li&gt;
        it is fine to submit your CV to employers before your finish your
        PhD — you never know — however, make sure you start
        &lt;b&gt;applying for jobs 6 months before the end of you PhD&lt;/b&gt;
      &lt;/li&gt;
      &lt;li&gt;
        add your non-academic experience to your CV, courses related to
        business. Do not put too many academic details, i.e. faculty full names,
        professors names — unless it is someone very well known— just speak the
        language they do.
      &lt;/li&gt;
      &lt;li&gt;
        &lt;b&gt;smile&lt;/b&gt;, have positive attitude - talk about your motivation for
        transition in a positive way
      &lt;/li&gt;
      &lt;li&gt;
        be cautious about your responsibilities, some companies, especially
        start-up would hire you to do everything related with data management,
        while what they need is a team of data analyst, not one PhD graduate
      &lt;/li&gt;
    &lt;/ul&gt;

    &lt;h3&gt;The long story (for mindful ones)&lt;/h3&gt;
    &lt;p&gt;
      French educational system is complicated. To simplify, the university
      master and PhD is seen as a path exclusiv for academic career. This view
      is slowly changing and we hear more and more about companies hiring PhDs.
      Universities are also trying to promote alternative carriers organizing
      &apos;professional breakfasts&apos;, workshops, interview simulations or CV tuning
      (I have participated in some organized by
      &lt;a href=&quot;http://cfdip.uspc.fr/fr/&quot; target=&quot;_blank&quot;&gt;CFDiP&lt;/a&gt; and
      &lt;a href=&quot;http://www.adoc-tm.com/&quot; target=&quot;_blank&quot;&gt;Adoc Talent Managment&lt;/a
      &gt;). There is even a
      &lt;a
        href=&quot;http://hub11.ecolearning.eu/course/doctorat-et-poursuite-de-carriere/&quot;
        target=&quot;_blank&quot;
        &gt;MOOC&lt;/a
      &gt;
      that should help PhDs orientate their careers. Also, recently title
      &lt;i&gt;Docteur-PhD&lt;/i&gt; has been officially recognized and there will be
      created a nation network for PhD holders with a yearbook and a
      standardized e-mail address system.
    &lt;/p&gt;


    &lt;p&gt;
      Despite this, PhD graduates still face big challenges entering the world
      of work. This is why, as a PhD student, I decided to find out a bit more
      about hiring companies and their requirements by participating in four job
      fairs organized in Paris in November.
    &lt;/p&gt;

    &lt;div class=&quot;scalable scalable-16-9&quot;&gt;
      &lt;div class=&quot;scalable-content&quot;&gt;
        &lt;iframe
          src=&quot;https://www.youtube.com/embed/J6scnDFKWRI&quot;
          allowfullscreen
        &gt;&lt;/iframe&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;em&gt;Video of PhDTalent explaining value of PhDs for a company [FR]&lt;/em&gt;
    &lt;br /&gt;&lt;br /&gt;
    &lt;p&gt;
      &lt;b&gt;PhDTalent fair&lt;/b&gt;, two data science fairs:
      &lt;a href=&quot;http://www.datajob.fr/&quot; target=&quot;_blank&quot;&gt;&lt;b&gt;DATAJOB&lt;/b&gt;&lt;/a&gt; and
      &lt;a href=&quot;http://forumdatadays.fr/programme/&quot; target=&quot;_blank&quot;
        &gt;&lt;b&gt;Forum Data Paris&lt;/b&gt;&lt;/a
      &gt;
      and one organized at
      &lt;a href=&quot;http://jnum.parisdescartes.fr/job-dating/&quot; target=&quot;_blank&quot;
        &gt;&lt;b&gt;&lt;i&gt;Journée Numérique&lt;/i&gt;&lt;/b&gt;&lt;/a
      &gt;
      - &lt;i&gt;Digital Day&lt;/i&gt; at University Paris Descartes.
    &lt;/p&gt;

    &lt;blockquote class=&quot;twitter-tweet&quot; data-lang=&quot;en-gb&quot;&gt;
      &lt;p lang=&quot;fr&quot; dir=&quot;ltr&quot;&gt;
        Création d&amp;#39;un réseau des docteurs avec un annuaire annuel et une
        adresse mail : prenom.nom@phdfrance.fr
        &lt;a href=&quot;https://x.com/sup_recherche&quot;&gt;@sup_recherche&lt;/a&gt;
      &lt;/p&gt;
      &amp;mdash; Thierry Mandon (@mandonthierry)
      &lt;a href=&quot;https://x.com/mandonthierry/status/800712541634560001&quot;
        &gt;21 November 2016&lt;/a
      &gt;
    &lt;/blockquote&gt;
    &lt;script
      async
      src=&quot;//platform.x.com/widgets.js&quot;
      charset=&quot;utf-8&quot;
    &gt;&lt;/script&gt;

    &lt;p&gt;
      At any job fair, meetup or a conference when we introduce ourselves as
      Computational Biologists, it is not always clear to other people what it
      actually means.
    &lt;/p&gt;
    &lt;p&gt;
      As a &lt;b&gt;Computational Biologist&lt;/b&gt; working on data analysis in the
      context of imunno-oncology, with some additional on-line training, I
      believe I have most of basic knowledge asked for post of data analyst or
      data scientist: working in R, Machine Learning, statistics, data
      visualization, git. However, this is not clear for people working out of
      research in biology and healthcare. There are so many flavors of
      computational biologists, using different tools and applying them to
      different fields of biology, that it crucial to state clear what we
      actually do given the desired job. Computational biologist profile also
      usually comes with weak points as lack of professional experience out of
      academia, no professional network, little experience with business and
      consumers. What is sure, &lt;b&gt;most of computational biologist&lt;/b&gt;, with some
      additional training to broad their technical horizons and with open mind,
      &lt;b&gt;have very attractive profile for many industries&lt;/b&gt;.
    &lt;/p&gt;

    &lt;p&gt;
      I have spend around 1-2 h of my time at each job fair, that took place
      during the week office hours. Even though, I didn&apos;t manage to talk to all
      companies, especially the most popular, where the line were very long, I
      managed to leave my CV to 15 companies and talk to around 20. Here is a
      little summary of each job fair.
    &lt;/p&gt;
    &lt;p&gt;
      &lt;span class=&quot;image right&quot;
        &gt;&lt;img src=&quot;/images/logo_PhDTalent.png&quot; alt=&quot;&quot;
      /&gt;&lt;/span&gt;
    &lt;/p&gt;

    &lt;h3&gt;
      &lt;a href=&quot;https://www.phdtalent.org/&quot; target=&quot;_blank&quot;&gt;PhDTalent fair&lt;/a&gt;
    &lt;/h3&gt;
    &lt;p&gt;
      PhDTalent attracted more than 70 companies form different industries.
      These could be roughly split into three groups: Biotech / Pharma,
      consulting, Policy, Robotics &amp; Data science &amp; Big data &amp; IT &amp; Computing.
      There were presented a wide variety of different job offers. Some like
      Orange or L&apos;Oréal recruit for their PostDocoral programs. During the day,
      I have learned that some companies, during the
      &lt;b&gt;recruitment process&lt;/b&gt; can send applicants a
      &lt;b&gt;weekly challenge&lt;/b&gt; or test through up to &lt;b&gt;7 interviews&lt;/b&gt; for a
      R&amp;D position. In addition, I also spoke to various start-ups that generate
      diverse data; they need people to manage them and sometimes they do hope
      one PhD can replace data management team. I also encountered even very
      surprising job descriptions: one start-up in particular was looking to
      hire
      &lt;b
        &gt;bioinformatician to model hormone detection in the air by their
        humanoid robot&lt;/b
      &gt;. I am not sure if they correctly interpreted what the skills of a
      bioinformatician are ;) (For those who are not sure what bioinformatician
      does have a look at the article in
      &lt;a
        href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4408859/&quot;
        target=&quot;_blank&quot;
        &gt;Frontiers of Genetics: Who qualifies to be bioinformatician?&lt;/a
      &gt;)
    &lt;/p&gt;
    &lt;p&gt;
      &lt;span class=&quot;image left&quot;
        &gt;&lt;img src=&quot;/images/DATAJOBS.jpg&quot; alt=&quot;&quot;
      /&gt;&lt;/span&gt;
    &lt;/p&gt;
    &lt;p&gt;
      &lt;span class=&quot;image right&quot;
        &gt;&lt;img src=&quot;/images/forumdata.jpg&quot; alt=&quot;&quot;
      /&gt;&lt;/span&gt;
    &lt;/p&gt;

    &lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;em
      &gt;Data Jobs forum at Bourse de Paris. Image credit to
      &lt;a href=&quot;http://www.datajob.fr/&quot; target=&quot;_blank&quot;&gt;datajobs.fr&lt;/a&gt;&lt;/em
    &gt;&lt;br /&gt;&lt;br /&gt;
    &lt;h3&gt;
      &lt;a href=&quot;http://forumdatadays.fr/programme/&quot; target=&quot;_blank&quot;
        &gt;Forum Data&lt;/a
      &gt;
      and &lt;a href=&quot;http://www.datajob.fr/&quot; target=&quot;_blank&quot;&gt;Data Jobs&lt;/a&gt;
    &lt;/h3&gt;
    &lt;p&gt;
      Those two job fairs had splendid sponsors and a wide array of known
      companies present. Even though, it was not designed solely for PhD
      graduates I found companies representatives very enthousiastic and helpful
      - perhaps reflecting vigor for the potential that the data industry has to
      offer. Key things that I took home from these events were that, of course,
      it is too early now for me to apply being mid-way through my PhD. However
      they recommended looking at role descriptions on company websites and
      keeping an eye out for events to get involved with. For example, some
      firms organize their own meetups where you have the opportunity to meet
      employees or work on mini projects for the company. I was also advised to
      attend
      &lt;a href=&quot;https://startupweekend.org/locations/europe/fr &quot; target=&quot;_blank&quot;
        &gt;&lt;b&gt;startup weekends&lt;/b&gt;&lt;/a
      &gt;
      where many startup present at those and similar events hire actively. When
      trying to gather information about data analysis positions in an
      established beauty company with big R&amp;D sector, I was surprised to
      discover that they &lt;b&gt;only&lt;/b&gt; hire PhD graduates with at least three year
      experience working in company (whilst hiring engineers straight away). In
      contrast, Other companies were also looking for three years experience.
      However in practice, if companies are looking for people and you interview
      well, you have every chance of being hired. &lt;br /&gt;Therefore, I buried my
      hopes to stay close to biology an healthcare, and analyze biological data.
      Probably for a first job, one needs to be more flexible than that.
    &lt;/p&gt;

    &lt;h3&gt;
      &lt;a href=&quot;http://jnum.parisdescartes.fr/job-dating/&quot; target=&quot;_blank&quot;
        &gt;Journées numériques&lt;/a
      &gt;
    &lt;/h3&gt;
    &lt;p&gt;
      Among numerous activities of &lt;i&gt;Digital Day&lt;/i&gt; there were a little job
      fair organized. A few number of companies presented job offers. However,
      the &lt;i&gt;job offers in digital expertise&lt;/i&gt; was a bit misleading and most
      of jobs proposed were related to advanced computing, IT management, web
      marketing and communication.
    &lt;/p&gt;

    &lt;h3&gt;How to choose a company?&lt;/h3&gt;
    &lt;p&gt;
      &lt;span class=&quot;image fit&quot;
        &gt;&lt;img src=&quot;/images/office.jpg&quot; alt=&quot;&quot;
      /&gt;&lt;/span&gt;
    &lt;/p&gt;

    &lt;p&gt;
      It is very important issue. Let&apos;s imagine, you have a great profile,
      desired by many companies. How do you chose the company?
    &lt;/p&gt;
    &lt;p&gt;
      As a foreigner in France and academia-trained I do not have clear idea
      about model of functioning of different companies and possible advantage
      and disadvantages of specific setups.
    &lt;/p&gt;

    &lt;p&gt;
      I encountered many consulting offices, where employees work actually for
      different companies to solve an issue or work on a project. I was told,
      their offer can seem very attractive at the first sight: variety of
      subjects, cutting edge technologies, big companies. However, they also are
      some drawbacks, as employees are hired by the consulting company, they are
      lured by the most attractive projects, but there is no guarantee to which
      one they will be associated because it depends a lot on client choice, and
      other available consultants. On the other hand, even a short contract in a
      company developing new technologies such as &lt;i&gt;Orange&lt;/i&gt; or
      &lt;i&gt;Microsoft&lt;/i&gt; can be very good for your career, while consulting job
      can be easier to get but will not always meet your expectations.
    &lt;/p&gt;

    &lt;h3&gt;Closing remarks&lt;/h3&gt;
    &lt;p&gt;
      Career transition is probably not easy, future is not all roses, but with
      some determination, I am pretty sure a
      &lt;b
        &gt;Camputational Biologist has great chances to find a well-paid job
        outside academia&lt;/b
      &gt;. Job market is very competitive and demands are high. Luckily for us,
      programming skills combined with capacity for analysis seem to be in high
      demand. There will be still some knowledge gaps, some unknowns, but our
      duty is to turn them into challenges.
    &lt;/p&gt;
    &lt;p&gt;
      &lt;span class=&quot;image fit&quot;
        &gt;&lt;img src=&quot;/images/horizon.jpg&quot; alt=&quot;&quot;
      /&gt;&lt;/span&gt;
    &lt;/p&gt;
    &lt;!-- Go to www.addthis.com/dashboard to customize your tools --&gt;
    &lt;script
      type=&quot;text/javascript&quot;
      src=&quot;//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-584ec4ce89deed84&quot;
    &gt;&lt;/script&gt;
    &lt;!-- Go to www.addthis.com/dashboard to customize your tools --&gt;
    &lt;div class=&quot;addthis_inline_share_toolbox&quot;&gt;&lt;/div&gt;
  &lt;/section&gt;
&lt;/html&gt;
</description>
        <pubDate>Mon, 30 Jan 2017 00:00:00 +0100</pubDate>
        <link>http://urszulaczerwinska.github.io/thoughts/jobsCB/</link>
        <guid isPermaLink="true">http://urszulaczerwinska.github.io/thoughts/jobsCB/</guid>
        
        
        <category>thoughts</category>
        
      </item>
    
      <item>
        <title>From Science to Data Science</title>
        <description>&lt;!DOCTYPE HTML&gt;
&lt;html lang=&quot;en-US&quot;&gt;
    &lt;head&gt;
        &lt;meta charset=&quot;UTF-8&quot;&gt;
        &lt;meta http-equiv=&quot;refresh&quot; content=&quot;1; url=&quot;&gt;
        &lt;script type=&quot;text/javascript&quot;&gt;
            window.location.href = &quot;https://urszulaczerwinska.github.io/FromSciencetoDataScience/&quot;
        &lt;/script&gt;
        &lt;title&gt;Page Redirection&lt;/title&gt;
    &lt;/head&gt;
    &lt;body&gt;
        &lt;!-- Note: don&apos;t tell people to `click` the link, just tell them that it is a link. --&gt;
        If you are not redirected automatically, follow this &lt;a href=&apos;https://urszulaczerwinska.github.io/FromSciencetoDataScience/&apos;&gt;link to example&lt;/a&gt;.
    &lt;/body&gt;
&lt;/html&gt;</description>
        <pubDate>Mon, 30 Jan 2017 00:00:00 +0100</pubDate>
        <link>http://urszulaczerwinska.github.io/works/FromSciencetoDataScience</link>
        <guid isPermaLink="true">http://urszulaczerwinska.github.io/works/FromSciencetoDataScience</guid>
        
        <category>Machine Learning</category>
        
        <category>Careers</category>
        
        <category>Data Science</category>
        
        <category>Data viz</category>
        
        
        <category>works</category>
        
        <category>FSctoDs</category>
        
      </item>
    
  </channel>
</rss>
