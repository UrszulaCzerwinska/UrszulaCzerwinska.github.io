<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Urszula Czerwinska | Data Scientist &amp; Deep Learning Engineer</title>
    <description>Explore the journey of Urszula Czerwinska from PhD to Data Science, featuring insights on Data Science projects, Machine Learning, and Deep Learning. Discover how to become a Data Scientist or Machine Learning Engineer.</description>
    <link>http://urszulaczerwinska.github.io/</link>
    <atom:link href="http://urszulaczerwinska.github.io/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Mon, 02 Sep 2024 22:07:44 +0200</pubDate>
    <lastBuildDate>Mon, 02 Sep 2024 22:07:44 +0200</lastBuildDate>
    <generator>Jekyll v4.3.3</generator>
    
      <item>
        <title>The Mamba Effect</title>
        <description>&lt;head&gt;
  &lt;meta name=&quot;description&quot; content=&quot;Unlock the potential of AI with our comprehensive guide on ML Mamba models. Discover how these advanced machine learning frameworks are revolutionizing data analysis, predictive analytics, and automated decision-making. Learn about their key features, benefits, and applications across various industries. Enhance your understanding and leverage ML Mamba models to stay ahead in the rapidly evolving world of artificial intelligence.&quot; /&gt;
&lt;/head&gt;

&lt;p&gt;Already 8 papers since December 2023 !&lt;/p&gt;

&lt;p&gt;Mamba is already a new wave starting as a replacement for vanilla transformer, it has already been adapted to text, vison, video…&lt;/p&gt;

&lt;p&gt;Mamba models represent a significant breakthrough in neural network architecture.&lt;/p&gt;

&lt;p&gt;Among published papers, besides the original Mamba paper, I would like to distinguish two spin-offs: VMamba and MambaBytes. There it also a bunch of papers with specific biomedical applications that I am not expert to evaluate impact.&lt;/p&gt;

&lt;p&gt;Here I compiled a short overview of all (?) those papers adapting the template from &lt;a href=&quot;https://www.deeplearning.ai/the-batch/&quot;&gt;The Batch newsletter from deeplearning.ai&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;here is a list of mamba papers : &lt;a href=&quot;https://github.com/yyyujintang/Awesome-Mamba-Papers&quot;&gt;https://github.com/yyyujintang/Awesome-Mamba-Papers&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;arxiv-231201-mamba-linear-time-sequence-modeling-with-selective-state-spaces&quot;&gt;Arxiv 23.12.01: Mamba: Linear-Time Sequence Modeling with Selective State Spaces&lt;/h2&gt;

&lt;p&gt;#mamba&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2312.00752&quot;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/state-spaces/mamba&quot;&gt;Code&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This paper is the first to introduce the Mamba architecture. Mamba offers faster inference, linear scaling with sequence length, and strong performance.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*aPbcg2rPGh68SViRpTi21Q.png&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*bAj0TMSgZetUghDij6U-zA.png&quot; alt=&quot;&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What’s New&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A new architecture, Mamba, integrating SSMs without relying on attention or MLP blocks.&lt;/li&gt;
  &lt;li&gt;Implementation of a hardware-aware parallel algorithm for efficient computation.&lt;/li&gt;
  &lt;li&gt;Long context: the quality and efficiency together yield performance improvements on real data up to sequence length 1M&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;How it Works:&lt;/strong&gt; Mamba adds recurrent and convolutional models, to &lt;strong&gt;a unique selection mechanism&lt;/strong&gt; that enables the model to prioritize or ignore inputs based on the content relevance. This approach allows for &lt;strong&gt;linear scalability&lt;/strong&gt; in sequence length. Mamba integrates selective SSMs into a simplified neural network architecture with gates. They are structured to enable the model to selectively propagate or to forget information based on the current token.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Results:&lt;/strong&gt; Mamba demonstrates state-of-the-art performance across various modalities. In language modeling, &lt;strong&gt;Mamba-3B outperforms Transformers&lt;/strong&gt; of the same size, matches Transformers twice its size in both pretraining and downstream evaluation. In terms of efficiency, it achieves &lt;strong&gt;5× higher throughput than Transformers&lt;/strong&gt; and scales linearly with sequence length.
This in various domains such as language, genomics, audio modeling. It efficiently handles &lt;strong&gt;sequences up to a million lengths&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Behind the News:&lt;/strong&gt; The development of Mamba marks a significant &lt;strong&gt;shift from the dominant Transformer-based architectures&lt;/strong&gt;. It opens new avenues in sequence modeling, especially for applications requiring efficient processing of long data sequences. Authors mentions their ambition to make Mamba alternative to Tranformers and CNN as a &lt;a href=&quot;https://medium.com/adevinta-tech-blog/foundation-models-a-new-vision-for-e-commerce-76904a3066e8&quot;&gt;Foundation Model&lt;/a&gt; backbone.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Why it Matters:&lt;/strong&gt; Mamba has the potential to revolutionize various applications in deep learning. This first paper already resulted in 7 other papers in the same month as code is under apache licene and public.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;We’re Thinking:&lt;/strong&gt; We are missing distance to see how Mamba module will perform in practice. However the fact that many publicaitons already applied Mamba, modified it and obtained pulishable resutls is promising.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;In Detail:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Selective SSMs&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/2560/1*j4g2N5BtJUvjEsXqfVPmJg.png&quot; alt=&quot;&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;The key contribution of the paper lies in the novel implementation of &lt;strong&gt;Selective&lt;/strong&gt; State Space Models. It leverages parameters that control if the model response to current inputs or it maintains its existing state. For instance, a parameter ∆ (Delta) in the model’s architecture determines the balance between focusing on the current input (larger ∆ values) and preserving the ongoing state (smaller ∆ values). The selective modulation of parameters B and C tunes how the inputs influence the state and, conversely, how the state influences the outputs. The selective approach also manages context and resets boundaries in scenarios where sequences are concatenated. This prevents the unwanted bleed of information between concatenated sequences.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Other ressources:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/ai-insights-cobet/building-mamba-from-scratch-a-comprehensive-code-walkthrough-5db040c28049&quot;&gt;building-mamba-from-scratch-a-comprehensive-code-walkthrough&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/@joeajiteshvarun/mamba-revolutionizing-sequence-modeling-with-selective-state-spaces-8a691319b34b&quot;&gt;mamba-revolutionizing-sequence-modeling-with-selective-state-spaces&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/@athekunal/mamba-and-state-space-models-explained-b1bf3cb3bb77&quot;&gt;mamba-and-state-space-models-explained&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;arxiv-240108-moe-mamba-efficient-selective-state-space-models-with-mixture-of-experts&quot;&gt;Arxiv 24.01.08: &lt;strong&gt;MoE-Mamba: Efficient Selective State Space Models with Mixture of Experts&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;#LLM&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2401.04081&quot;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The paper introduces a novel model in the field of sequential modeling.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*wCH-77Q_ThqTubu4H-Sl9w.png&quot; alt=&quot;&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Introduction:&lt;/strong&gt; The paper presents MoE-Mamba, a model that integrates State Space Models (SSMs) with Mixture of Experts (MoE) to enhance sequential modeling. This combination aims to leverage the strengths of both SSMs, known for their efficient performance, and MoE, a technique for scaling up models efficiently.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What’s New:&lt;/strong&gt; Integration of SSMs and MoE. MoE layers are used by Mistral, one of SOTA LLM models.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How it Works:&lt;/strong&gt; The MoE-Mamba architecture replaces every other Mamba layer with a MoE feed-forward layer.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;
- MoE-Mamba achieves better performance than both Mamba and Transformer-MoE models.
- It reaches the same performance as Mamba in significantly fewer training steps.
- The model scales well with the number of experts, with optimal results at 32 experts.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Behind the News:&lt;/strong&gt; The scalability potential of MoE-Mamba is remarkable. Although the current study focuses on smaller models, the architecture suggests a promising avenue for handling larger, more complex models efficiently.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Why it Matters:&lt;/strong&gt; Given the efficiency in training and inference, MoE-Mamba shows promise for deployment in large-scale language models.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;We’re Thinking:&lt;/strong&gt; It is also not clear what are the scalability limits of MoE-Mamba, especially in comparison to existing large-scale models like GPT-3? Also, we would like to have a look a the code…&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;In Detail&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Integration of MoE layers into the Mamba architecture&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This design choice enables MoE-Mamba to leverage the conditional processing capabilities of MoE and the context integration of Mamba. By alternating between unconditional processing by the Mamba layer and conditional processing by a MoE layer, MoE-Mamba achieves a balance between efficient state compression and selective information retention.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*hsiJ04bKp4QspHpyFmrApw.png&quot; alt=&quot;&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Authors also investigate a unified Mamba module containg MoE&lt;/p&gt;

&lt;h2 id=&quot;arxiv-240109-u-mamba-enhancing-long-range-dependency-for-biomedical-image-segmentation&quot;&gt;Arxiv 24.01.09: &lt;strong&gt;U-Mamba: Enhancing Long-range Dependency for Biomedical Image Segmentation&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2401.04722&quot;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/bowang-lab/U-Mamba&quot;&gt;Code&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;#biomedical #segmentation #cv&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*RM4DAO5rm5n7kE79iWtw_w.png&quot; alt=&quot;&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;This paper presents an innovative network architecture for biomedical image segmentation.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What’s new:&lt;/strong&gt; U-Mamba is a novel network integrating Mamba blocks, into a U-Net based architecture. This hybrid CNN-SSM structure enables modeling of long-range dependencies in images.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How it works:&lt;/strong&gt; U-Mamba follows an encoder-decoder structure, where each building block comprises Residual blocks followed by a Mamba block. This design captures both local features and long-range dependencies. The network’s self-configuring mechanism allows it to adapt automatically to various datasets.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Results:&lt;/strong&gt; The experiments across biomedical segmentation tasks show that U-Mamba outperforms the state-of-the-art CNN/Transformer-based networks in terms of segmentation accuracy by a small margin.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Behind the news:&lt;/strong&gt; The integration of Mamba blocks within a U-Net architecture represents an interesting integration of two architectures, highlighting the potential of State Space Models in this domain.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Why it matters:&lt;/strong&gt; Yet one more architecture smoothly integrating Mamba with good results.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;We’re thinking:&lt;/strong&gt; The U-Mamba network’s adaptability and performance set a new benchmark in medical image segmentation. This work might stimulate further exploration of hybrid architectures in medical image analysis.&lt;/p&gt;

&lt;p&gt;While U-Mamba shows some advantages over existing methods, the improvement margin in some cases appears modest. For example, in 3D organ segmentation, U-Mamba’s DSC scores are marginally higher than nnU-Net, which is one of the closest competitors.&lt;/p&gt;

&lt;p&gt;Authors do not support with numbers efficiency of the model, model size and comparison in that matter of the two architecture variants.&lt;/p&gt;

&lt;p&gt;Should we wait for U-Mamba perfomance for standart segmentation benchmarks such as &lt;a href=&quot;https://paperswithcode.com/sota/semantic-segmentation-on-ade20k&quot;&gt;ADE20K&lt;/a&gt; or PASCAL to decide U-Mamba generic value?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;In Detail&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;U-Mamba Architecture&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The U-Mamba architecture follows the encoder-decoder pattern of U-Net, known for its effectiveness in medical imaging.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Building Blocks:&lt;/strong&gt; Each block contains two successive Residual blocks followed by a Mamba block. The Residual block includes a plain convolutional layer, Instance Normalization, and Leaky ReLU.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Mamba Block:&lt;/strong&gt; It processes image features that are flattened and transposed, followed by Layer Normalization. The Mamba block has two parallel branches:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;The first branch&lt;/strong&gt; expands the features and processes them through a linear layer, a 1D convolutional layer, SiLU activation, and the SSM layer.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;The second branch&lt;/strong&gt; also expands the features, followed by SiLU activation. Features from both branches are then merged using the Hadamard product and projected back to their original shape.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Encoder and Decoder:&lt;/strong&gt; The U-Mamba encoder, captures both local features and long-range dependencies. The decoder focuses on local information and resolution recovery, using Residual blocks, transposed convolutions, and inherits skip connections from U-Net. The output is passed through a convolutional layer and a Softmax layer for the final segmentation probability map.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Variants:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;U-Mamba_Bot: Uses the U-Mamba block only in the bottleneck.&lt;/li&gt;
  &lt;li&gt;U-Mamba_Enc: Employs the U-Mamba block in all encoder blocks.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;arxiv-240117-vision-mamba-efficient-visual-representation-learning-with-bidirectional-state-space-model&quot;&gt;Arxiv 24.01.17: &lt;strong&gt;Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2401.09417``&quot;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/hustvl/Vim&quot;&gt;Code&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;#computervision #classification #segmentation&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*D6KVLVs8MJYnd_DBCDUKuw.png&quot; alt=&quot;&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Vim utilizes bidirectional State Space Models (SSMs) to process image sequences.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What’s New&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Vim proposes a pure-SSM-based method for vision tasks, differing from self-attention-based models.&lt;/li&gt;
  &lt;li&gt;Incorporation of &lt;strong&gt;bidirectional SSMs&lt;/strong&gt; for efficient visual data processing.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;How It Works&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Vim transforms images into sequences of flattened 2-D patches, applying bidirectional SSM.&lt;/li&gt;
  &lt;li&gt;The system uses a combination of position embeddings and bidirectional state space models for visual data processing.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;On tasks like ImageNet classification Vim outperforms established models like DeiT and ViTs in terms of accuracy with smaller size.&lt;/li&gt;
  &lt;li&gt;Authors compare also segmentation UperNet framework with Vim, DeiT and ResNet on ADE20k with similar conclusion.&lt;/li&gt;
  &lt;li&gt;For object detection benchmark Vim slighly outperforms DeiT in the scope of Cascade Mask R-CNN on COCO.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Behind the News:&lt;/strong&gt; Vim challenges the dominance of self-attention in visual representation, offering an alternative that’s more efficient in handling large-scale and high-resolution datasets.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Why It Matters:&lt;/strong&gt; Vim’s efficiency in processing high-resolution images makes it a promising backbone for future vision foundation models.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;We’re Thinking:&lt;/strong&gt; Will Vim show its efficiency/accuracy across other frameworks than UperNet or Cascade Mask R-CNN ?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;In Detail&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Bidirectional State Space Model&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;It processes the visual data both forward and backward, unlike traditional unidirectional models.&lt;/li&gt;
  &lt;li&gt;This bidirectionality allows for more robust capturing of visual contexts and dependencies, particularly in dense prediction tasks.&lt;/li&gt;
  &lt;li&gt;The model effectively compresses the visual representation, leveraging position embeddings to maintain spatial awareness.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;arxiv-240118-vmamba-visual-state-space-model&quot;&gt;Arxiv 24.01.18 : &lt;strong&gt;VMamba: Visual State Space Model&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2401.10166&quot;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/MzeroMiko/VMamba&quot;&gt;Code&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;#computervision #classification #segmentation #detection&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;VMamba&lt;/strong&gt; presents new approach in visual representation learning. It merges CNNs’ and ViTs’ strengths and does not have their limitations.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What’s new:&lt;/strong&gt; VMamba has a unique architecture: integrates global receptive fields and dynamic weights within a linear computational complexity framework.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How it works:&lt;/strong&gt; The model is based on a &lt;strong&gt;Cross-Scan Module (CSM)&lt;/strong&gt;. This module processes visual data in &lt;strong&gt;four directions&lt;/strong&gt;. This ensures global information integration without increased complexity. The 2D Selective Scan combines CMS with S6 mamba block and merge output features creating 2D feature map.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Results:&lt;/strong&gt; VMamba shows significant performance improvements in different task : image classification, object detection, and semantic segmentation. For instance, it surpasses established benchmarks like ResNet and Swin in ImageNet-1K classification. In COCO object detection, VMamba models outperform their counterparts in mean Average Precision (mAP) and mean Intersection over Union (mIoU).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Behind the news:&lt;/strong&gt; VMamba maintains high performance across various input image sizes, which can indicate a robustness to changes in input conditions. This feature is crucial for practical applications where image resolutions can vary significantly.&lt;/p&gt;

&lt;p&gt;It dethrones Vim just the next day of Vim publication.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Why it matters:&lt;/strong&gt; VMamba’s approach combines the strengths of CNNs and ViTs and on the top it is computationally efficient.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*F7uFR9RPyq4bDvrO4a3Ycg.png&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*yK0hfDpKYFnGk0NupiBHXg.png&quot; alt=&quot;&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;We’re thinking:&lt;/strong&gt; While VMamba’s results are promising, its practical applicability in diverse real-world scenarios require time verification. For data scientists, VMamba holds a promise of efficient processing of large-scale image datasets and a new playground.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;In Detail&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;VMamba’s 2D Selective Scan&lt;/strong&gt; operates by dynamically adjusting weights based on the importance of different areas in an image. This process involves an algorithm that assesses each pixel’s contribution to the target task. The scan prioritizes regions with higher information content, therefore it reduces computational load on less relevant areas. This method contrasts with traditional approaches where all pixels are treated equally. Which also leads to higher computational costs.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/1200/1*Ajt_aoa_ElU82EEXM63n2g.jpeg&quot; alt=&quot;&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Vim vs VMamba&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;VMamba beats Vim on ImageNet-1k benchmark&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;VMamba-T with 22M params achieves 82.2 % acc while comparable VimS with 26M achieves 80.3% acc&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;VMamba beats Vim on COCO benchmark&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;the smalles model VMamba-T achieves APbox of 46.5 and APmask of 42.1 while Vim-T achives 45.7 and 39.2 respecitively&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;VMamba beats Vim on ADE20k benchmark&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;numbers are not directly comparable but Vmamaba seems to have better perfomance&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;arxiv-240124-segmamba-long-range-sequential-modeling-mamba-for-3d-medical-image-segmentation&quot;&gt;Arxiv 24.01.24 : &lt;strong&gt;SegMamba: Long-range Sequential Modeling Mamba For 3D Medical Image Segmentation&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2401.13560&quot;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/ge-xing/SegMamba&quot;&gt;Code&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;#segmentation #computervision&lt;/p&gt;

&lt;p&gt;This paper integrates the Mamba model with a U-shape structure for 3D medical image segmentation, aiming to efficiently process high-dimensional images.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What’s New&lt;/strong&gt;: SegMamba’s combines the Mamba model, known for handling long-range dependencies, with a U-shaped architecture, with application in 3D medical image segmentation.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How it Works&lt;/strong&gt;: SegMamba employs a Mamba encoder, a 3D decoder, and skip-connections. The Mamba encoder, with depth-wise convolution and flattening operations, reduces computational load while handling high-dimensional features.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;: On the BraTS2023 dataset, SegMamba achieved best performance.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Why it Matters:&lt;/strong&gt; SegMamba’s is quite niche but confims again universality of Mamba block.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;We’re Thinking:&lt;/em&gt; Questions arise about SegMamba’s applicability to various medical imaging forms and datasets, and its comparison with other state-of-the-art methods.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;In Detail&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;SegMamba vs Umamba&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The key difference is in their approach to enhancing the U-Net architecture. SegMamba emphasizes the Mamba model for 3D segmentation, while U-Mamba combines CNNs with SSMs for versatile segmentation tasks.&lt;/p&gt;

&lt;h2 id=&quot;arxiv-240124-mambabyte-token-free-selective-state-space-model&quot;&gt;Arxiv 24.01.24: &lt;strong&gt;MambaByte: Token-free Selective State Space Model&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2401.13660&quot;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/kyegomez/MambaByte&quot;&gt;Code&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;#llm #tokenfree&lt;/p&gt;

&lt;p&gt;This model is token-free, directly learning from raw bytes and bypassing the biases inherent in subword tokenization.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What’s New&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;“MambaByte” is a token-free language model, a novel approach as it directly learns from raw bytes, no tokenization.&lt;/li&gt;
  &lt;li&gt;A unique perspective on efficiency and performance compared to other byte-level models.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;How It Works:&lt;/strong&gt; “MambaByte” slightly modfies Mamba module to accept raw bytes.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Results:&lt;/strong&gt; The model surpasses the performance of state-of-the-art subword Transformers with lower computational resources. It demonstrates linear scaling in sequence length, leading to faster inference times.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Behind the News:&lt;/strong&gt; “MambaByte” proposes an alternative to autoregressive Transformers, libertaing us from tokenization.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Why It Matters:&lt;/strong&gt; This efficient byte sequence processing opens new avenues for language models in large-scale and diverse applications.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;We’re Thinking:&lt;/strong&gt; This paper shows a potential for token free llm learning. New applications outside of LLM should come soon.&lt;/p&gt;

&lt;h2 id=&quot;arxiv-240125-vivim-a-video-vision-mamba-for-medical-video-object-segmentation&quot;&gt;Arxiv 24.01.25 &lt;strong&gt;Vivim: a Video Vision Mamba for Medical Video Object Segmentation&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2401.14168&quot;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/scott-yjyang/Vivim&quot;&gt;Code&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;#video #biomedical&lt;/p&gt;

&lt;p&gt;A framework for medical video object segmentation, focusing on addressing challenges in long-sequence modeling in video analysis. It uses a Temporal Mamba Block, which allows the model to obtain excellent segmentation results with improved speed performance compared to existing methods.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*uqxFkqdGvoi3vf7opR57mQ.png&quot; alt=&quot;&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What’s New:&lt;/strong&gt; Vivim integrates the Mamba model into a multi-level transformer architecture, transforming video clips into feature sequences containing spatiotemporal information at various scales.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How It Works&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The Temporal Mamba Block employs a sequence reduction process for efficiency, integrating a spatial self-attention module and a Mix-FeedForward layer.&lt;/li&gt;
  &lt;li&gt;The Mamba module explores correlations among patches of input frames, while a Detail-specific FeedForward preserves fine-grained details.&lt;/li&gt;
  &lt;li&gt;A lightweight CNN- based decoder head integrates multi-level feature sequences to predict segmentation masks.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Results:&lt;/strong&gt; Vivim demonstrates superior performance on the breast US dataset outperforming existing video- and image-based segmentation methods.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Why It Matters:&lt;/strong&gt; Vivim’s approach represents a significant advancement in medical video analysis, particularly for tasks like lesion segmentation in ultrasound videos.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;We’re Thinking:&lt;/strong&gt; What are the potential limitations or challenges in scaling Vivim for broader clinical applications? The paper focuses solely on breast ultrasound videos, which may limit generalizability.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;In Detail:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Temporal Mamba Block&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;This block starts with a spatial self-attention module for extracting spatial features, followed by a Mix-FeedForward layer.&lt;/li&gt;
  &lt;li&gt;For temporal modeling, it transposes and flattens the spatiotemporal feature embedding into a 1D long sequence.&lt;/li&gt;
  &lt;li&gt;The Mamba module within the block tackles the correlation among patches in input frames, while the Detail-specific FeedForward focuses on preserving fine-grained details through depth-wise convolution.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;arxiv-240125-mambamorph-a-mamba-based-backbone-with-contrastive-feature-learning-for-deformable-mr-ct-registration&quot;&gt;Arxiv 24.01.25: &lt;strong&gt;MambaMorph: a Mamba-based Backbone with Contrastive Feature Learning for Deformable MR-CT Registration&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2401.13934.pdf&quot;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/guo-stone/mambamorph&quot;&gt;Code&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;#biomedical&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*QNfM9VLXeYFlpF57yU69Ag.png&quot; alt=&quot;&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;A multi-modality deformable registration network designed specifically for aligning Magnetic Resonance (MR) and Computed Tomography (CT) images.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What’s New:&lt;/strong&gt; MambaMorph combines Mamba blocks with a feature extractor.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How It Works:&lt;/strong&gt; MambaMorph integrates a Mamba-based backbone with contrastive feature learning for deformable MR-CT registration.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Mamba-Based Registration Module: This module utilizes the Mamba blocks for efficient handling and processing of high-dimensional imaging data.&lt;/li&gt;
  &lt;li&gt;Contrastive Feature Learning: a feature extractor that employs supervised contrastive learning. This is designed to learn fine-grained, modality-specific features from the MR and CT images.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;: MambaMorph demonstrates superior performance over existing methods in MR-CT registration, showing improvements in accuracy and efficiency.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Behind the News:&lt;/strong&gt; The development of MambaMorph is a significant step in addressing the prevalent issues in multi-modality image registration, particularly in the context of MR and CT images.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Why It Matters:&lt;/strong&gt; The success of MambaMorph in MR-CT image registration has significant implications for medical imaging analysis.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;We’re Thinking:&lt;/strong&gt; While MambaMorph shows promising results, questions remain about its applicability to other forms of medical imaging and its performance in varied clinical scenarios.&lt;/p&gt;

&lt;footer&gt;
  &lt;p&gt;Exported from &lt;a href=&quot;https://medium.com&quot;&gt;Medium&lt;/a&gt; on Feb 03
    2024.&lt;/p&gt;
  &lt;p&gt;&lt;a href=&quot;https://medium.com/@ulalaparis/the-mamba-effect-mamba-models-gaining-ground-f2d2c9b9245c&quot;&gt;View
      the original&lt;/a&gt;&lt;/p&gt;
&lt;/footer&gt;
&lt;script type=&quot;text/javascript&quot; src=&quot;//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-584ec4ce89deed84&quot;&gt;&lt;/script&gt;

&lt;div class=&quot;addthis_inline_share_toolbox&quot;&gt;&lt;/div&gt;

</description>
        <pubDate>Sat, 03 Feb 2024 00:00:00 +0100</pubDate>
        <link>http://urszulaczerwinska.github.io/thoughts/the-mamba-effect-mamba-models-gaining-ground/</link>
        <guid isPermaLink="true">http://urszulaczerwinska.github.io/thoughts/the-mamba-effect-mamba-models-gaining-ground/</guid>
        
        
        <category>thoughts</category>
        
      </item>
    
      <item>
        <title>AI Foundation Models</title>
        <description>&lt;head&gt;
  &lt;meta name=&quot;description&quot; content=&quot;Discover how AI foundation models are revolutionizing e-commerce by enhancing product development, driving sustainability, and fostering collaboration. Explore the challenges and strategies for successful implementation.&quot; /&gt;
&lt;/head&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;&lt;em&gt;“Foundation models are models that are trained on broad data and can be adapted to a wide range of downstream tasks. (…) In choosing this term, we take ‘foundation’ to designate the function of these models: a foundation is built first and it alone is fundamentally unfinished, requiring (possibly substantial) subsequent building to be useful. ‘Foundation’ also conveys the gravity of building durable, robust, and reliable bedrock through deliberate and judicious action.”&lt;/em&gt;&lt;/strong&gt;
 — the Stanford Institute for Human-Centred AI founded the &lt;a href=&quot;https://arxiv.org/pdf/2108.07258.pdf&quot;&gt;Center for Research on Foundation Models&lt;/a&gt; (CRFM), 2021&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*PDC2WWQyZPpIcKZU&quot; alt=&quot;Members of the Cognition team at Adevinta discussing the latest trends in AI and computer vision.&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;In the competitive realm of digital commerce, embracing technological advancements is not a luxury but a necessity for maintaining success. Among ML tools, foundation models are emerging as a formidable force. But what are foundation models, and why have they become a focal point among technologists and business leaders?&lt;/p&gt;

&lt;p&gt;As members of ‘Cognition’, a team dedicated to computer vision at &lt;a href=&quot;https://adevinta.com/&quot;&gt;Adevinta&lt;/a&gt;, we are eager to share the insights we have gathered through our technology trends watch and recent attendance of the &lt;a href=&quot;https://iccv2023.thecvf.com/&quot;&gt;ICCV 2023&lt;/a&gt; conference. This practice not only ensures our internal services remain up to date but also improves the standards experienced by our users, enhancing the services provided to customers across &lt;a href=&quot;https://adevinta.com/our-brands&quot;&gt;Adevinta’s brands&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;what-are-foundation-models&quot;&gt;What are foundation models ?&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Foundation models&lt;/strong&gt; are a breed of artificial intelligence (AI) models pre-trained on a vast amount of data, laying a robust groundwork for further customisation on specific tasks.&lt;/p&gt;

&lt;p&gt;Unlike traditional machine learning models, which require from-scratch training or fine tuning for every new task, &lt;strong&gt;foundation models offer a substantial head start&lt;/strong&gt;. They have already learned a good deal from the data they were initially trained on, which includes recognising patterns, objects, and in the domain of computer vision, even understanding the semantics of a scene.&lt;/p&gt;

&lt;p&gt;Foundation models can be leveraged in various ways, each with its own balance of resource consumption and performance enhancement. The most resource-efficient method involves extracting features from an image, “freezing” them, and then using them directly as a zero-shot retrieval, classifier or detector. This zero-shot approach requires no further learning, allowing for immediate application.&lt;/p&gt;

&lt;p&gt;Alternatively, these embeddings can serve as inputs to other models, such as an MLP or an XGBoost classifier, through transfer learning. This strategy necessitates a minimal training dataset, yet it remains swift and cost-effective. &lt;a href=&quot;https://arxiv.org/abs/2209.07932&quot;&gt;Pastore et al&lt;/a&gt; reported that there can be &lt;strong&gt;10x to 100x speed increase&lt;/strong&gt; coupled with limited accuracy decrease (1–5% on average), depending on the dataset, when using a kernel classifier on top of frozen features. For a well-known CIFAR100 dataset, the authors observed 10x to 12x speed increase and −3.70% accuracy decrease. From our preliminary experiments, preparing for deploying image embedding services for Adevinta marketplaces, we noted a 5x to 10x speed increase with less than 3% accuracy drop for ImageNet1K dataset with Dinov2 frozen features compared to fine tuning a CNN backbone.&lt;/p&gt;

&lt;p&gt;For those seeking even greater performance enhancements, fine-tuning either the last layers or the entire network is an option. This process may demand a deeper understanding of machine learning and a larger dataset for model refinement, but can lead to substantial improvements. A key challenge in this approach is maintaining the model’s generalisability and preventing it from “forgetting” previously learned datasets.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*yvtVGxc_UkJgw2q6&quot; alt=&quot;A visual representation of the AI model development process, highlighting the efficiency gained by using foundation models.&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;To further enrich the model with bespoke data, one can explore self-supervised learning techniques for pre-training or distillation on domain-specific data. Moreover, to ensure the model remains current with new data, continuous learning methodologies can be employed. These advanced techniques not only enhance the model’s performance, but also tailor it more closely to specific business needs and data environments.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*2ccVFc0mWHKRw6L0&quot; alt=&quot;An illustration of the fine-tuning process in AI model development, emphasizing the balance between performance and resource use&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Beyond the singular applications of foundation models lies the potential for a transformative synergy. By harnessing models trained on diverse datasets with various loss functions, we can unlock new heights of performance. This approach was masterfully demonstrated by &lt;a href=&quot;https://arxiv.org/abs/2306.00984?utm_campaign=The%20Batch&amp;amp;utm_medium=email&amp;amp;_hsmi=281785463&amp;amp;utm_content=281787502&amp;amp;utm_source=hs_email&quot;&gt;Krishnan et al&lt;/a&gt;, who capitalised on images synthesised by Stable Diffusion. They adeptly trained another model (StableRep) using a contrastive loss approach and ViT backbone to achieve remarkable success in a classification benchmark. This strategy showcases the innovative fusion of generative and discriminative model capabilities, setting a new standard for adaptive and robust AI applications.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;&lt;em&gt;“Different foundation models understand different aspects of the world. It’s exciting that a large diffusion model, which is good at generating images, can be used to train a large vision transformer, which is good at analysing images!”&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;&lt;em&gt;—&lt;/em&gt;&lt;/strong&gt; &lt;a href=&quot;https://info.deeplearning.ai/openai-empowers-developers-ai-risk-in-the-spotlight-decoding-schizophrenic-language-synthetic-data-helps-image-classification-1?ecid=ACsprvum6jLSdI3_MtO8GVvBlJrsfj1iNuU7d7wJk3k6DmAu6jDwlmqxh0ZqOYQpd5P6T9SkgLDq&amp;amp;utm_campaign=The%20Batch&amp;amp;utm_medium=email&amp;amp;_hsmi=281785463&amp;amp;_hsenc=p2ANqtz-_AfPCrj7xxHY4m3H4td4jKSdynMLio8p3y-HqpQE0KbMIn5qoGh6dicKnKqf-6eVEcThLfdSR4_uMpwahHLcZqGQKfVg&amp;amp;utm_content=281787502&amp;amp;utm_source=hs_email&quot;&gt;The Batch @Deeplearning.ai newsletter&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;the-rise-of-foundation-models&quot;&gt;The rise of foundation models&lt;/h2&gt;

&lt;p&gt;The history of foundation models is closely tied to the rise of deep learning, particularly with the advent of large-scale models like GPT (Generative Pre-trained Transformer) by OpenAI and BERT (Bidirectional Encoder Representations from Transformers) by Google, which demonstrated the feasibility and effectiveness of pre-training models on vast datasets and then fine-tuning them for specific tasks.&lt;/p&gt;

&lt;p&gt;As technology advanced, so did the scale and capabilities of these models, with models like GPT-3, GPT-4 and T5 showcasing unprecedented levels of generalisation and adaptability across numerous domains including natural language processing, computer vision, and even multimodal tasks combining both vision and text. The success of these models started &lt;strong&gt;a new era where the focus shifted from training task-specific models from scratch to developing robust, versatile foundation models.&lt;/strong&gt; This new type of model could be fine-tuned or used in transfer-learning to excel at a broad spectrum of tasks. This shift not only catalysed significant advancements in AI research but also broadened adoption of AI across various industries, paving the way for more sophisticated and capable foundation models that continue to push the boundaries of what’s achievable with Artificial Intelligence.&lt;/p&gt;

&lt;p&gt;Notable examples of foundation models abound in the tech landscape. For instance, DINOv2 and MAE (Masked Autoencoder) by Meta AI for image understanding. On the other hand, models like CLIP and BLIP from OpenAI have shown the potential of bridging the gap between vision and language. These models, pre-trained on diverse and voluminous datasets, encapsulate a broad spectrum of knowledge that can be adapted for more specialised tasks, making them particularly advantageous for industries with data-rich environments like e-commerce.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*EHZAOKUjrq7Sz6uP&quot; alt=&quot;An illustration demonstrating the innovative use of foundation models to achieve advanced AI capabilities.&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Here is a short description of a few of those models:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;DINOv2:&lt;/strong&gt; Developed by Meta, &lt;a href=&quot;https://ai.meta.com/blog/dino-v2-computer-vision-self-supervised-learning/&quot;&gt;DINOv2&lt;/a&gt; is recognised for its self-supervised learning approach in training computer vision models, achieving significant results.&lt;/p&gt;

&lt;p&gt;The model underscores the potency of self-supervised learning in advancing computer vision capabilities​​.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Masked Autoencoders (&lt;/strong&gt;&lt;a href=&quot;https://arxiv.org/abs/2111.06377#:~:text=This%20paper%20shows%20that%20masked,based%20on%20two%20core%20designs&quot;&gt;&lt;strong&gt;MAE&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;):&lt;/strong&gt; MAE is a scalable self-supervised learning approach for computer vision that involves masking random patches of the input image and reconstructing the missing pixels.&lt;/p&gt;

&lt;p&gt;Meta AI demonstrated the effectiveness of MAE pre-pre training for billion-scale pretraining, combining self-supervised (1st stage) and weakly-supervised learning (2nd stage) for improved performance​.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2103.00020&quot;&gt;&lt;strong&gt;CLIP&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;(Contrastive Language-Image Pre-Training):&lt;/strong&gt; Developed by OpenAI, CLIP is a groundbreaking model that bridges computer vision and natural language processing, leveraging an abundantly available source of supervision: the text paired with images found across the internet.&lt;/p&gt;

&lt;p&gt;CLIP is the first multimodal model tackling computer vision, trained on a variety of (image, text) pairs, achieving competitive zero-shot performance on a variety of image classification datasets. It brings many of the recent developments from the realm of natural language processing into the mainstream of computer vision, including unsupervised learning, transformers, and multimodality​&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Segment Anything Model (&lt;/strong&gt;&lt;a href=&quot;https://segment-anything.com/&quot;&gt;&lt;strong&gt;SAM&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;):&lt;/strong&gt; Developed by Meta’s FAIR lab, SAM is a state-of-the-art image segmentation model that aims to revolutionise the field of computer vision by identifying which pixels in an image belong to which object, producing detailed object masks from input prompts.&lt;/p&gt;

&lt;p&gt;SAM is built on foundation models that have significantly impacted natural language processing (NLP), and focuses on promptable segmentation tasks, adapting to diverse downstream segmentation problems using prompt engineering​.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2211.06220&quot;&gt;&lt;strong&gt;OneFormer&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;/&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2105.15203&quot;&gt;&lt;strong&gt;SegFormer&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;:&lt;/strong&gt; A state-of-the-art multi-task image segmentation framework implemented using transformers. Parameters: 219 million. Architecture: ViT&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.microsoft.com/en-us/research/publication/florence-a-new-foundation-model-for-computer-vision/#:~:text=While%20existing%20vision%20foundation%20models,videos&quot;&gt;&lt;strong&gt;Florence&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;:&lt;/strong&gt; Introduced by Microsoft, this foundation model has set new benchmarks on several leaderboards such as TextCaps Challenge 2021, nocaps, Kinetics-400/Kinetics-600 action classification, and OK-VQA Leaderboard. Florence aims to expand representations from coarse (scene) to fine (object), and from static (images) to dynamic (videos)​.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2112.10752&quot;&gt;&lt;strong&gt;Stable Diffusion&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;:&lt;/strong&gt; A generative model utilising AI and deep learning to generate images, functioning as a diffusion model with a sequential application of denoising autoencoders​.&lt;/p&gt;

&lt;p&gt;It employs a U-Net model, specifically a Residual Neural Network (ResNet), originally developed for image segmentation in biomedicine, to denoise images and control the image generation process without retraining​​.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://openai.com/dall-e-3&quot;&gt;&lt;strong&gt;DALL-E&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;:&lt;/strong&gt; Developed by OpenAI, DALL-E is a generative model capable of creating images from textual descriptions, showcasing a unique blend of natural language understanding and image generation. It employs a version of the GPT-3 architecture to generate images, demonstrating the potential of transformer models in tasks beyond natural language processing​&lt;/p&gt;

&lt;p&gt;The tech titans, often bundled as GAFA (Google, Amazon, Facebook and Apple), alongside several other companies such as Hugging Face, Anthropic, AI21 Labs, Cohere, Aleph Alpha, Open AI and Salesforce have been instrumental in developing, utilising and advancing foundation models. Substantial investments in these models underscore their potential, as these corporations harness foundation models to augment various facets of their operations, setting a benchmark for &lt;a href=&quot;https://crfm.stanford.edu/2021/10/18/reflections.html#:~:text=Simultaneously%2C%20in%20industry%2C%20several%20startups,that%20impact%20billions%20of%20people&quot;&gt;other sectors&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Insights from industry leaders at &lt;a href=&quot;https://venturebeat.com/ai/foundation-models-2022s-ai-paradigm-shift/#:~:text=Foundation%20models%20like%20DALL,computer%20science%20%20department%20at&quot;&gt;Google&lt;/a&gt;, &lt;a href=&quot;https://www.microsoft.com/en-us/research/academic-program/accelerate-foundation-models-research-fall-2023/#:~:text=About%20the%20program,society%20while%20mitigating%20risks&quot;&gt;Microsoft&lt;/a&gt; and &lt;a href=&quot;https://research.ibm.com/topics/foundation-models&quot;&gt;IBM&lt;/a&gt;, alongside academic institutions, provide a rich tapestry of knowledge and perspectives​.&lt;/p&gt;

&lt;p&gt;Percy Liang, a director of the Center for Research on Foundation Models, emphasised in &lt;a href=&quot;https://www.protocol.com/enterprise/foundation-models-ai-standards-stanford&quot;&gt;this article&lt;/a&gt; that foundation models like DALL-E and GPT-3 herald new creative opportunities and novel interaction mechanisms with systems, showcasing the innovation that these models can bring to the table. He also mentions potential risks of such powerful models​.&lt;/p&gt;

&lt;p&gt;At the &lt;a href=&quot;https://iccv2023.thecvf.com/&quot;&gt;ICCV 2023 conference&lt;/a&gt;, held this year in Paris, foundation models were a very present topic. William T. Freeman, Professor of Computer Science, MIT, talked about the foundation models in his talk in &lt;a href=&quot;https://gkioxari.github.io/Tutorials/iccv2023/&quot;&gt;QUO VADIS Computer Vision&lt;/a&gt; workshop. He cited reasons why he &lt;a href=&quot;https://drive.google.com/file/d/1HfSrxSMS54c6-rYQNKBZnqgk_eRYqwOx/view&quot;&gt;does not like foundation models&lt;/a&gt; as an academic:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;em&gt;They don’t tell us how vision works.&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;They’re not fundamental (and therefore not stable)&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;They separate academia from industry&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This highlights the importance of foundation models for the future of computer vision and their established position and pragmatic aspect of those models focusing on performance.&lt;/p&gt;

&lt;p&gt;IBM Research posits that &lt;a href=&quot;https://research.ibm.com/topics/foundation-models&quot;&gt;foundation models will significantly expedite AI adoption&lt;/a&gt; in business settings. The general applicability of these models, enabled through self-supervised learning and fine-tuning, allows for a wide range of AI applications, thereby accelerating AI deployment across various business domains​.&lt;/p&gt;

&lt;p&gt;Microsoft Research highlights that foundation models are instigating &lt;a href=&quot;https://research.ibm.com/topics/foundation-models&quot;&gt;a fundamental shift in computing research&lt;/a&gt; and across various scientific domains. This shift is underpinned by the models’ ability to fuel industry-led advances in AI, thereby contributing to a vibrant and diverse research ecosystem that’s poised to unlock the promise of AI for societal benefit while addressing associated risks.&lt;/p&gt;

&lt;p&gt;Experts also underscore the critical role of computer vision foundation models in solving real-world applications, emphasising their &lt;a href=&quot;https://crfm.stanford.edu/2021/10/18/reflections.html#:~:text=Simultaneously%2C%20in%20industry%2C%20several%20startups,that%20impact%20billions%20of%20people&quot;&gt;adaptability to a myriad of downstream&lt;/a&gt; tasks due to training on diverse, large-scale datasets​. Moreover, foundation models like CLIP &lt;a href=&quot;https://arxiv.org/abs/2103.00020&quot;&gt;enable zero-shot learning&lt;/a&gt;, allowing for versatile applications like classifying video frames, identifying scene changes and building semantic image search engines without necessitating prior training.&lt;/p&gt;

&lt;p&gt;In another workshop of ICCV 2023, &lt;a href=&quot;https://bigmac-vision.github.io/&quot;&gt;BigMAC&lt;/a&gt;: Big Model Adaptation for Computer Vision, the &lt;a href=&quot;https://bigmac-vision.github.io/pdfs/ludwig.pdf&quot;&gt;robustness of the CLIP model&lt;/a&gt; on the popular ImageNet benchmark was discussed. In conclusion, thanks to training on a large, versatile dataset means that zero-shot predictions of the CLIP model are less vulnerable to data drift than popular CNN models trained and fine tuned on imageNet. In this &lt;a href=&quot;https://www.youtube.com/watch?v=XiouM3MEOKs&amp;amp;t=4546s&quot;&gt;recording of Ludwig’s presentation&lt;/a&gt; different ways to preserve CLIP robustness while fine-tuned are discussed.&lt;/p&gt;

&lt;p&gt;On a side note, the ICCV conference was quite an event. With five days of workshops, talks and demos! Big tech companies such as Meta marked their presence with impressive hubs, answering attendees’ questions. Numerous poster sessions gave us a chance to interact with authors and select some ideas we would like to contribute to the tech stack at Adevinta.&lt;/p&gt;

&lt;p&gt;In the subsequent sections, we will dig into real-world instances, underscoring their impact on e-commerce and elaborate how investing in this technology can galvanise collaboration and innovation across various teams within a company.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*ZtACVLp3aedfQYAy&quot; alt=&quot;An overview of popular foundation models and their applications in various AI domains.&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;h2 id=&quot;real-world-adoption-of-foundation-models&quot;&gt;Real-World Adoption of foundation models&lt;/h2&gt;

&lt;p&gt;Major tech companies have paved the way in producing and distributing ready-to-use foundation models, which are now being utilised by various businesses to &lt;a href=&quot;https://www.forbes.com/sites/moorinsights/2023/07/21/the-extraordinary-ubiquity-of-generative-ai-and-how-major-companies-are-using-it/&quot;&gt;enhance or create new products&lt;/a&gt; for tech-savvy consumers&lt;a href=&quot;https://www.forbes.com/sites/moorinsights/2023/07/21/the-extraordinary-ubiquity-of-generative-ai-and-how-major-companies-are-using-it/&quot;&gt;​&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;e-commerce-andretail&quot;&gt;E-commerce and retail&lt;/h2&gt;

&lt;p&gt;In the sphere of e-commerce, companies like Pinterest and eBay, have &lt;a href=&quot;https://developer.nvidia.com/blog/pinterest-uses-ai-to-enhance-its-recommendations-system/#:~:text=Developers%20from%20Pinterest%2C%20along%20with,objects%20saved%20has%20crossed&quot;&gt;invested in deep learning&lt;/a&gt; and machine learning technologies to enhance user experiences. Pinterest has developed PinSage for advertising and shopping recommendations and a multi-task deep metric &lt;a href=&quot;https://blog.acolyer.org/2019/10/11/learning-a-unified-embedding-for-visual-search-at-pinterest/#:~:text=The%20foundation%20of%20Pinterest%E2%80%99s%20approach,task%20learning&quot;&gt;learning system for unified image embedding&lt;/a&gt; to aid in &lt;a href=&quot;https://arxiv.org/abs/1908.01707&quot;&gt;visual search&lt;/a&gt; and recommendation systems​&lt;a href=&quot;https://blog.acolyer.org/2019/10/11/learning-a-unified-embedding-for-visual-search-at-pinterest/#:~:text=The%20foundation%20of%20Pinterest%E2%80%99s%20approach,task%20learning&quot;&gt;​&lt;/a&gt;. eBay, on the other hand, utilises a convolutional neural network for its &lt;a href=&quot;https://www.ebayinc.com/stories/news/an-easier-way-to-search-ebay-computer-vision-with-find-it-on-ebay-and-image-search-is-now-live/#:~:text=When%20you%20upload%20images%20to,the%20live%20listings%20on%20eBay&quot;&gt;image search feature&lt;/a&gt;, “Find It On eBay.”​&lt;/p&gt;

&lt;p&gt;Computer vision applications are transforming e-commerce, aiding in creating seamless omnichannel shopping experiences​. When it comes to the importance of visuals in shopping experiences, a study by PowerReviews found that &lt;strong&gt;88% of consumers specifically&lt;/strong&gt; &lt;a href=&quot;https://www.ebayinc.com/stories/news/an-easier-way-to-search-ebay-computer-vision-with-find-it-on-ebay-and-image-search-is-now-live/#:~:text=When%20you%20upload%20images%20to,the%20live%20listings%20on%20eBay&quot;&gt;&lt;strong&gt;look for visuals&lt;/strong&gt;&lt;/a&gt; submitted by other consumers prior to making a purchase​.&lt;/p&gt;

&lt;h3 id=&quot;broader-techindustry&quot;&gt;Broader tech industry&lt;/h3&gt;

&lt;p&gt;In the broader tech industry, Microsoft has introduced &lt;a href=&quot;https://www.ebayinc.com/stories/news/an-easier-way-to-search-ebay-computer-vision-with-find-it-on-ebay-and-image-search-is-now-live/#:~:text=When%20you%20upload%20images%20to,the%20live%20listings%20on%20eBay&quot;&gt;Florence&lt;/a&gt;, a novel foundation model for computer vision. The underlying technology of foundation models is designed to provide a solid base that can be fine-tuned for various specific tasks, an advantage that has been recognised and harnessed by industry giants.&lt;/p&gt;

&lt;p&gt;Take Copenhagen-based startup Modl.ai for instance, which relies on foundation models, self-supervised training and computer vision for &lt;a href=&quot;https://the-decoder.com/ai-startup-wants-to-bring-foundation-models-to-game-development/#:~:text=Copenhagen,with%20and%20against%20human%20players&quot;&gt;developing AI bots&lt;/a&gt; to test video games for bugs and performance. Such applications demonstrate the versatility and potential of foundation models in different sectors​.&lt;/p&gt;

&lt;p&gt;The practical implementations of foundation models in these different sectors underscores their potential to drive innovation, enhance user experiences and foster cross-functional collaboration within and beyond the e-commerce spectrum. The flexibility and adaptability of foundation models, as demonstrated by these real-world examples, make them a valuable asset for companies striving to stay ahead in the competitive e-commerce landscape.&lt;/p&gt;

&lt;h2 id=&quot;investing-in-foundation-models-cost-benefit-analysis&quot;&gt;Investing in foundation models: Cost-benefit analysis&lt;/h2&gt;

&lt;p&gt;The investment in foundation models for computer vision transcends the mere financial outlay. It encapsulates a strategic foresight to harness advanced AI technologies for bolstering e-commerce operations.&lt;/p&gt;

&lt;p&gt;Investing in foundation models for computer vision in e-commerce does entail upfront costs such as acquiring computational resources and the requisite expertise. OpenAI’s GPT-3 model, for example, reportedly cost $4.6M to train. According to another OpenAI report, the cost of training a large AI model is &lt;a href=&quot;https://encord.com/blog/visual-foundation-models-vfms-explained/#:~:text=OpenAI%E2%80%99s%20GPT%2D3%20model%2C%20for%20example%2C%20reportedly%20cost%20%244.6MM%20to%20train.%20According%20to%20another%20OpenAI%20report%2C%20the%20cost%20of%20training%20a%20large%20AI%20model%20is%20projected%20to%20rise%20from%20%24100MM%20to%20%24500MM%20by%202030.&quot;&gt;projected to rise&lt;/a&gt; from $100M to $500M by 2030.&lt;/p&gt;

&lt;p&gt;However, the potential benefits could justify the investment. For instance, the &lt;strong&gt;global visual search market,&lt;/strong&gt; which is significantly powered by computer vision technology, is projected to reach &lt;strong&gt;$15 billion by 2023&lt;/strong&gt;. Early adopters who incorporate visual search on their platforms could see &lt;a href=&quot;https://blog.taskmonk.ai/what-role-will-computer-vision-play-in-the-future-of-ecommerce/#:~:text=Early%20adopters%20who%20incorporate%20visual,of%20their%20online%20shopping%20experience&quot;&gt;&lt;strong&gt;revenues increase by 30%&lt;/strong&gt;&lt;/a&gt;. The computer vision market itself is soaring with an expected &lt;strong&gt;annual growth rate of 19.5%&lt;/strong&gt;, predicted to reach a value of $100.4 billion by 2023​&lt;a href=&quot;https://encord.com/blog/visual-foundation-models-vfms-explained/#:~:text=April%2024%2C%202023%20%E2%80%A2%205,9Bn%20in%202022&quot;&gt;​&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;These figures suggest that the integration of computer vision, particularly through foundation models, can be a lucrative venture in the long-term. Consumers are increasingly leaning towards platforms that offer visual search and other AI-driven features. Therefore, the cost of investment could be offset by the subsequent increase in revenue, enhanced user engagement and improved operational efficiency brought about by the advanced capabilities of foundation models in computer vision.​&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;&lt;em&gt;“Foundation models cut down on data labelling requirements anywhere from a factor of like 10 times, 200 times, depending on the use case”&lt;/em&gt;&lt;/strong&gt;— &lt;a href=&quot;https://venturebeat.com/ai/foundation-models-2022s-ai-paradigm-shift/#:~:text=%E2%80%9CFoundation%20models%20cut%20down%20on%20data%20labeling%20requirements%20anywhere%20from%20a%20factor%20of%20like%2010%20times%2C%20200%20times%2C%20depending%20on%20the%20use%20case%2C%E2%80%9D%20Dakshi%20Agrawal%2C%20IBM%20fellow%20and%20CTO%20of%20IBM%20AI%2C&quot;&gt;Dakshi Agrawal&lt;/a&gt;, IBM fellow and CTO of IBM AI&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Moreover, the global computer vision market, which encompasses technologies enabling such visual experiences, is expected to &lt;a href=&quot;https://www.syte.ai/blog/visual-ai/how-visual-ai-is-changing-omnichannel-retail/&quot;&gt;grow substantially&lt;/a&gt;, indicating the increasing importance of investment in visual technologies for retail and e-commerce​. The role of visual AI, which includes &lt;a href=&quot;https://research.aimultiple.com/computer-vision-retail/#:~:text=The%20global%20computer%20vision%20market,improve%20efficiency%20in%20omnichannel&quot;&gt;computer vision&lt;/a&gt;, is also highlighted in how it’s changing omnichannel retail, showcasing the intertwined relationship between visual technology and &lt;a href=&quot;https://losspreventionmedia.com/computer-vision-future-of-retail/&quot;&gt;enhanced shopping experiences&lt;/a&gt; across channels​.&lt;/p&gt;

&lt;h2 id=&quot;examples-of-application-of-foundation-models-in-e-commerce&quot;&gt;Examples of application of foundation models in e-commerce&lt;/h2&gt;

&lt;p&gt;Because of their pre-training on expansive datasets, foundation models in computer vision bring a treasure trove of capabilities to the table. &lt;strong&gt;The pre-trained nature of foundation models significantly accelerates the deployment of computer vision applications in e-commerce, as they require less data and resources for fine-tuning compared to training models from scratch.&lt;/strong&gt; Let’s illustrate this through real-world examples within the e-commerce sector.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Product Categorisation&lt;/strong&gt;: Leveraging a foundation model for automated product categorisation can be a time and resource-saver.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Visual Search&lt;/strong&gt;: Implementing visual search features can be expedited with foundation models. Their pre-trained knowledge can be leveraged to recognise fashion or product trends, making visual search more intuitive.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Counterfeit Detection&lt;/strong&gt;: Counterfeit detection is a complex task; however, with a foundation model, the pre-existing knowledge about different objects can be fine-tuned to identify subtle discrepancies between genuine and counterfeit products&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Moderation&lt;/strong&gt;: Detection of unwanted or harmful content can be done through a classification head added on top of image embeddings generated by a foundation model.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*xDEvavmYRc4MF2xE&quot; alt=&quot;A diagram showcasing the various applications of foundation models in e-commerce, from product categorization to counterfeit detection.&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Beyond these examples, foundation models also hold promise in enhancing user experiences in recommendation systems and augmented reality (AR) shopping.&lt;/p&gt;

&lt;p&gt;Most of this use-case could be applied to Adevinta marketplaces or replace existing services based on more traditional models.&lt;/p&gt;

&lt;h2 id=&quot;empowering-teams-across-the-e-commerce-spectrum&quot;&gt;Empowering teams across the e-commerce spectrum&lt;/h2&gt;

&lt;p&gt;Foundation models in computer vision open up avenues for fostering cross-functional collaboration, expediting product development, and making data-driven decision-making a norm across an e-commerce enterprise. Let’s delve into how these models can act as catalysts in harmonising the efforts of various teams and speeding up the journey from conception to market-ready solutions.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*3hnJ3nPcTBr68D70&quot; alt=&quot;Members of the Cognition team at Adevinta discussing the latest trends in AI and computer vision.&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;h2 id=&quot;accelerating-the-product-development-cycle&quot;&gt;Accelerating the product development cycle&lt;/h2&gt;

&lt;p&gt;The pre-trained nature of foundation models significantly &lt;strong&gt;cuts down the time traditionally required to develop, train and deploy machine learning models&lt;/strong&gt;. This acceleration in the product development cycle is invaluable in the fiercely competitive e-commerce market, where being the first to introduce innovative features can provide a substantial competitive edge. Moreover, the resource efficiency of foundation models ensures that &lt;strong&gt;teams can iterate and improve upon models swiftly&lt;/strong&gt;, aligning with dynamic market trends and customer expectations.&lt;/p&gt;

&lt;h2 id=&quot;stepping-stone-to-broader-business-objectives&quot;&gt;Stepping stone to broader business objectives&lt;/h2&gt;

&lt;p&gt;Foundation models can act as a springboard towards achieving broader business goals such as sustainability and promoting the second-hand goods trade. By enabling smarter product listings and verifications through image recognition and visual search capabilities, these models can streamline the process of listing and verifying second-hand goods. This, in turn, &lt;strong&gt;promotes a circular economy, encouraging the reuse and recycling of products&lt;/strong&gt;, which aligns with the sustainability goals of many modern e-commerce platforms.&lt;/p&gt;

&lt;h2 id=&quot;challenges-and-overcoming-strategies&quot;&gt;Challenges and overcoming strategies&lt;/h2&gt;

&lt;p&gt;Incorporating foundation models for computer vision within an e-commerce setting comes with a range of challenges, but with the right strategies, these hurdles can be navigated to unlock the models’ full potential.&lt;/p&gt;

&lt;h2 id=&quot;computational-requirements&quot;&gt;Computational requirements&lt;/h2&gt;

&lt;p&gt;Foundation models are computationally intensive due to their large-scale nature, which necessitates &lt;a href=&quot;https://snorkel.ai/foundation-models/#:~:text=Cost,for%20their%20end%20use%20caes&quot;&gt;significant computational resources&lt;/a&gt; for training and fine-tuning. The good news is that, once the substantial work of domain-learning or fine tuning is done, numerous teams and projects can benefit from the foundation model with minimal additional effort and cost.&lt;/p&gt;

&lt;h2 id=&quot;bias-andfairness&quot;&gt;Bias and fairness&lt;/h2&gt;

&lt;p&gt;Foundation models may inherit biases present in the training data, which can lead to unfair or discriminatory behaviour. For instance, &lt;a href=&quot;https://datagen.tech/blog/the-opportunities-and-risks-of-foundation-models/&quot;&gt;DALL-E and CLIP have shown biases&lt;/a&gt; regarding gender and race when generating images or interpreting text and images​. Implementing robust data preprocessing and bias mitigation strategies will help to address potential biases in training data.&lt;/p&gt;

&lt;h2 id=&quot;interpretability-andcontrol&quot;&gt;Interpretability and control&lt;/h2&gt;

&lt;p&gt;Understanding and controlling the behaviour of foundation models like CLIP remains a challenge due to their black-box nature. This makes it difficult to interpret the models’ predictions, which is a hurdle in applications where &lt;a href=&quot;https://arxiv.org/abs/2103.00020&quot;&gt;explainability is crucial&lt;/a&gt;​​. CRFM released recently a &lt;a href=&quot;https://crfm.stanford.edu/fmti/?utm_campaign=The%20Batch&amp;amp;utm_medium=email&amp;amp;_hsmi=280825441&amp;amp;utm_content=280827829&amp;amp;utm_source=hs_email&quot;&gt;Foundation Model Transparency Index&lt;/a&gt; “scoring 10 popular models on how well their makers disclosed details of their training, characteristics and use.”&lt;/p&gt;

&lt;p&gt;Foundation models, if widely adopted, could introduce &lt;strong&gt;single points of failure&lt;/strong&gt; in machine learning systems. If adversaries find vulnerabilities in a foundation model, they could &lt;a href=&quot;https://arxiv.org/abs/2103.11251&quot;&gt;exploit these weaknesses&lt;/a&gt; across multiple systems utilising the same model​.&lt;/p&gt;

&lt;p&gt;Foundation models are &lt;strong&gt;not the answer to all&lt;/strong&gt; machine learning problems.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;&lt;em&gt;“Foundation models are neither ‘foundational’ nor the foundations of AI. We deliberately chose ‘foundation’ rather than ‘foundational,’ because we found that ‘foundational’ implied that these models provide fundamental principles in a way that ‘foundation’ does not. (…) Further, ‘foundation’ describes the (role of) model and not AI; we neither claim nor believe that foundation models alone are the foundation of AI, but instead note they are ‘only one component (though an increasingly important component) of an AI system.’”&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;— the Stanford Institute for Human-Centred AI founded the Center for &lt;a href=&quot;https://arxiv.org/pdf/2108.07258.pdf&quot;&gt;Research on Foundation Models&lt;/a&gt; (CRFM), 2021&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;The transformative potential of foundation models in computer vision is unmistakable and pivotal for advancing the e-commerce domain. They encapsulate a significant stride towards creating smarter, more intuitive and user-centric online shopping experiences. The notable successes of early adopters, alongside the burgeoning global visual search market, exhibit the financial promise inherent in embracing these models​.&lt;/p&gt;

&lt;p&gt;The real-world implications extend beyond just improved product discovery and categorisation, to fostering a sustainable trading ecosystem for second-hand goods. &lt;strong&gt;The expertise and investment in these models can expedite the product development cycle, encourage data-driven decision-making and stimulate cross-functional collaboration across various company departments.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;However, it’s crucial to acknowledge the technical and ethical challenges that come with the deployment of foundation models. The computational costs, potential biases and the necessity for robust infrastructures demand a well-thought-out strategic approach. Yet, with the right investment in computational infrastructure, continuous learning and a commitment to ethical AI practices, these hurdles can be navigated successfully.&lt;/p&gt;

&lt;p&gt;While the promise of foundation models in computer vision is evident, discerning which model will perform optimally with your specific data remains a complex challenge.&lt;/p&gt;

&lt;p&gt;This uncertainty underscores the vital &lt;strong&gt;need for comprehensive benchmarks&lt;/strong&gt; that can guide businesses in selecting the most appropriate model. Investing time in testing and evaluation is crucial, as it enables a more informed decision-making process. A recent study highlighted in the article “&lt;a href=&quot;https://arxiv.org/pdf/2310.19909.pdf&quot;&gt;A Comprehensive Study on Backbone Architectures for Regular and Vision Transformers&lt;/a&gt;” delves into this subject by testing different model backbones across a range of downstream tasks and datasets. Such research is invaluable for businesses looking to capitalise on foundation models, as it provides critical insights into model performance and applicability, ensuring that their investment in AI is both strategic and effective.&lt;/p&gt;

&lt;p&gt;In Adevinta, as an e-commerce leader, we are evaluating the pros and cons of foundation models to best leverage their potential within our company. In Cognition, we are also working on internal benchmarks that will help to chose right foundation model for the task, estimate ressources needed and showcase its potential performance on the marketplace data.&lt;/p&gt;

&lt;p&gt;With industry behemoths and experts leading the era of foundation models, the call to action for e-commerce directors is clear: &lt;strong&gt;Embrace the paradigm shift that foundation models represent, and consider them as a long-term strategic asset for maintaining a competitive edge in the rapidly evolving e-commerce landscape.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Check out this mine of knowledge about foundation models: &lt;a href=&quot;https://cs.uwaterloo.ca/~wenhuche/teaching/cs886/&quot;&gt;https://cs.uwaterloo.ca/~wenhuche/teaching/cs886/&lt;/a&gt;&lt;/p&gt;

&lt;footer&gt;
  &lt;p&gt;Exported from &lt;a href=&quot;https://medium.com&quot;&gt;Medium&lt;/a&gt; on December 19
    2023.&lt;/p&gt;
  &lt;p&gt;&lt;a href=&quot;https://medium.com/adevinta-tech-blog/foundation-models-a-new-vision-for-e-commerce-76904a3066e8&quot;&gt;View
      the original&lt;/a&gt;&lt;/p&gt;
&lt;/footer&gt;
&lt;script type=&quot;text/javascript&quot; src=&quot;//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-584ec4ce89deed84&quot;&gt;&lt;/script&gt;

&lt;div class=&quot;addthis_inline_share_toolbox&quot;&gt;&lt;/div&gt;

</description>
        <pubDate>Tue, 19 Dec 2023 00:00:00 +0100</pubDate>
        <link>http://urszulaczerwinska.github.io/thoughts/foundation-models-a-new-vision-for-e-commerce</link>
        <guid isPermaLink="true">http://urszulaczerwinska.github.io/thoughts/foundation-models-a-new-vision-for-e-commerce</guid>
        
        
        <category>thoughts</category>
        
        <category>featured</category>
        
      </item>
    
      <item>
        <title>Deep Dive in PaddleOCR inference</title>
        <description>&lt;head&gt;
  &lt;meta name=&quot;description&quot; content=&quot;A deep dive into the complexities of using PaddleOCR for text extraction from images and how the Cognition team improved the service. Learn about the challenges and solutions that enhanced user experience in OCR services.&quot; /&gt;
&lt;/head&gt;

&lt;p&gt;This article is a deep dive into part of our work as described in &lt;a href=&quot;/works/text-in-image-2-0-improving-ocr-service-with-paddleocr&quot;&gt;&lt;strong&gt;Article 1: Text in Image 2.0: improving OCR service with PaddleOCR&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We are Cognition, an &lt;a href=&quot;https://www.adevinta.com/&quot;&gt;Adevinta&lt;/a&gt; Computer Vision Machine Learning (ML) team working on solutions for our marketplaces. Adevinta is a global classifieds specialist with market-leading positions in key European markets that aims to find perfect matches between its users and the platforms’ goods. As a Global Team, our team, Cognition, provides image processing APIs to all of our marketplaces.&lt;/p&gt;

&lt;p&gt;In the process of improving our OCR API for text extraction from images, we updated our existing Text in Image service to the &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR&quot;&gt;PaddleOCR&lt;/a&gt; framework, which was the winner of our benchmarks. In order to test if this framework was the most suitable solution, we carried out a deeper analysis of their code base. This article shares the challenges we encountered and how we overcame them.&lt;/p&gt;

&lt;p&gt;We believe our code version is easier to work with, given the use case of text extraction from images. The different steps and pre-processing and post-processing parts are clearly separated so they can be called independently, which should make further community extensions easier to add. It also makes putting into production easier, as the simplified, modular code combines well with the structure of &lt;em&gt;inference.py&lt;/em&gt; for serving SageMaker endpoints. Our proposed code version does not alter predictions (compared to the 2.6 release) for images.&lt;/p&gt;

&lt;h2 id=&quot;understanding-the-paddleocr-framework&quot;&gt;Understanding the PaddleOCR framework&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/PaddlePaddle/Paddle&quot;&gt;PaddlePaddle&lt;/a&gt; (short for Parallel Distributed Deep Learning) is an open source deep learning platform developed by Baidu Research. It is written in C++ and Python, and is designed to be easy to use and efficient for large-scale machine learning tasks.&lt;/p&gt;

&lt;p&gt;PaddlePaddle provides a range of tools and libraries for building and training deep learning models, including support for convolutional neural networks (CNNs), recurrent neural networks (RNNs) and long short-term memory (LSTM) networks.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR&quot;&gt;PaddleOCR&lt;/a&gt; builds on PaddlePaddle, an unfamiliar framework that our team had not used before. To make things even more challenging, PaddleOCR is not just one algorithm, it includes a range of pre-trained models and tools for recognising text in images and documents, as well as for training custom OCR models.&lt;/p&gt;

&lt;p&gt;PaddleOCR is divided into two main sections:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;PP-OCR&lt;/strong&gt;, an OCR system used for text extraction from images&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;PP-Structure&lt;/strong&gt;, a document analysis system which aims to perform layout analysis and table recognition&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;PP-OCR exists in three different versions (V1, V2 and V3). In these different releases, major improvements were brought to the models’ architecture.&lt;/p&gt;

&lt;p&gt;For our Text in Image service update, we focused on the most recent and most performant PP-OCRv3 release.&lt;/p&gt;

&lt;h3 id=&quot;the-paddleocrv3-models-architecture&quot;&gt;The PaddleOCRv3 models architecture&lt;/h3&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*1mI3YTIjAut_QMrl&quot; alt=&quot;PaddleOCRv3 Architecture&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;PP-OCRv3 is composed of three parts: detection, classification and recognition, all of which can be used independently. Each part has its own model trained with the PaddlePaddle framework. For those interested, model details can be found in this dedicated research article PP-OCRv3: &lt;a href=&quot;https://arxiv.org/abs/2206.03001v2&quot;&gt;More Attempts for the Improvement of Ultra Lightweight OCR System (Yanjun et al., 2022)&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;PP-OCRv3 text detection is made with the Differentiable Binarization algorithm (&lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.6/doc/doc_en/algorithm_det_db_en.md&quot;&gt;DB&lt;/a&gt;) trained using distillation strategy. The PP-OCRv3 recogniser is optimised based on the text recognition algorithm, Scene Text Recognition with a Single Visual Model (&lt;a href=&quot;https://arxiv.org/abs/2205.00159&quot;&gt;SVTR, Du et al. 2022)&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;PP-OCRv3 adopts the text recognition network SVTR_LCNet, and uses &lt;a href=&quot;https://arxiv.org/abs/2002.01276&quot;&gt;the guided training of Connectionist Temporal Classification (CTC&lt;/a&gt;, Z&lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Lin%2C+Z&quot;&gt;hiping&lt;/a&gt; et al., 2020) by the attention, data augmentation strategy, TextConAug, Unified Deep Mutual Learning and Unlabelled Images Mining (first introduced in &lt;a href=&quot;https://arxiv.org/abs/2109.03144&quot;&gt;PaddleOCRv2, Yanjun et al. 2021&lt;/a&gt;). The Text classifier is a simple binary classifier with classes 0 and 180°.&lt;/p&gt;

&lt;h3 id=&quot;paddleocr-inference-in-practice&quot;&gt;PaddleOCR inference in practice&lt;/h3&gt;

&lt;p&gt;While testing on our benchmarks, we used the PaddleOCR code for inference with default parameters and “latin” as a language (see their &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.6/doc/doc_en/quickstart_en.md&quot;&gt;QuickStart page&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Reading the documentation and looking into the class parameters, we saw lots of model combinations to test and therefore more opportunities to potentially improve our score.&lt;/p&gt;

&lt;p&gt;For instance, the &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.6/doc/doc_en/detection_en.md&quot;&gt;documentation&lt;/a&gt; suggests there is a choice between “DB” and “EAST” algorithms for detection, but it’s only the main inference &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.6/paddleocr.py&quot;&gt;script&lt;/a&gt; where the algorithm has to be “DB” — the &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/blob/23e034c40ecd5755af48d7b14dcc1bf6c5cf1128/tools/infer/predict_det.py#L62&quot;&gt;script&lt;/a&gt; of detection inference goes through a long list of algorithms. A similar situation occurs with text recognition where the pre-trained algorithm for Latin is “SVTR_LCNet”, but in &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/blob/23e034c40ecd5755af48d7b14dcc1bf6c5cf1128/paddleocr.py#L51&quot;&gt;theory&lt;/a&gt;, the accepted values are “‘CRNN’ and ‘SVTR_LCNet’ with the general &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/blob/18ddb6d5f9bdc2c1b0aa7f6e399ec0f76119dc87/doc/doc_en/inference_args_en.md&quot;&gt;documentation&lt;/a&gt; mentioning a plethora of models.&lt;/p&gt;

&lt;p&gt;Pre-trained English models are available in “‘CRNN’ and ‘SVTR_LCNet’ architectures. However, to find the information, the user would need to look into the pretrained model &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/blob/18ddb6d5f9bdc2c1b0aa7f6e399ec0f76119dc87/configs/rec/PP-OCRv3/en_PP-OCRv3_rec.yml#L39&quot;&gt;config&lt;/a&gt;. If the user does not specify the “rec_algorithm”, the default value, “SVTR_LCNet”, would be used, even if it isn’t correct. This doesn’t actually make any difference to the inference &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.6/tools/infer/predict_rec.py&quot;&gt;code&lt;/a&gt; as none of the “if” applies to ‘CRNN’ or ‘SVTR_LCNet’.&lt;/p&gt;

&lt;p&gt;In order to test a different architecture, we would need to train it ourselves and chain dedicated scripts.&lt;/p&gt;

&lt;h2 id=&quot;clarifying-paddleocr-inference&quot;&gt;Clarifying PaddleOCR inference&lt;/h2&gt;

&lt;p&gt;From digging into the code, we discovered several complexities, unnecessary for our use case. Firstly, the code seemed to grow organically, where the inference version is a limited choice entry to the multi-option code. This leaves us with numerous “factory patterns” and “if .. elses”, where the user has no choice at all. The English documentation was confusing and referenced different usage cases. We struggled to follow the logic as it neither explained parameters, nor clearly defined the limitations of the inference code.&lt;/p&gt;

&lt;p&gt;Despite these complexities, we managed to clarify the general way of working, calling the PaddleOCR.ocr() method from the ‘master’ file, &lt;em&gt;paddleocr.py&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*zwImfJ-4pOxDvrEI&quot; alt=&quot;PaddleOCR.ocr() Method&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;The input image and parameters are entered into the PaddleOCR.ocr() method which calls TextSystem class in order: TextDetector, TextClassifier and TextRecogniser, with a selection of helper functions, including one that formats the outputs of TextDetector into a list of cropped images being input to TextClassifier and TextRecogniser.&lt;/p&gt;

&lt;p&gt;The PaddleOCR.ocr() method is parsing params, including the language, version, type of OCR (or structure), downloads inference models and imports actual image (with check_image).&lt;/p&gt;

&lt;p&gt;If we want our image to go through a full OCR process, the TextSystem class will sequentially call classes responsible for detection, classification and recognition.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*B-7pY0A4Xv7eNTcr&quot; alt=&quot;TextSystem Class Flow&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Each of the main classes has an &lt;em&gt;__init__&lt;/em&gt; method that initialises pre- &amp;amp; post- processing classes and loads the model (create_predictor), and &lt;em&gt;__call__&lt;/em&gt; method that executes (pre- &amp;amp;) post-processing on the image and performs the model inference for the input image(s).&lt;/p&gt;

&lt;p&gt;Most of the scripts used for inference can be found under ‘tools/infer/’. The pre-processing scripts are under “ppocr/data/imaug/operators.py”. The post-processing classes are under ‘ppocr/postprocess/’.&lt;/p&gt;

&lt;p&gt;This schema enables us to reduce the essential inference code to just a couple of files and better understand exactly how the code works. To make it easier to maintain, we decided to reformat the code, keeping only the essential parts for our use case.&lt;/p&gt;

&lt;h2 id=&quot;paddleocr-inference-code-caveats-andfixes&quot;&gt;PaddleOCR inference code caveats and fixes&lt;/h2&gt;

&lt;p&gt;Let’s walk you through the PaddleOCR features we didn’t like and suggestions on how they could be improved.&lt;/p&gt;

&lt;h3 id=&quot;spaghetti-code&quot;&gt;Spaghetti code&lt;/h3&gt;

&lt;p&gt;Overall, most of the code is in object oriented programming style where classes are not modular and most things happen in very long &lt;em&gt;__init__&lt;/em&gt; and &lt;em&gt;__call__&lt;/em&gt; methods. We have noticed (fig. 2 and fig. 3) that generally, three parts can be extracted: pre-processing, inference and post-processing. We have removed ‘create_operators’ and ‘build_post_process’ intermediate functions and called directly the class performing the task such as “DBPostprocess” and “NormalizeImage”. To make things more straightforward, we transformed them into simple functions, performing what their &lt;em&gt;__call__&lt;/em&gt; method was doing before. This leaves us with more modular code and direct logic that fits our needs.&lt;/p&gt;

&lt;h3 id=&quot;parameter-parsing&quot;&gt;&lt;em&gt;Parameter parsing&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;We found it problematic that the inference class requires 105 parameters, of which more than 70 were ignored.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*jPMJx-wOF-R5DsmqJFs5BA.png&quot; alt=&quot;PaddleOCR inference parameters are not all used&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/blob/18ddb6d5f9bdc2c1b0aa7f6e399ec0f76119dc87/doc/doc_en/inference_args_en.md&quot;&gt;English documentation&lt;/a&gt; lists the parameters and gives a succinct definition of them. In the code, they are defined in at least three different places: &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/blob/23e034c40ecd5755af48d7b14dcc1bf6c5cf1128/paddleocr.py#L307&quot;&gt;paddleocr.py&lt;/a&gt;, &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/blob/23e034c40ecd5755af48d7b14dcc1bf6c5cf1128/ppstructure/utility.py#L21&quot;&gt;utility.py&lt;/a&gt; and different &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/blob/34b9569800a38af41a27ed893b12567757ef6c89/tools/infer/utility.py#L34&quot;&gt;utility.py&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;However while executing the code, we found that only 20 parameters were useful in our refactored code:&lt;/p&gt;

&lt;p&gt;When rewriting the code, we cleaned the parameter list, leaving only the relevant parameters.&lt;/p&gt;

&lt;h3 id=&quot;parameter-impact-on-prediction&quot;&gt;&lt;em&gt;Parameter impact on prediction&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;Some of the parameter definitions and effect they would have when changed from default, were not clear to us. We built a &lt;a href=&quot;https://streamlit.io/&quot;&gt;Streamlit app&lt;/a&gt; to visualise the changes in params on the predictions. For instance, “unclip ratio” would impact the size of the box, and “threshold” would detect two bounding boxes instead of one. We advise you to play with your own data and model to see how different parameters affect the detection. Overall, we were not able to see a major improvement from changing defaults.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*B4uqn-7vcxfu5aPz&quot; alt=&quot;The illustration of PaddleOCR parameters impact on the machine learning model prediction&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;h3 id=&quot;language-choice&quot;&gt;&lt;em&gt;Language choice&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;Normally in our role, we work with “‘PP-OCRv3”, the most recent version of the framework. As we are dealing with European languages, we would choose “fr”, “en”, “es” as the “lang” param, thinking that this means different models are being called. However, while looking into the paddleocr.py, we saw how the languages are interpreted:&lt;/p&gt;

&lt;p&gt;The first definition serves to define the recognition model name/path. But if we typed “fr” or “es”, it becomes lang = “latin”, yet “en” remains “en”. Then another simplification happens for the detection model.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;if lang in [“en”, “latin”]:&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;det_lang = “en”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;We are left with an English detection model and a Latin recognition model for any European language written with Latin characters except English, which has its own recognition model.&lt;/p&gt;

&lt;h3 id=&quot;downloading-models&quot;&gt;&lt;em&gt;Downloading models&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;Based on the language parameter and framework version, the first time we call the PaddleOCR class with those parameters, the model will be downloaded from the url encoded in paddleocr.py.&lt;/p&gt;

&lt;p&gt;Firstly, this could cause some issues when running the code in secure or offline environments.&lt;/p&gt;

&lt;p&gt;Secondly, we found inconsistencies between the model urls in the paddleocr.py and the models provided in the dedicated &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/blob/18ddb6d5f9bdc2c1b0aa7f6e399ec0f76119dc87/doc/doc_en/models_list_en.md&quot;&gt;documentation page&lt;/a&gt;. For instance, “en_PP-OCRv3_det_slim” is not an option when models are downloaded by the paddleocr.py script. In order to use some of the models from Model Zoo, a database of pre-trained models and code, you would need to download the model and provide the path to it manually.&lt;/p&gt;

&lt;p&gt;In order to remove this ambiguity and use the specific model we needed, we decided to pre-download the chosen model, then provide the path directly. In the original code, it is possible to provide det_model_dir, cls_model_dir and rec_model_dir. The language param will then be ignored and any pre-trained model with the accepted backbones can be used. After this process, we removed the model download functionality from our code.&lt;/p&gt;

&lt;h3 id=&quot;using-onnxmodels&quot;&gt;&lt;em&gt;Using ONNX models&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;PaddleOCR provides a &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/blob/18ddb6d5f9bdc2c1b0aa7f6e399ec0f76119dc87/deploy/paddle2onnx/readme.md&quot;&gt;handy way&lt;/a&gt; to export models to the &lt;a href=&quot;https://onnx.ai/&quot;&gt;ONNX framework&lt;/a&gt; that can serve or integrate in different pipelines. We exported the pre-trained models using PaddleOCR instructions. In the PaddleOCR class, there is a parameter “use_onnx”. If one sets “use_onnx” and provides a direct path to the ONNX models to PaddleOCR(), the model would use the ONNX model for prediction. However, there is a small bug that occurs while running ONNX with GPUs, described further in this &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/issues/8688&quot;&gt;issue&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We applied the modification suggested and tested the code with ONNX models, obtaining satisfactory results on both CPU and GPU (even though we noticed small numerical differences between the Paddle and ONNX model versions).&lt;/p&gt;

&lt;h3 id=&quot;documentation&quot;&gt;&lt;em&gt;Documentation&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;If you look at the &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/tree/18ddb6d5f9bdc2c1b0aa7f6e399ec0f76119dc87/doc&quot;&gt;documentation pages&lt;/a&gt;, you will find a lot of resources in both English and Chinese. However, when looking at &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/issues&quot;&gt;Issues&lt;/a&gt;, you will find most of them are in Chinese, Japanese or Korean. The same applies to blog posts and community resources online. We also found that some documentation is only partially translated to English and the Chinese version contains much more detail.&lt;/p&gt;

&lt;p&gt;We did not find a solution for this. We made sure to always check both the English and Chinese documentation (translated to English by an automatic translator) to ensure that we have all the possible information.&lt;/p&gt;

&lt;h3 id=&quot;tests--pylint-typing&quot;&gt;&lt;em&gt;Tests &amp;amp;&lt;/em&gt; &lt;a href=&quot;https://pylint.pycqa.org/en/latest/&quot;&gt;&lt;em&gt;pylint&lt;/em&gt;&lt;/a&gt; &lt;em&gt;&amp;amp;&lt;/em&gt; &lt;a href=&quot;https://docs.python.org/3/library/typing.html&quot;&gt;&lt;em&gt;typing&lt;/em&gt;&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;In general, as the original code is not modular, it was not tested according to the standards of our team. Once we cleaned and simplified the code, we worked on linting and variable typing. Our next step will be to write meaningful unit tests to secure the code base.&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;PaddleOCR is a powerful and optimised library for the extraction of text from images. However, we found that the code doesn’t fit the standards of our team as it is too complex to maintain and understand. In this article, we pointed out some of the pain points for us that other PaddleOCR users may experience when working with this framework. The fixes we proposed made our lives easier and the code more transparent for any team member and the wider community, without compromising the speed or the original model accuracy.&lt;/p&gt;

&lt;footer&gt;
  &lt;p&gt;Exported from &lt;a href=&quot;https://medium.com&quot;&gt;Medium&lt;/a&gt; on June 06,
    2023.&lt;/p&gt;
  &lt;p&gt;&lt;a href=&quot;https://medium.com/adevinta-tech-blog/deep-dive-in-paddleocr-inference-e86f618a0937&quot;&gt;View
      the original. This article was orignally co-authored by Cognition team members, special credits to Joaquin Cabezas&lt;/a&gt;&lt;/p&gt;
&lt;/footer&gt;
&lt;script type=&quot;text/javascript&quot; src=&quot;//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-584ec4ce89deed84&quot;&gt;&lt;/script&gt;

&lt;div class=&quot;addthis_inline_share_toolbox&quot;&gt;&lt;/div&gt;

</description>
        <pubDate>Mon, 06 Mar 2023 00:00:00 +0100</pubDate>
        <link>http://urszulaczerwinska.github.io/works/deep-dive-in-paddleocr-inference</link>
        <guid isPermaLink="true">http://urszulaczerwinska.github.io/works/deep-dive-in-paddleocr-inference</guid>
        
        <category>data science</category>
        
        <category>OCR</category>
        
        <category>deep learning</category>
        
        <category>computer vision</category>
        
        <category>machine learning</category>
        
        <category>featured</category>
        
        
        <category>works</category>
        
      </item>
    
      <item>
        <title>Text in Image 2.0 - improving OCR service with PaddleOCR</title>
        <description>&lt;head&gt;
  &lt;meta name=&quot;description&quot; content=&quot;Discover how the Cognition team at Adevinta enhanced the Text in Image service using PaddleOCR, leading to significant improvements in OCR accuracy and performance.&quot; /&gt;
&lt;/head&gt;

&lt;h2 id=&quot;understanding-ocr-what-is-optical-character-recognition&quot;&gt;Understanding OCR: What is Optical Character Recognition?&lt;/h2&gt;

&lt;p&gt;Optical Character Recognition (OCR) is a popular topic for both industry and personal use. In this article, we share how we tested and used an existing open source library, PaddleOCR, to extract text from an image. This read is for anyone who would like to find out more about OCR, the needs of our customers at &lt;a href=&quot;https://www.adevinta.com/&quot;&gt;Adevinta&lt;/a&gt;, and the challenges we face in attending to them. You’ll find out how we upgraded an existing service, benchmarked different solutions and delivered the selected one to satisfy our customers.&lt;/p&gt;

&lt;h2 id=&quot;key-ocr-applications-how-ocr-transforms-business-and-daily-operations&quot;&gt;Key OCR applications: How OCR transforms business and daily operations&lt;/h2&gt;

&lt;p&gt;OCR stands for “Optical Character Recognition” and is a technology that allows computers to recognise and extract text from images and scanned documents. OCR software uses optical recognition algorithms to interpret the text in images and convert it into machine-readable text that can be edited, searched and stored electronically.&lt;/p&gt;

&lt;p&gt;There are numerous use-cases where OCR can be used:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Digitising paper documents&lt;/strong&gt;: to convert scanned images of text into digital text. This is useful for organisations that want to reduce their reliance on paper and improve their document management processes.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Extracting data from images&lt;/strong&gt;: eg from documents such as invoices, receipts and forms. This can be useful for automating data entry tasks and reducing the need for manual data entry.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Translating documents&lt;/strong&gt;: to extract text from images of documents written in foreign languages and translate them into a different language.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Archiving&lt;/strong&gt;: to create digital copies of important documents that need to be preserved for long periods of time.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Improving accessibility&lt;/strong&gt;: to make scanned documents more accessible to people with disabilities by converting the text into a format that can be read by assistive technologies such as screen readers.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Searching documents&lt;/strong&gt;: to make scanned documents searchable, allowing users to easily find specific information within a large collection of documents.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;the-adevinta-context-why-ocr-matters-in-global-marketplace&quot;&gt;The Adevinta context: Why OCR matters in global marketplace&lt;/h2&gt;

&lt;p&gt;Within &lt;a href=&quot;https://www.adevinta.com/&quot;&gt;Adevinta&lt;/a&gt;, a global classifieds specialist with market-leading positions in key European markets, there is space for all of the cited use cases. However, for this article, we focus specifically on “extracting data from images.”&lt;/p&gt;

&lt;p&gt;Applying deep learning to images is the main expertise of our team, Cognition. We are Data Scientists and Machine Learning (ML) Engineers that work together to develop image-based ML solutions at scale, helping Adevinta’s marketplaces build better products and experiences for their customers. Adevinta’s mission is to connect buyers and sellers, enabling people to find jobs, homes, cars, consumer goods and more. By making an accessible ML API with features tailored to our different marketplaces’ needs, Adevinta’s marketplaces are empowered with ML tools at a reasonable cost.&lt;/p&gt;

&lt;h2 id=&quot;text-extraction-in-images-why-its-crucial-for-adevintas-services&quot;&gt;Text Extraction in Images: Why It’s Crucial for Adevinta’s Services&lt;/h2&gt;
&lt;p&gt;Text extraction from images enables us to:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Detect unwanted content in ads (e.g., insults, hidden messages).&lt;/li&gt;
  &lt;li&gt;Better understand image content to improve search capabilities.&lt;/li&gt;
  &lt;li&gt;Support more efficient searches using visible text on items.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With over 100 million requests per month and growing, our existing Text in Image service was ripe for enhancement. We aimed to improve accuracy and performance, leading to the development of Text in Image 2.0.&lt;/p&gt;

&lt;h2 id=&quot;why-we-chose-paddleocr-benchmarking-the-best-ocr-solution&quot;&gt;Why we chose PaddleOCR: Benchmarking the best OCR solution&lt;/h2&gt;

&lt;p&gt;The existing service was based on &lt;a href=&quot;https://arxiv.org/abs/1801.01671&quot;&gt;Fast Oriented Text Spotting with a Unified Network (Yan et al., 2018)&lt;/a&gt;. Despite being state of the art in 2018, the algorithm achieved 0.4 accuracy on our internal benchmark of 200 marketplace images. Nevertheless, accuracy was not the sole criteria of choice for the Text in Image 2.0, so we compiled a list of edge cases where our partner marketplaces require high-performing algorithms.&lt;/p&gt;

&lt;p&gt;After reviewing different open source OCR frameworks (including &lt;a href=&quot;https://github.com/open-mmlab/mmocr&quot;&gt;MMOCR&lt;/a&gt;, &lt;a href=&quot;https://github.com/JaidedAI/EasyOCR&quot;&gt;EASY OCR&lt;/a&gt;, &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR&quot;&gt;PaddleOCR&lt;/a&gt; and &lt;a href=&quot;https://thehive.ai/apis/ocr&quot;&gt;HiveOCR&lt;/a&gt;) and different combinations of proposed models on our internal benchmark and on the edge cases, a indisputable winner was PaddleOCR with an average accuracy of 0.8 and an acceptable performance on our edge cases. This result competes with the paid &lt;a href=&quot;https://cloud.google.com/vision/docs/ocr&quot;&gt;Google Cloud Vision OCR API&lt;/a&gt; on the best accuracy we measured.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*UUEf-TKs1Lfn7_wx&quot; alt=&quot;Graph showing benchmark results for various OCR frameworks&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;h2 id=&quot;how-we-validated-paddleocr-building-a-comprehensive-benchmark&quot;&gt;How We Validated PaddleOCR: Building a Comprehensive Benchmark&lt;/h2&gt;

&lt;p&gt;In order to construct our independent benchmark and validate the choice of PaddleOCR at scale, we built a “Text in Image generator” that uses open source images from &lt;a href=&quot;https://unsplash.com/license&quot;&gt;Unsplash&lt;/a&gt; and &lt;a href=&quot;https://pikwizard.com/free-license&quot;&gt;Pikwizard&lt;/a&gt; and adds randomly generated text on top of them. The created tool is highly customisable in order to simulate a wide variety of cases that combine factors such as font type, rotation, text length, background type, image resolution etc. Using a simulated benchmark of 20k images with a distribution of cases matching business needs, we obtained an improvement factor of x1.4.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*sWpBlrJtdxsRlqj4&quot; alt=&quot;Sample of Text in Image generator output showing simulated text scenarios&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;h2 id=&quot;challenges-with-paddleocr-identifying-and-mitigating-issues&quot;&gt;Challenges with PaddleOCR: Identifying and mitigating issues&lt;/h2&gt;

&lt;p&gt;We identified several cases where PaddleOCR fails. This is mostly when there are different angles of rotated text, some alternative fonts and differing colour/contrast. We also observed that in some cases, the correct words are detected but the spaces between them are not placed correctly. This may or may not be an issue depending on the way the extracted text is used further.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*3CO2dWUYPpVPPBZJDpx4EA.png&quot; alt=&quot;Example of OCR results with incorrectly spaced text&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;h2 id=&quot;deep-dive-how-we-optimized-paddleocr-for-production&quot;&gt;Deep Dive: How We Optimized PaddleOCR for Production&lt;/h2&gt;

&lt;p&gt;In order to evaluate the potential for improvement and mitigation of these errors, in addition to defining the serving strategy, we had to deep dive into the PaddleOCR framework.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR&quot;&gt;PaddleOCR&lt;/a&gt; builds on &lt;a href=&quot;https://github.com/PaddlePaddle/Paddle&quot;&gt;PaddlePaddle.&lt;/a&gt; Our team had no previous experience with this and it’s less popular in our community than other frameworks such as Tensorflow, Keras or Pytorch.&lt;/p&gt;

&lt;p&gt;From a technical point of view, PaddleOCR is composed of three distinct models:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Detection&lt;/strong&gt;, for detecting a bounding box where possible text is&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Classification&lt;/strong&gt;, rotating the text 180° if necessary&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Recognition&lt;/strong&gt;, translating the detected image frame to raw text&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Pre-trained models in different languages are &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/blob/18ddb6d5f9bdc2c1b0aa7f6e399ec0f76119dc87/doc/doc_en/models_list_en.md&quot;&gt;provided by authors&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;refactoring-paddleocr-creating-a-clean-production-ready-codebase&quot;&gt;Refactoring PaddleOCR: Creating a Clean, Production-Ready Codebase&lt;/h3&gt;

&lt;p&gt;Whilst exploring the code base of PaddleOCR for inference, we were faced with convoluted code, which was difficult to read and understand. As we wanted to use the PaddleOCR solution in production, we decided to refactor the code, keeping in mind to preserve the performance and the speed of the original code. You can read about the details of that process and the PaddleOCR model in the complementary article of this series. After refactoring the code, we had created a clean and readable code base.&lt;/p&gt;

&lt;p&gt;We believe our code version is easier to work with, given the use case of text extraction from images, and are working on making the code available open source. The different steps and pre-processing and post-processing parts are clearly separated, so they can be called independently, which should make further community extensions easier to add. It also makes putting into production easier, as the simplified, modular code combines well with the structure of inference.py for serving SageMaker endpoints. Our proposed code version does not alter predictions (compared to the 2.6 release) for images.&lt;/p&gt;

&lt;h2 id=&quot;deploying-text-in-image-20-achieving-superior-performance-with-paddleocr&quot;&gt;Deploying Text in Image 2.0: Achieving Superior Performance with PaddleOCR&lt;/h2&gt;

&lt;p&gt;Using the refactored code, we made the model available as an API. To help our customers’ transition, we maintained the same API contract used in the previous service.&lt;/p&gt;

&lt;p&gt;Serving PaddleOCR can be done in multiple ways. The straightforward approach is calling its own Python API (provided by the &lt;a href=&quot;https://pypi.org/project/paddleocr/&quot;&gt;PaddleOCR&lt;/a&gt; package) from within a well-known framework. We selected Multi Model Server, Flask and FastAPI to conduct our benchmark. All our proposed solutions are served by AWS SageMaker Endpoint, building our own container (BYOC) from the same Docker base image.&lt;/p&gt;

&lt;p&gt;MultiModel Server uses its own JAVA ModelServer, while for Flask and FastAPI, we use nginx+gunicorn (combined with &lt;a href=&quot;https://fastapi.tiangolo.com/deployment/server-workers/&quot;&gt;uvicorn workers for the ASGI FastAPI&lt;/a&gt;). The frontend for our customers is served by an API Gateway, which is out of the scope of this article.&lt;/p&gt;

&lt;h2 id=&quot;benchmarking-deployment-options-multi-model-server-flask-and-fastapi&quot;&gt;Benchmarking Deployment Options: Multi-Model Server, Flask, and FastAPI&lt;/h2&gt;

&lt;p&gt;For the performance testing, we recreated a number of requests with a controlled amount of text and different image sizes, mimicking the expected distribution from our customers. We used &lt;a href=&quot;https://locust.io/&quot;&gt;Locust&lt;/a&gt; as the testing framework, and stimulated heavy bursts in the &lt;a href=&quot;https://docs.locust.io/en/stable/writing-a-locustfile.html#wait-time-attribute&quot;&gt;waiting time&lt;/a&gt; as a stress test.&lt;/p&gt;

&lt;p&gt;With the data gathered from the performance tests, we were able to define our infrastructure (type of instance and autoscaling policy) in relation to the Service Level Agreement (SLA) terms, while balancing the risk of a sudden shift from the observed distribution (the service is sensitive to the amount of text per image).&lt;/p&gt;

&lt;p&gt;Currently, we deal with 330 million requests per month, and we have estimated that next year, more Adevinta marketplaces will onboard a Text in Image service, resulting in a 400% growth.&lt;/p&gt;

&lt;h2 id=&quot;results-and-impact-transforming-text-in-image-service-with-paddleocr&quot;&gt;Results and impact: Transforming Text in Image service with PaddleOCR&lt;/h2&gt;

&lt;p&gt;The new API resulted in an improved latency 7.5x compared to the FOTS-based solution, while providing a 7% cost reduction in serving. Also, since the new API being 12x cheaper than a typical external solution, such as GCP OCR, we received positive feedback from our users about both the speed and the accuracy of the Text in Image 2.0.&lt;/p&gt;

&lt;h2 id=&quot;key-takeaways-enhancing-ocr-with-paddleocr&quot;&gt;Key Takeaways: Enhancing OCR with PaddleOCR&lt;/h2&gt;

&lt;p&gt;As a computer vision team working for an international company serving millions of people every day, we aimed to improve our OCR API for text extraction from classified ads. After testing numerous frameworks, we built an image simulator in order to find the algorithm matching the needs of our users. The selected framework, PaddleOCR, went through our internal review and revamp. (There were challenges along the way and you can read more about them in &lt;a href=&quot;/works/deep-dive-in-paddleocr-inference&quot;&gt;&lt;strong&gt;Article 2: Deep Dive in PaddleOCR inference&lt;/strong&gt;&lt;/a&gt;). Now, we’re pleased to say we’re providing a more accurate, faster and cheaper API using the PaddleOCR framework.&lt;/p&gt;

&lt;footer&gt;
  &lt;p&gt;Exported from &lt;a href=&quot;https://medium.com&quot;&gt;Medium&lt;/a&gt; on June 06,
    2023.&lt;/p&gt;
  &lt;p&gt;&lt;a href=&quot;https://medium.com/adevinta-tech-blog/text-in-image-2-0-improving-ocr-service-with-paddleocr-61614c886f93&quot;&gt;This article was orignally co-authored by Cognition team members, special credits to Joaquin Cabezas&lt;/a&gt;&lt;/p&gt;
&lt;/footer&gt;
&lt;script type=&quot;text/javascript&quot; src=&quot;//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-584ec4ce89deed84&quot;&gt;&lt;/script&gt;

&lt;div class=&quot;addthis_inline_share_toolbox&quot;&gt;&lt;/div&gt;

</description>
        <pubDate>Mon, 06 Mar 2023 00:00:00 +0100</pubDate>
        <link>http://urszulaczerwinska.github.io/works/text-in-image-2-0-improving-ocr-service-with-paddleocr</link>
        <guid isPermaLink="true">http://urszulaczerwinska.github.io/works/text-in-image-2-0-improving-ocr-service-with-paddleocr</guid>
        
        <category>data science</category>
        
        <category>OCR</category>
        
        <category>deep learning</category>
        
        <category>computer vision</category>
        
        <category>machine learning</category>
        
        <category>API</category>
        
        
        <category>works</category>
        
      </item>
    
      <item>
        <title>Named Entity Recognition Tool by Cour de Cassation</title>
        <description>&lt;!DOCTYPE HTML&gt;
&lt;html lang=&quot;en-US&quot;&gt;
    &lt;head&gt;
        &lt;meta charset=&quot;UTF-8&quot;&gt;
        &lt;meta http-equiv=&quot;refresh&quot; content=&quot;0; url=https://github.com/Cour-de-cassation/moteurNER&quot;&gt;
        &lt;meta name=&quot;description&quot; content=&quot;Redirecting to the Named Entity Recognition (NER) tool repository by Cour de Cassation. It is a page about building deep learning NLP application for French justice&quot;&gt;
        &lt;script type=&quot;text/javascript&quot;&gt;
            window.location.href = &quot;https://github.com/Cour-de-cassation/moteurNER&quot;
        &lt;/script&gt;
        &lt;title&gt;Redirecting to Named Entity Recognition Tool by Cour de Cassation, building NER, NLP deep learning applications for French Supreme Court.&lt;/title&gt;
    &lt;/head&gt;
    &lt;body&gt;
        &lt;p&gt;You are being redirected to the Named Entity Recognition tool repository by Cour de Cassation. If you are not redirected automatically, &lt;a rel=&quot;canonical&quot; href=&apos;https://github.com/Cour-de-cassation/moteurNER&apos;&gt;click here to proceed to the repository.&lt;/a&gt;&lt;/p&gt;
    &lt;/body&gt;

      &lt;!-- Go to www.addthis.com/dashboard to customize your tools --&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-584ec4ce89deed84&quot;&gt;&lt;/script&gt;
&lt;!-- Go to www.addthis.com/dashboard to customize your tools --&gt; &lt;div class=&quot;addthis_inline_share_toolbox&quot;&gt;&lt;/div&gt;

&lt;/html&gt;</description>
        <pubDate>Wed, 01 Sep 2021 00:00:00 +0200</pubDate>
        <link>http://urszulaczerwinska.github.io/works/ner_cc</link>
        <guid isPermaLink="true">http://urszulaczerwinska.github.io/works/ner_cc</guid>
        
        <category>Machine Learning</category>
        
        <category>Python</category>
        
        <category>Deep Learning</category>
        
        <category>NER</category>
        
        <category>NLP</category>
        
        <category>Justice</category>
        
        <category>Flair</category>
        
        <category>featured</category>
        
        
        <category>works</category>
        
      </item>
    
      <item>
        <title>Push the limits of machine learning explainability</title>
        <description>&lt;head&gt;
  &lt;meta
    name=&quot;description&quot;
    content=&quot;Explore the power of SHAP in enhancing model interpretability in data science and machine learning. This guide provides detailed insights and practical examples for data professionals.&quot;
  /&gt;
&lt;/head&gt;

&lt;section&gt;
  &lt;h1&gt;
    Summary - A Comprehensive Guide to SHAP: Enhancing Machine Learning
    Interpretability
  &lt;/h1&gt;
  &lt;p&gt;
    This article is a guide to the advanced and lesser-known features of the
    python SHAP library. It is based on an example of tabular data
    classification.
  &lt;/p&gt;
  &lt;p&gt;
    But first, let’s talk about the motivation and interest in explainability at
    Saegus that motivated and financed my explorations.
  &lt;/p&gt;

  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img src=&quot;/images/engineering.jpg&quot; alt=&quot;&quot; width=&quot;70%&quot;
  /&gt;&lt;/span&gt;

  &lt;h3&gt;The Theory Behind Explainability in AI and Machine Learning&lt;/h3&gt;
  &lt;p&gt;
    The explainability of algorithms is taking more and more place in the
    discussions about Data Science. We know that algorithms are powerful, we
    know that they can assist us in many tasks: price prediction, document
    classification, video recommendation.
  &lt;/p&gt;
  &lt;p&gt;
    From now on, more and more questions are being asked about this
    prediction:&lt;br /&gt;- Is it ethical?&lt;br /&gt;- Is it affected by bias?&lt;br /&gt;- Is
    it used for the right reasons?
  &lt;/p&gt;
  &lt;p&gt;
    In many domains such as medicine, banking or insurance, algorithms can be
    used if, and only if, it is possible to trace and explain (or better,
    interpret) the decisions of these algorithms.
  &lt;/p&gt;
  &lt;h4&gt;Key Terminology in Machine Learning Interpretability and SHAP&lt;/h4&gt;
  &lt;p&gt;In this article we would like to distinguish the terms:&lt;/p&gt;
  &lt;p&gt;
    &lt;strong&gt;Explainability&lt;/strong&gt;: possibility to explain from a technical
    point of view the prediction of an algorithm.
  &lt;/p&gt;
  &lt;p&gt;
    &lt;strong&gt;Interpretability&lt;/strong&gt;: the ability to explain or provide meaning
    in terms that are understandable by a human being.
  &lt;/p&gt;
  &lt;p&gt;
    &lt;strong&gt;Transparency&lt;/strong&gt;: a model is considered transparent if it is
    understandable on its own.
  &lt;/p&gt;
  &lt;h4&gt;Why Explainability Matters in Data Science ?&lt;/h4&gt;
  &lt;p&gt;
    Interpretability helps to ensure impartiality in decision-making, i.e. to
    detect and therefore correct biases in the training data set. In addition,
    it facilitates robustness by highlighting potential adverse disturbances
    that could change the prediction. It can also act as an assurance that only
    significant features infer the outcome.
  &lt;/p&gt;
  &lt;p&gt;
    Sometimes, it would be more advisable to abandon the machine learning
    approach, and use deterministic algorithms based on rules justified by
    industry knowledge or legislation [1].
  &lt;/p&gt;
  &lt;p&gt;
    Nevertheless, it is too tempting to access the capabilities of machine
    learning algorithms that can offer high accuracy. We can talk about the
    trade-off between accuracy and explainability. This trade-off consists in
    discarding more complex models such as neural networks for simpler
    algorithms that can be explained.
  &lt;/p&gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img src=&quot;/images/model-interpret.png&quot; alt=&quot;&quot; /&gt;&lt;em
      &gt;As described in [2] relation between interpretability and accuracy of the
      model. For some models improvements can be made towards a more
      interpretable or more relevant model.&lt;/em
    &gt;&lt;/span
  &gt;
  &lt;p&gt;
    To achieve these goals, a new field has emerged: XAI (Explainable Artificial
    Intelligence), which aims to produce algorithms that are both powerful and
    explainable.
  &lt;/p&gt;
  &lt;p&gt;
    Many frameworks have been proposed to help explain non-transparent
    algorithms. A very good presentation of these methods can be found in the
    Cloudera white paper [3].
  &lt;/p&gt;
  &lt;p&gt;
    In this article we will deal with one of the most used frameworks: SHAP.
  &lt;/p&gt;
  &lt;h4&gt;Exploring the Audience for Explainable AI in Data Science&lt;/h4&gt;
  &lt;p&gt;
    Different profiles interested in expainability or interpretability have been
    identified:
  &lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;
      Business expert/model user — in order to trust the model, understand the
      causality of the prediction
    &lt;/li&gt;
    &lt;li&gt;
      Regulatory bodies to certify compliance with the legislation, auditing
    &lt;/li&gt;
    &lt;li&gt;
      Managers and executive board to assess regulatory compliance, understand
      enterprise AI applications
    &lt;/li&gt;
    &lt;li&gt;
      Users impacted by model decisions in order to understand the situation,
      verify decisions
    &lt;/li&gt;
    &lt;li&gt;
      Data scientist, developer, PO to ensure/improve product performance, find
      new features, explain functioning/predictions to superiors
    &lt;/li&gt;
  &lt;/ul&gt;
  &lt;p&gt;
    In order to make explainability accessible to people with low technical
    skills, first of all, the creator: a data scientist/developer must be
    comfortable with the tools of explainability.
  &lt;/p&gt;
  &lt;p&gt;
    The data scientist will use them above all to understand and improve his
    model and then to communicate with his superiors and regulatory bodies.&lt;br /&gt;Recently,
    explainability tools have become more and more accessible.
  &lt;/p&gt;
  &lt;p&gt;
    For example,
    &lt;a
      href=&quot;https://www.dataiku.com/&quot;
      data-href=&quot;https://www.dataiku.com/&quot;
      class=&quot;markup--anchor markup--p-anchor&quot;
      rel=&quot;noopener&quot;
      target=&quot;_blank&quot;
      &gt;Dataiku &lt;/a
    &gt;— ML’s platform — has added in its latest version 7.0 published on March 2,
    2020 explainability tools: Shapley values and “The Individual Conditional
    Expectation” (ICE).
  &lt;/p&gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img src=&quot;/images/dataiku.png&quot; alt=&quot;&quot; /&gt;&lt;em
      &gt;Dataiku prediction studio&lt;/em
    &gt;&lt;/span
  &gt;
  &lt;p&gt;
    &lt;a href=&quot;https://azure.microsoft.com/en-us/services/machine-learning/&quot;
      &gt;Azure ML&lt;/a
    &gt;
    proposes its own version of Shap and alternative tools adding interactive
    &lt;a
      href=&quot;https://docs.microsoft.com/en-us/azure/machine-learning/how-to-machine-learning-interpretability-aml&quot;
      &gt;dashboards&lt;/a
    &gt;.
  &lt;/p&gt;

  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img src=&quot;/images/azure.png&quot; alt=&quot;&quot; /&gt;&lt;em
      &gt;Azure ML interpretability dashboard&lt;/em
    &gt;&lt;/span
  &gt;

  &lt;p&gt;
    There are also open-source webapps such as this one described in the medium
    article [4] that facilitate the exploration of the SHAP library.
  &lt;/p&gt;
  &lt;div&gt;
    &lt;a
      href=&quot;https://towardsdatascience.com/understand-the-machine-learning-blackbox-with-ml-interpreter-7b0f9a2d8e9f&quot;
      &gt;&lt;strong
        &gt;Understand the machine learning Blackbox with ML interpreter&lt;/strong
      &gt;&lt;br /&gt;&lt;em class=&quot;markup--em markup--mixtapeEmbed-em&quot;
        &gt;There are dangers in having models running the world and making
        decisions from hiring to criminal justice&lt;/em
      &gt;towardsdatascience.com&lt;/a
    &gt;&lt;a
      href=&quot;https://towardsdatascience.com/understand-the-machine-learning-blackbox-with-ml-interpreter-7b0f9a2d8e9f&quot;
    &gt;&lt;/a&gt;
  &lt;/div&gt;
  &lt;div&gt;&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;
  &lt;p&gt;
    These tools, very interesting to get a quick overview of interpretation, do
    not necessarily give an understanding of the full potential of the SHAP
    library. Few allow to explore interaction values or to use different
    background or display sets.
  &lt;/p&gt;
  &lt;p&gt;
    I investigated the SHAP framework and I present you my remarks and the usage
    of less known features, available in the official version of the library in
    open source. I also propose some interactive visualizations easy to
    integrate in your projects.
  &lt;/p&gt;
  &lt;h3&gt;Step-by-Step Guide: Using SHAP for Machine Learning Models&lt;/h3&gt;
  &lt;p&gt;
    Most data scientists have already heard of the SHAP framework.&lt;br /&gt;In this
    post, we won’t explain in detail how the calculations behind the library are
    done. Many resources are available online such as the SHAP documentation
    [5], publications by authors of the library [6,7], the great book
    “Interpretable Machine Learning” [8] and multiple medium articles [9,10,11].
  &lt;/p&gt;
  &lt;p&gt;
    In summary, Shapley’s values calculate the importance of a feature by
    comparing what a model predicts with and without this feature. However,
    since the order in which a model sees the features can affect its
    predictions, this is done in all possible ways, so that the features are
    compared fairly. This approach is inspired by game theory.
  &lt;/p&gt;
  &lt;p&gt;
    Having worked with many clients, for example in the banking and insurance
    sectors, one can see that their data scientists are struggling to exploit
    the full potential of SHAP. They don’t know how this tool could really be
    useful for understanding a model and how to use it to go beyond simply
    extracting the importance of features.
  &lt;/p&gt;
  &lt;blockquote&gt;The devil is in the detail&lt;/blockquote&gt;
  &lt;p&gt;
    SHAP comes with a set of visualizations that are quite complex and not
    always intuitive, even for a data scientist.
  &lt;/p&gt;
  &lt;p&gt;
    On top of that, there are several technical nuances to be able to use SHAP
    with your data. Francesco Porchetti’s blog article [12] expresses some of
    these frustrations by exploring the SHAP,
    &lt;a href=&quot;https://github.com/marcotcr/lime&quot;&gt;LIME&lt;/a&gt;,
    &lt;a
      href=&quot;https://github.com/SauceCat/PDPbox&quot;
      data-href=&quot;https://github.com/SauceCat/PDPbox&quot;
      &gt;PDPbox &lt;/a
    &gt;(PDP and ICE) and
    &lt;a
      href=&quot;https://eli5.readthedocs.io/en/latest/autodocs/sklearn.html#module-eli5.sklearn.permutation_importance&quot;
      &gt;ELI5 &lt;/a
    &gt;libraries.
  &lt;/p&gt;
  &lt;p&gt;
    &lt;strong
      &gt;&lt;em
        &gt;At Saegus, I worked on a course which aims to give more clarity to the
        SHAP framework and to facilitate the use of this tool.&lt;/em
      &gt;&lt;/strong
    &gt;
  &lt;/p&gt;
  &lt;p&gt;
    In this post I would like to share with you some observations collected
    during that process.
  &lt;/p&gt;
  &lt;p&gt;
    SHAP is used to explain an existing model. Taking a binary classification
    case built with a sklearn model. We train, tune and test our model. Then we
    can use our data and the model to create an additional SHAP model that
    explains our classification model.
  &lt;/p&gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/shap.png&quot;
      alt=&quot;SHAP plot demonstrating deep learning AI model interpretability in data science.&quot;
    /&gt;&lt;em&gt;Image source: SHAP github&lt;/em&gt;&lt;/span
  &gt;
  &lt;h4&gt;Vocabulary&lt;/h4&gt;
  &lt;p&gt;
    It is important to understand all the bricks that make up a SHAP
    explanation.
  &lt;/p&gt;
  &lt;p&gt;
    Often, by using default values for parameters, the complexity of the choices
    we make remains obscure.
  &lt;/p&gt;
  &lt;p&gt;
    &lt;strong&gt;global explanations&lt;br /&gt;&lt;/strong&gt;explanations of how the model
    works from a general point of view
  &lt;/p&gt;
  &lt;p&gt;
    &lt;strong&gt;local explanations&lt;/strong&gt;&lt;br /&gt;explanations of the model for a
    sample (a data point)
  &lt;/p&gt;
  &lt;p&gt;
    &lt;strong&gt;explainer &lt;/strong&gt;(shap.explainer_type(params))&lt;br /&gt;type of
    explainability algorithm to be chosen according to the model used.
  &lt;/p&gt;
  &lt;p&gt;
    The parameters are different for each type of model. Usually, the model and
    training data must be provided, at a minimum.
  &lt;/p&gt;
  &lt;p&gt;
    &lt;strong&gt;base value&lt;/strong&gt; (explainer.expected_value)&lt;br /&gt;&lt;em
      &gt;E(y_hat)&lt;/em
    &gt;
    is “the value that would be predicted if we didn’t know any features of the
    current output” is the &lt;em&gt;mean(y_hat)&lt;/em&gt; prediction for the training data
    set or the background set. We can call it “reference value”, it’s a scalar
    (&lt;em&gt;n&lt;/em&gt;).
  &lt;/p&gt;
  &lt;p&gt;
    It’s important to choose your background set carefully — if we have the
    unbalanced training set this will result in a base value placed among the
    majority of samples. This can also be a desired effect: for example if for a
    bank loan we want to answer the question: “how is the customer in question
    different from customers who have been approved for the loan” or “how is my
    false positive different from the true positives”.
  &lt;/p&gt;
  &lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt; &lt;span class=&quot;c1&quot;&gt;# equilibrated case background = X.sample(1000) #X is
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;equilibrated&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# background used in explainer defines base value explainer =
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;shap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;TreeExplainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xgb_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model_output&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;raw&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shap_values&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;explainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;shap_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# background used in the plot, the points that are
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;visible&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;on&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;summary_plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shap_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;feature_names&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt; &lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt; &lt;span class=&quot;c1&quot;&gt;#
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;base&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shifted&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;class1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;class1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,:]&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#X is equilibrated # background
&lt;/span&gt;  &lt;span class=&quot;k&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;1 &lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;used&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;explainer&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;defines&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;base&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;explainer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;shap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;TreeExplainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xgb_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;class1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model_output&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;raw&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shap_values&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;explainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;shap_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# points from class 0 is used in the plot, the points
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;that&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;are&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;visible&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;on&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;summary_plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shap_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;class0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,:],&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;feature_names&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img src=&quot;/images/background-shap.png&quot; alt=&quot;&quot; /&gt;&lt;em
      &gt;Selecting the background dataset changes the question answered by
      shap values.&lt;/em
    &gt;&lt;/span
  &gt;
  &lt;p&gt;
    &lt;strong&gt;SHAPley values&lt;/strong&gt; (explainer.shap_values(x))&lt;br /&gt;the average
    contribution of each feature to each prediction for each sample based on all
    possible features. It is a (&lt;em&gt;n,m&lt;/em&gt;) &lt;em&gt;n &lt;/em&gt;— samples,
    &lt;em&gt;m &lt;/em&gt;— features matrix that represents the contribution of each
    feature to each sample.
  &lt;/p&gt;
  &lt;p&gt;
    &lt;strong&gt;output value&lt;/strong&gt; (for a sample)&lt;br /&gt;the value predicted by the
    algorithm (the probability, logit or raw output values of the model)
  &lt;/p&gt;
  &lt;p&gt;
    &lt;strong&gt;display features&lt;/strong&gt; (&lt;em&gt;n &lt;/em&gt;x &lt;em&gt;m&lt;/em&gt;)&lt;br /&gt;a matrix of
    original values — before transformation/encoding/engineering of features
    etc. — that can be provided to some graphs to improve interpretation. Often
    overlooked and essential for interpretation.
  &lt;/p&gt;
  &lt;p&gt;____&lt;/p&gt;
  &lt;p&gt;&lt;strong&gt;SHAPley values&lt;/strong&gt;&lt;/p&gt;
  &lt;p&gt;
    Shapley values remain the central element. Once we realize that this is
    simply a matrix with the same dimensions as our input data and that we can
    analyze it in different ways to explain the model and not only. We can
    reduce its dimensions, we can cluster it, we can use it to create new
    features. An interesting exploration described in the article [12] aims at
    improving anomaly detection using auto encoders and SHAP. The SHAP library
    proposes a rich but not exchaustive exploration through visualizations.
  &lt;/p&gt;
  &lt;h4&gt;Visualizing SHAP: Enhancing Interpretability in Deep Learning Models&lt;/h4&gt;
  &lt;p&gt;
    The SHAP library offers different visualizations. A good explanation on how
    to read the colors of the summary plot can be found in this medium article
    [14].
  &lt;/p&gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img src=&quot;/images/shap-global.png&quot; alt=&quot;&quot; /&gt;&lt;em
      &gt;A summary of graphical visualizations to analyze global explanations&lt;/em
    &gt;&lt;/span
  &gt;
  &lt;p&gt;
    &lt;strong&gt;The summary plot&lt;/strong&gt; shows the most important features and the
    magnitude of their impact on the model. It can take several graphical forms
    and for the models explained by TreeExplainer we can also observe the&lt;strong
      &gt;&lt;em&gt; interaction values &lt;/em&gt;&lt;/strong
    &gt;using the “compact dot” with shap_interaction_values in input.
  &lt;/p&gt;
  &lt;p&gt;
    &lt;strong&gt;The dependency plot&lt;/strong&gt; allows to analyze the features two by
    two by suggesting a possibility to observe the interactions. The scatter
    plot represents a dependency between a feature(x) and the shapley values (y)
    colored by a second feature(hue).
  &lt;/p&gt;
  &lt;p&gt;
    On a personal note, I find that an observation of a three-factor
    relationship at the same time is not intuitive for the human brain (at least
    mine). I also doubt that an observation of dependency by observing colours
    can be scientifically accurate. Shap can give us an interaction relationship
    that is calculated as a correlation between the shapley values of the first
    feature and the values of the second feature. If possible (for
    TreeExplainer) it makes more sense to use the shapley interaction values to
    observe interactions.
  &lt;/p&gt;
  &lt;figure&gt;
    &lt;script src=&quot;https://gist.github.com/UrszulaCzerwinska/b7853f5f0b209f2f89e2a3590dd6f329.js&quot;&gt;&lt;/script&gt;
    &lt;figcaption class=&quot;imageCaption&quot;&gt;
      Snippet code to reproduce my dependence plot variant.
    &lt;/figcaption&gt;
  &lt;/figure&gt;
  &lt;p&gt;
    I propose an interactive variant of dependency plot that allows to observe
    the relationship between a feature(x), the shapley values (y) and the
    prediction (histogram colors). What seems important to me in this version is
    the possibility to display on the graph the original values (Income in k
    USD) instead of the normalized space used by the model.
  &lt;/p&gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/my-dep-plot.png&quot;
      alt=&quot;Histogram visualizing SHAP interaction values in a data science model analysis.&quot;
    /&gt;&lt;em&gt;My variant of dependence plot&lt;/em&gt;&lt;/span
  &gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/shap-local.png&quot;
      alt=&quot;Histogram visualizing SHAP interaction values in a data science model analysis.&quot;
    /&gt;&lt;em
      &gt;a summary of graphical visualizations to analyze local explanations&lt;/em
    &gt;&lt;/span
  &gt;
  &lt;p&gt;
    There are three alternatives for the visualization of explanations of a
    sample: force plot, decision plot and waterfall plot.
  &lt;/p&gt;
  &lt;p&gt;
    For a sample, these three representations are redundant, they represent the
    information in a very similar way. At the same time, some elements of these
    graphs are complementary. By putting the three side by side, I have the
    impression to understand the result in a more intuitive way. The force plot
    is good to see where the “output value” fits in relation to the “base
    value”. We also see which features have a positive (red) or negative (blue)
    impact on the prediction and the magnitude of this impact. The water plot
    also allows us to see the amplitude and the nature of the impact of a
    feature with its quantification. It also allows to see the order of
    importance of the features and the values taken by each feature for the
    studied sample. The Decision plot makes it possible to observe the amplitude
    of each change, “a trajectory” taken by a samplefor the values of the
    displayed features.
  &lt;/p&gt;
  &lt;p&gt;
    By using force plot and decision plot we can represent several samples at
    the same time.
  &lt;/p&gt;
  &lt;p&gt;
    The force plot for a set of samples can be compared to the last level of a
    dendrogram. The samples are grouped by similarity or by selected feature. In
    my opinion, this graph is difficult to read for a random sample. It is much
    more meaningful if we represent the contrasting cases or with a hypothesis
    behind.
  &lt;/p&gt;
  &lt;p&gt;
    The decision plot, for a set of samples, quickly becomes cumbersome if we
    select too many samples. It is very useful to observe a ‘trajectory
    deviation’ or ‘diverging/converging trajectories’ of a limited group of
    samples.
  &lt;/p&gt;
  &lt;h4&gt;
    Explainers in SHAP: Understanding Different Model Interpretability
    Approaches
  &lt;/h4&gt;
  &lt;p&gt;
    Explainers are the models used to calculate shapley values. The diagram
    below shows different types of Explainers.
  &lt;/p&gt;
  &lt;p&gt;
    The choice of Explainers depends mainly on the selected learning model. For
    linear models, the “Linear Explainer” is used, for decision trees and “set”
    type models — “TreeExplainer”. “Kernel Explainer” is slower than the above
    mentioned explainers.
  &lt;/p&gt;
  &lt;p&gt;
    In addition the “Tree Explainer” allows to display the interaction values
    (see next section). It also allows to transform the model output into
    probabilities or logloss, which is useful for a better understanding of the
    model or to compare several models.
  &lt;/p&gt;
  &lt;p&gt;
    The Kernel Explainer creates a model that substitutes the closest to our
    model. Kernel Explainer can be used to explain neural networks. For deep
    learning models, there are the Deep Explainer and the Grandient Explainer.
    For this paper we have not investigated the explainability of neural
    networks.
  &lt;/p&gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/explainer.png&quot;
      alt=&quot;Force plot visualization of SHAP values enhancing transparency in AI model predictions; Waterfall plot depicting SHAP value contributions to machine learning predictions.&quot;
    /&gt;&lt;em&gt;a summary of Explainer types in the SHAP library&lt;/em&gt;&lt;/span
  &gt;
  &lt;h4&gt;Shapley values of interactions&lt;/h4&gt;
  &lt;p&gt;
    One of the properties that allows to go further in the analysis of a model
    that can be explained with the “Tree Explainer” is the calculation of
    shapley values of interactions.
  &lt;/p&gt;
  &lt;p&gt;
    These values make it possible to quantify the impact of an interaction
    between two features on the prediction for each sample. As the matrix of
    shapley values has two dimensions (samples x features), the interactions are
    a tensor with three dimensions (samples x features x features).
  &lt;/p&gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/interactions-1.png&quot;
      alt=&quot;Interactive SHAP dependence plot showing feature impact in deep learning models.&quot;
    /&gt;&lt;em
      &gt;a summary of graphical visualizations to analyze local explanations&lt;/em
    &gt;&lt;/span
  &gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/interactions-2.png&quot;
      alt=&quot;Interactive SHAP dependence plot showing feature impact in deep learning models.&quot;
    /&gt;&lt;em
      &gt;a summary of graphical visualizations to analyze local explanations&lt;/em
    &gt;&lt;/span
  &gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/interactions-3.png&quot;
      alt=&quot;Interactive SHAP dependence plot showing feature impact in deep learning models.&quot;
    /&gt;&lt;em
      &gt;a summary of graphical visualizations to analyze local explanations&lt;/em
    &gt;&lt;/span
  &gt;

  &lt;p&gt;
    &lt;strong
      &gt;Here’s how interaction values help interpret a binary classification
      model.&lt;/strong
    &gt;
  &lt;/p&gt;
  &lt;p&gt;
    I used a Kaggle [15] dataset that represents a client base and the binary
    dependent feature: did the client accept the personal loan? NO/YES (0/1).
  &lt;/p&gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/table.png&quot;
      alt=&quot;A summary of SHAP graphical
      visualizations to analyze local explanations of machine learning model&quot;
    /&gt;&lt;em
      &gt;a summary of graphical visualizations to analyze local explanations&lt;/em
    &gt;&lt;/span
  &gt;

  &lt;p&gt;
    I’ve trained several models, including an xgboost model that we treated with
    the Tree Explainer.
  &lt;/p&gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/sklearn-model.png&quot;
      alt=&quot;A summary of SHAP graphical
        visualizations to analyze local explanations of machine learning model&quot;
    /&gt;&lt;em
      &gt;a summary of graphical visualizations to analyze local explanations&lt;/em
    &gt;&lt;/span
  &gt;

  &lt;p&gt;The background dataset was balanced and represented 40% of the dataset.&lt;/p&gt;
  &lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt; &lt;span class=&quot;c1&quot;&gt;# xgb - traned model # X_background - background
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;explainer_raw&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;TreeExplainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xgb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;model_output&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;raw&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;feature_perturbation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;tree_path_dependent&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# project
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;point&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;background&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datasetshap_values&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;explainer_raw&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;shap_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# obtain interaction values
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;shap_interaction_values&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;explainer_raw&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;shap_interaction_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;# dimensions shap_values.shape &amp;gt;&amp;gt;&amp;gt;(2543, 16) shap_interaction_values.shape
&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2543&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;summary_plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shap_interaction_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;plot_type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;compact_dot&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/summary-plot.png&quot;
      alt=&quot;Summary plot showcasing SHAP values and feature interactions in machine learning models.&quot;
    /&gt;&lt;em&gt;Summary plot with interactions&lt;/em&gt;&lt;/span
  &gt;

  &lt;p&gt;To better explore interactions, a heatmap can be very useful.&lt;/p&gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/histogram.png&quot;
      alt=&quot;Heatmap illustrating SHAP values and feature interactions in machine learning models.&quot;
    /&gt;&lt;em&gt;Histogram of interaction values&lt;/em&gt;&lt;/span
  &gt;
  &lt;p&gt;
    In the Summary_plot one can observe the importance of features and the
    importance the interactions. The interactions appear in double which
    confuses a little the reading.
  &lt;/p&gt;
  &lt;p&gt;
    In the histogram, we observe directly the interactions. The strongests of
    them of being: Income-Education, Income — Family, Income — CCAvg and
    Family-Education, Income-Age.
  &lt;/p&gt;
  &lt;p&gt;
    Then I investigated the interactions two by two.&lt;br /&gt;To understand the
    difference between a dependency_plot and a dependency_plot of interactions
    here are the two:
  &lt;/p&gt;

  &lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt; &lt;span class=&quot;c1&quot;&gt;# dependence_plot classique shap.dependence_plot(&quot;Age&quot;,
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;shap_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;features&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;display_features&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_background_display&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;interaction_index&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Income&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/dep-plot.png&quot;
      alt=&quot;SHAP dependency plot illustrating interaction effects in machine learning algorithm&quot;
  /&gt;&lt;/span&gt;
  &lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt; &lt;span class=&quot;c1&quot;&gt;# dependence_plot des interactions
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;shap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;dependence_plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Age&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Income&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shap_interaction_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;features&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;X_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;display_features&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_background_display&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/interaction-dep-plot.png&quot;
      alt=&quot;SHAP dependency plot illustrating interaction effects in machine learning algorithm&quot;
  /&gt;&lt;/span&gt;

  &lt;p&gt;
    Even when using the ‘display_features’ parameter, the Age and Income values
    are displayed in the transformed space.
  &lt;/p&gt;
  &lt;p&gt;
    For this reasons I offer an interactive version, which displays the
    non-transformed values.
  &lt;/p&gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/interactive-dep-plot.png&quot;
      alt=&quot;SHAP dependency plot illustrating interaction effects in machine learning algorithm&quot;
  /&gt;&lt;/span&gt;

  &lt;p&gt;And here is the code to reproduce this plot:&lt;/p&gt;
  &lt;figure&gt;
    &lt;script src=&quot;https://gist.github.com/UrszulaCzerwinska/b78e85c6e1795d949321209cbe41587a.js&quot;&gt;&lt;/script&gt;
  &lt;/figure&gt;
  &lt;p&gt;Here we have the strongest interactions:&lt;/p&gt;
  &lt;p&gt;Income — Education&lt;/p&gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/income-education.png&quot;
      alt=&quot;Shapley interaction plot for better visualization of features interaction in machine learning model&quot;
  /&gt;&lt;/span&gt;

  &lt;p&gt;
    In this graph, we notice that with an Education level 1 (undergrad), low
    income (under 100 k USD) is an encouraging factor to take a credit, and high
    income (over 120 k USD) is an inhibiting interaction.&lt;br /&gt;For individuals
    with Education 2 &amp;amp; 3 (graduated &amp;amp; advanced/professional), the
    interaction effect is slightly lower and opposite to that for Education ==
    1.
  &lt;/p&gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/income-family.png&quot;
      alt=&quot;Shapley interaction plot for better visualization of features interaction in machine learning model&quot;
  /&gt;&lt;/span&gt;

  &lt;p&gt;
    For the features “Family” and “number of people” in the household, the
    interaction is positive when income is low (below USD 100k) and the family
    has 1–2 members. For higher incomes (&amp;gt; 120 k USD), for family with 1–2
    members has a negative effect. The opposite is true for families of 3–4
    people.
  &lt;/p&gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/income-ccavg.png&quot;
      alt=&quot;Shapley interaction plot for better visualization of features interaction in machine learning model&quot;
  /&gt;&lt;/span&gt;

  &lt;p&gt;
    The interaction between income and credit card average spending is more
    complex. For low income (&amp;lt;100 k USD) and low CCAvg (&amp;lt;4 k USD) the
    interaction has a negative effect, for income between 50 and 110 k USD and
    CCAvg 2–6 k USD the effect is strongly positive, this could define a
    potential target for credit canvassing along these two axes. For high
    incomes (&amp;gt; 120 k USD), the low CCAvg has a positive impact on the
    prediction of class 1, high CCAvg has a small negative effect on the
    predictions, the medium CCAvg has a stronger negative impact.
  &lt;/p&gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/family-education.png&quot;
      alt=&quot;Shapley interaction plot for better visualization of features interaction in machine learning model&quot;
  /&gt;&lt;/span&gt;
  &lt;p&gt;
    The interaction between two features is a little less readible. For a family
    of 1 and 2 members with “undergrad” education, the interaction has a
    negative impact. For a family of 3–4 members the effect is the opposite.
  &lt;/p&gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/income-age.png&quot;
      alt=&quot;Shapley interaction plot for better visualization of features interaction in machine learning model&quot;
  /&gt;&lt;/span&gt;

  &lt;p&gt;
    For low incomes (&amp;lt; 70k USD), impact changes linearly with age, the higher
    the age, the more the impact varies positively. For high incomes (&amp;gt;120k
    USD), the interaction impact is lower, at middle age (~40 years) the impact
    is slightly positive, at low age the impact is negative and for age &amp;gt;45
    the impact is neutral.
  &lt;/p&gt;
  &lt;p&gt;
    These findings would be more complicated to interpret if the values of the
    features had not corresponded to original values. For instance, speaking of
    age or income in negative in units. Therefore, representing explanations in
    an understandable dimension facilitates interpretation.
  &lt;/p&gt;
  &lt;h4&gt;Comparing Models: How SHAP Improves Machine Learning Interpretability&lt;/h4&gt;
  &lt;p&gt;
    In some situations, we may want to compare the predictions of different
    models for the same samples. Understand why one model classifies the sample
    correctly and the other one does not.&lt;br /&gt;To start, we can display the
    summary plots for each model, look at the importance of features and the
    shapley value distributions. This gives a first general idea.
  &lt;/p&gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/tree-explainer.png&quot;
      alt=&quot;SHAP tree explainer illustrating how to compare the predictions of different models for the same samples&quot;
  /&gt;&lt;/span&gt;

  &lt;p&gt;
    Decision plot allows to compare on the same graph the predictions of
    different models for the same sample.&lt;br /&gt;You just have to create an object
    that simulates multiclass classification.
  &lt;/p&gt;
  &lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt; &lt;span class=&quot;c1&quot;&gt;# we have tree models : xgb, gbt, rf # for each model
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;we&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;an&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;explainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_explainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;probabilities &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_probs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shapley&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#xgb xgb_explainer =
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;shap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;TreeExplainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xgb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model_output&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;probability&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;feature_perturbation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;interventional&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xgb_values&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;xgb_explainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;shap_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xgb_probs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;xgb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;predict_proba&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pipeline_trans&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xgb_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;xgb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pipeline_trans&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# rf rf_explainer =
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;shap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;TreeExplainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model_output&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;probability&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;feature_perturbation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;interventional&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rf_values&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;rf_explainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;shap_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rf_probs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;rf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;predict_proba&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pipeline_trans&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rf_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;rf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pipeline_trans&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# gbt gbt_explainer =
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;shap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;TreeExplainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gbt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model_output&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;probability&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;feature_perturbation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;interventional&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gbt_values&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;gbt_explainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;shap_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gbt_probs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;gbt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;predict_proba&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pipeline_trans&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gbt_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;gbt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pipeline_trans&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;######## # we make a list
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;explaners&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;base_values&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xgb_explainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;expected_value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;rf_explainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;expected_value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gbt_explainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;expected_value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shap_values&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xgb_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rf_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gbt_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xgb_probs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rf_probs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gbt_probs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;xgb&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;rf&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;gbt&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# index of a
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Plot idx = 100 shap.multioutput_decision_plot(base_values,
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;shap_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;feature_names&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;to_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;legend_labels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;legend_location&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;lower right&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/model-comparison.png&quot;
      alt=&quot;Comparative analysis of machine learning models using SHAP values for data science insights.&quot;
  /&gt;&lt;/span&gt;

  &lt;p&gt;
    The only difficulty consists in checking the dimensions of shapley values
    because for some models, shapley values are calculated for each class, in
    the case of the binary classification (class 0 and 1), while for others we
    obtain a single matrix which corresponds to class 1. In our example, we
    select a second matrix (index 1) for random forest.
  &lt;/p&gt;
  &lt;h4&gt;
    Running Simulations with SHAP: Enhancing Model Predictions in Data Science
  &lt;/h4&gt;
  &lt;p&gt;
    By default SHAP does not contain functions that make it easier to answer the
    “What if?” question. “What if I could earn an extra 10K USD a year, would my
    credit be extended?”&lt;br /&gt;Nevertheless, it is possible to run the
    simulations by varying a feature and calculating hypothetical shapley
    values.
  &lt;/p&gt;
  &lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt; &lt;span class=&quot;n&quot;&gt;explainer_margin_i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;shap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;TreeExplainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xgb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model_output&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;raw&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;feature_perturbation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;interventional&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rg&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;202&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;R&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;hypothetical_shap_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hypothetical_predictions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_r&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
  &lt;span class=&quot;nf&quot;&gt;simulate_with_shap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Income&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;explainer_margin_i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;pipeline_trans&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pipeline_trans&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
  &lt;p&gt;
    I created a `simulate_with_shap` function that simulates different values of
    the feature and calculates the hypothetical shapley values.
  &lt;/p&gt;
  &lt;figure&gt;
    &lt;script src=&quot;https://gist.github.com/UrszulaCzerwinska/c324a8ec8e37ebe4758f4affe52456be.js&quot;&gt;&lt;/script&gt;
  &lt;/figure&gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/simulation.png&quot;
      alt=&quot;The SHAP simulation allows to see how we could change the prediction using new values and what the shapley values would be for these&quot;
  /&gt;&lt;/span&gt;

  &lt;p&gt;
    This simulation allows us to see for the selected sample, if we freeze all
    the features apart from Income, how we could change the prediction and what
    the shapley values would be for these new values.
  &lt;/p&gt;
  &lt;p&gt;
    It is possible to simulate the changes ‘feature by feature’, it would be
    interesting to be able to make several changes simultaneously.
  &lt;/p&gt;
  &lt;h3&gt;
    Future Trends in Data Science: The Growing Role of SHAP and Interpretability
  &lt;/h3&gt;
  &lt;p&gt;
    AI algorithms are taking up more and more space in our lives. The
    explanability of predictions is an important topic for data scientists,
    decision-makers and individuals who are impacted by predictions.
  &lt;/p&gt;
  &lt;p&gt;
    Several frameworks have been proposed in order to transform non-explainable
    models into explainable ones. One of the best known and most widely used
    frameworks is SHAP.
  &lt;/p&gt;
  &lt;p&gt;
    Despite very good documentation, it is not clear how to exploit all its
    features in depth.
  &lt;/p&gt;
  &lt;p&gt;
    I have proposed some simple graphical enhancements and tried to demonstrate
    the usefulness of less known and not understood features in most standard
    uses of SHAP.
  &lt;/p&gt;
  &lt;h3&gt;Acknowledgements&lt;/h3&gt;
  &lt;p&gt;
    I would like to thank the Saegus DATA team who participated in this work
    with good advice, in particular Manager Fréderic Brajon and Senior
    Consultant Manager Clément Moutard.
  &lt;/p&gt;
  &lt;h3&gt;Bibliography&lt;/h3&gt;
  &lt;p&gt;
    [1] Stop Explaining Black Box Machine Learning Models for High Stakes
    Decisions and Use Interpretable Models Instead; Cynthia Rudin
    &lt;a
      href=&quot;https://arxiv.org/pdf/1811.10154.pdf&quot;
      data-href=&quot;https://arxiv.org/pdf/1811.10154.pdf&quot;
      target=&quot;_blank&quot;
      &gt;https://arxiv.org/pdf/1811.10154.pdf&lt;/a
    &gt;
  &lt;/p&gt;
  &lt;p&gt;
    [2] Explainable Artificial Intelligence (XAI): Concepts, Taxonomies,
    Opportunities and Challenges toward Responsible AI; Arrietaa et al.
    &lt;a
      href=&quot;https://arxiv.org/pdf/1910.10045.pdf&quot;
      data-href=&quot;https://arxiv.org/pdf/1910.10045.pdf&quot;
      target=&quot;_blank&quot;
      &gt;https://arxiv.org/pdf/1910.10045.pdf&lt;/a
    &gt;
  &lt;/p&gt;
  &lt;p&gt;
    [3] Cloudera Fast Forward Interpretability:
    &lt;a
      href=&quot;https://ff06-2020.fastforwardlabs.com/?utm_campaign=Data_Elixir&amp;amp;utm_source=Data_Elixir_282&quot;
      data-href=&quot;https://ff06-2020.fastforwardlabs.com/?utm_campaign=Data_Elixir&amp;amp;utm_source=Data_Elixir_282&quot;
      target=&quot;_blank&quot;
      &gt;https://ff06-2020.fastforwardlabs.com/?utm_campaign=Data_Elixir&amp;amp;utm_source=Data_Elixir_282&lt;/a
    &gt;
  &lt;/p&gt;
  &lt;p&gt;
    [4]
    &lt;a
      href=&quot;https://towardsdatascience.com/understand-the-machine-learning-blackbox-with-ml-interpreter-7b0f9a2d8e9f&quot;
      data-href=&quot;https://towardsdatascience.com/understand-the-machine-learning-blackbox-with-ml-interpreter-7b0f9a2d8e9f&quot;
      target=&quot;_blank&quot;
      &gt;https://towardsdatascience.com/understand-the-machine-learning-blackbox-with-ml-interpreter-7b0f9a2d8e9f&lt;/a
    &gt;
  &lt;/p&gt;
  &lt;p&gt;
    [5]
    &lt;a
      href=&quot;https://github.com/slundberg/shap&quot;
      data-href=&quot;https://github.com/slundberg/shap&quot;
      target=&quot;_blank&quot;
      &gt;https://github.com/slundberg/shap&lt;/a
    &gt;
  &lt;/p&gt;
  &lt;p&gt;
    [6]
    &lt;a
      href=&quot;http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions&quot;
      data-href=&quot;http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions&quot;
      target=&quot;_blank&quot;
      &gt;http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions&lt;/a
    &gt;
  &lt;/p&gt;
  &lt;p&gt;
    [7]
    &lt;a
      href=&quot;https://www.nature.com/articles/s42256-019-0138-9&quot;
      data-href=&quot;https://www.nature.com/articles/s42256-019-0138-9&quot;
      target=&quot;_blank&quot;
      &gt;https://www.nature.com/articles/s42256-019-0138-9&lt;/a
    &gt;
  &lt;/p&gt;
  &lt;p&gt;
    [8]
    &lt;a
      href=&quot;https://christophm.github.io/interpretable-ml-book/&quot;
      data-href=&quot;https://christophm.github.io/interpretable-ml-book/&quot;
      target=&quot;_blank&quot;
      &gt;https://christophm.github.io/interpretable-ml-book/&lt;/a
    &gt;
  &lt;/p&gt;
  &lt;p&gt;
    [9]
    &lt;a
      href=&quot;https://towardsdatascience.com/shap-explained-the-way-i-wish-someone-explained-it-to-me-ab81cc69ef30&quot;
      data-href=&quot;https://towardsdatascience.com/shap-explained-the-way-i-wish-someone-explained-it-to-me-ab81cc69ef30&quot;
      target=&quot;_blank&quot;
      &gt;https://towardsdatascience.com/shap-explained-the-way-i-wish-someone-explained-it-to-me-ab81cc69ef30&lt;/a
    &gt;
  &lt;/p&gt;
  &lt;p&gt;
    [10]
    &lt;a
      href=&quot;https://medium.com/@gabrieltseng/interpreting-complex-models-with-shap-values-1c187db6ec83&quot;
      data-href=&quot;https://medium.com/@gabrieltseng/interpreting-complex-models-with-shap-values-1c187db6ec83&quot;
      target=&quot;_blank&quot;
      &gt;https://medium.com/@gabrieltseng/interpreting-complex-models-with-shap-values-1c187db6ec83&lt;/a
    &gt;
  &lt;/p&gt;
  &lt;p&gt;
    [11]
    &lt;a
      href=&quot;https://medium.com/@stanleyg1/a-detailed-walk-through-of-shap-example-for-interpretable-machine-learning-d265c693ac22&quot;
      data-href=&quot;https://medium.com/@stanleyg1/a-detailed-walk-through-of-shap-example-for-interpretable-machine-learning-d265c693ac22&quot;
      target=&quot;_blank&quot;
      &gt;https://medium.com/@stanleyg1/a-detailed-walk-through-of-shap-example-for-interpretable-machine-learning-d265c693ac22&lt;/a
    &gt;
  &lt;/p&gt;
  &lt;p&gt;
    [12]
    &lt;a
      href=&quot;https://francescopochetti.com/whitening-a-black-box-how-to-interpret-a-ml-model/&quot;
      data-href=&quot;https://francescopochetti.com/whitening-a-black-box-how-to-interpret-a-ml-model/&quot;
      target=&quot;_blank&quot;
      &gt;https://francescopochetti.com/whitening-a-black-box-how-to-interpret-a-ml-model/&lt;/a
    &gt;
  &lt;/p&gt;
  &lt;p&gt;
    [13] Explaining Anomalies Detected by Autoencoders Using SHAP; Antwarg et
    al.
    &lt;a
      href=&quot;https://arxiv.org/pdf/1903.02407.pdf&quot;
      data-href=&quot;https://arxiv.org/pdf/1903.02407.pdf&quot;
      target=&quot;_blank&quot;
      &gt;https://arxiv.org/pdf/1903.02407.pdf&lt;/a
    &gt;
  &lt;/p&gt;
  &lt;p&gt;
    [14]
    &lt;a
      href=&quot;https://medium.com/fiddlerlabs/case-study-explaining-credit-modeling-predictions-with-shap-2a7b3f86ec12&quot;
      data-href=&quot;https://medium.com/fiddlerlabs/case-study-explaining-credit-modeling-predictions-with-shap-2a7b3f86ec12&quot;
      target=&quot;_blank&quot;
      &gt;https://medium.com/fiddlerlabs/case-study-explaining-credit-modeling-predictions-with-shap-2a7b3f86ec12&lt;/a
    &gt;
  &lt;/p&gt;
  &lt;p&gt;
    [15]
    &lt;a
      href=&quot;https://www.kaggle.com/itsmesunil/bank-loan-modelling&quot;
      data-href=&quot;https://www.kaggle.com/itsmesunil/bank-loan-modelling&quot;
      target=&quot;_blank&quot;
      &gt;https://www.kaggle.com/itsmesunil/bank-loan-modelling&lt;/a
    &gt;
  &lt;/p&gt;

  &lt;footer&gt;
    This blog post was originally published with
    &lt;a
      href=&quot;https://medium.com/swlh&quot;
      alt=&quot;Get smarter at building your thing. Follow to join The Startup’s +8 million monthly readers &amp; +778K followers.&quot;
      &gt;The Startup&lt;/a
    &gt;
    at
    &lt;a
      href=&quot;https://medium.com/swlh/push-the-limits-of-explainability-an-ultimate-guide-to-shap-library-a110af566a02&quot;
      alt=&quot;Discover how to push the boundaries of explainability in data science and machine learning. This comprehensive guide to SHAP (SHapley Additive exPlanations) covers everything from deep learning algorithms to model interpretability, making complex AI models more transparent and trustworthy. Ideal for data science professionals and enthusiasts looking to enhance their understanding of machine learning interpretability with cutting-edge tools and techniques.&quot;
      &gt;Medium&lt;/a
    &gt;. &lt;br /&gt;&lt;br /&gt;
  &lt;/footer&gt;
  &lt;script
    type=&quot;text/javascript&quot;
    src=&quot;//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-584ec4ce89deed84&quot;
  &gt;&lt;/script&gt;
  &lt;div class=&quot;addthis_inline_share_toolbox&quot;&gt;&lt;/div&gt;
&lt;/section&gt;
</description>
        <pubDate>Sun, 01 Mar 2020 00:00:00 +0100</pubDate>
        <link>http://urszulaczerwinska.github.io/works/interpretability-shap</link>
        <guid isPermaLink="true">http://urszulaczerwinska.github.io/works/interpretability-shap</guid>
        
        <category>data science</category>
        
        <category>interpretability</category>
        
        <category>XAI</category>
        
        <category>machine learning</category>
        
        <category>featured</category>
        
        <category>explainable AI</category>
        
        <category>Python</category>
        
        
        <category>works</category>
        
      </item>
    
      <item>
        <title>Mastering Named Entity Recognition (NER) in Data Science</title>
        <description>&lt;h2 id=&quot;context-of-development-of-a-keyword-extraction-application-using-nlp-language-model&quot;&gt;Context of development of a keyword extraction application using NLP language model&lt;/h2&gt;

&lt;p&gt;Named Entity Recognition, often abbreviated as NER, has gained traction as a critical tool for extracting meaningful insights from text data. Whether you’re diving into data science projects or exploring the cutting edge of AI applied to language, understanding how to utilize NER is essential. In this post, I’ll walk you through a practical example of using SpaCy, a go-to library for NLP, to detect keywords from Medium articles. But first, let’s explore why NER is becoming a must-have skill in the data science and engineering toolbox.&lt;/p&gt;

&lt;p&gt;Inspired by a solution developed for a customer in the Pharmaceutical industry, we presented at the &lt;a href=&quot;https://paris.egg.dataiku.com/&quot;&gt;EGG PARIS 2019&lt;/a&gt; conference an application based on NLP (Natural Language Processing) and developed on a &lt;a href=&quot;https://www.dataiku.com/&quot;&gt;Dataiku&lt;/a&gt; &lt;a href=&quot;https://www.dataiku.com/dss/&quot;&gt;DSS&lt;/a&gt; environment.&lt;/p&gt;

&lt;p&gt;More precisely, we trained a deep learning model to recognize the keywords of a blog article, precisely from &lt;a href=&quot;https://medium.com/&quot;&gt;Medium blogging platform&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;By &lt;strong&gt;automatically generate tags and/or keywords&lt;/strong&gt;, this approach enables personalized content recommendations, improving user experience by aligning content with reader expectations. The method holds significant potential, particularly for automated text analysis of complex documents, including scientific papers and legal texts.&lt;/p&gt;

&lt;p&gt;To showcase its functionality, we integrated a voice command feature using &lt;a href=&quot;https://azure.microsoft.com/en-us/services/cognitive-services/&quot;&gt;Azure’s cognitive services API&lt;/a&gt;. The &lt;em&gt;speech to text&lt;/em&gt; module translates spoken queries into text, which is then processed by the algorithm. The output is a recommendation of articles, classified by relevance according to the field of research.&lt;/p&gt;

&lt;p&gt;In this article, I’ll walk you through our approach to creating the underlying NLP model.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/zg0pTe-GyF0&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;sub&gt;&lt;sup&gt;[To view the comments, please enable subtitles] A video that illustrates our web application created for the EGG Dataiku 2019 conference&lt;/sup&gt;&lt;/sub&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;why-extract-keywords-from-medium-blog-articles-with-ai-&quot;&gt;Why Extract Keywords from Medium Blog Articles with AI ?&lt;/h2&gt;

&lt;p&gt;Medium has two categorization systems: &lt;strong&gt;tags&lt;/strong&gt; and &lt;strong&gt;topics&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Topics&lt;/strong&gt; are predefined by the platform and correspond to broad categories like data science or machine learning. Authors have no control over these.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Tags&lt;/strong&gt;, on the other hand, are keywords selected by the author, with a maximum of five tags per article. These tags help increase the visibility of the article but often may not accurately reflect the content. For instance, tags like “TECHNOLOGY,” “MINDFULNESS,” or “LIFE LESSONS” might make an article easier to find but can complicate the reader’s search for specific content.&lt;/p&gt;

&lt;p&gt;Our approach aims to improve this by automatically tagging articles, increasing their relevance. With these “new tags” or “keywords,” searching for articles becomes more efficient.&lt;/p&gt;

&lt;p&gt;Going further, this method could be used to build a recommendation system that suggests related articles based on the one you’re currently reading or aligned with your reading habits.&lt;/p&gt;

&lt;h2 id=&quot;the-ner-named-entity-recognition-approach&quot;&gt;The NER (Named Entity Recognition) approach&lt;/h2&gt;

&lt;p&gt;Using the NER (Named Entity Recognition) approach, we can extract entities across various categories. Several pre-trained models, like &lt;a href=&quot;https://github.com/explosion/spacy-models/releases/tag/en_core_web_md-2.2.0&quot;&gt;en_core_web_md&lt;/a&gt; can recognize entities like people, places, dates, etc.&lt;/p&gt;

&lt;p&gt;For example, in the sentence &lt;em&gt;“I think Barack Obama met founder of Facebook at occasion of a release of a new NLP algorithm.”&lt;/em&gt;, the en_core_web_md model detects “Facebook” and “Barack Obama” as entities.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/UrszulaCzerwinska/11a8fab0cc4c936b67e374e2b55e0fa0.js&quot;&gt;&lt;/script&gt;

&lt;div&gt;&lt;span class=&quot;image fit&quot;&gt;&lt;img src=&quot;/images/NER_img1.png&quot; alt=&quot;NER process using SpaCy in data science&quot; /&gt;&lt;/span&gt;&lt;/div&gt;
&lt;p align=&quot;center&quot;&gt;&lt;sub&gt;&lt;sup&gt;Dependency graph: result of line 9 (# 1)&lt;/sup&gt;&lt;/sub&gt;&lt;/p&gt;

&lt;div&gt;&lt;span class=&quot;image fit&quot;&gt;&lt;img src=&quot;/images/NER_img2.png&quot; alt=&quot;NER process using SpaCy in data science&quot; /&gt;&lt;/span&gt;&lt;/div&gt;
&lt;p align=&quot;center&quot;&gt;&lt;sub&gt;&lt;sup&gt;Entity detection: result of line 10 (# 2)&lt;/sup&gt;&lt;/sub&gt;&lt;/p&gt;

&lt;p&gt;With some annotated data, we trained the algorithm to detect this new entity type.&lt;/p&gt;

&lt;p&gt;The concept is straightforward: an article tagged with “Data Science,” “AI,” “Machine Learning,” or “Python” might still cover vastly different technologies. Our algorithm is designed to detect specific technologies mentioned in the article, such as GANs, reinforcement learning, or Python libraries, while still recognizing places, organizations, and people.&lt;/p&gt;

&lt;p&gt;During training, the model learns to identify keywords without prior knowledge. For example, it might recognize “random forest” as a topic, even if it wasn’t in the training data. By analyzing other algorithms discussed in articles, the NER model can identify phrase patterns that indicate a specific topic.&lt;/p&gt;

&lt;h2 id=&quot;the-machine-learning-language-model-behind&quot;&gt;The machine learning language model behind&lt;/h2&gt;

&lt;h3 id=&quot;spacy-framework-for-nlp&quot;&gt;SpaCy Framework for NLP&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://spacy.io/&quot;&gt;SpaCy&lt;/a&gt; is an open-source library tailored for advanced natural language processing in Python. It’s built for production use and helps create applications that process large volumes of text. SpaCy can be used to build information extraction systems, natural language understanding systems, or text preprocessing pipelines for deep learning. Among its features are tokenization, parts-of-speech (PoS) tagging, text classification, and named entity recognition.&lt;/p&gt;

&lt;p&gt;SpaCy offers an efficient, statistical system for NER in Python. Beyond the default entities, SpaCy allows us to add custom classes to the NER model and train it with new examples.&lt;/p&gt;

&lt;p&gt;SpaCy’s NER model is based on &lt;strong&gt;Convolutional Neural Networks (CNNs)&lt;/strong&gt;. For those interested, more details on how SpaCy’s NER model works can be found in the video below:&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/sqDHBH9IjRU&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/p&gt;

&lt;h3 id=&quot;training-data&quot;&gt;Training data&lt;/h3&gt;

&lt;p&gt;To train our model to recognize tech keywords, we scraped some Medium articles through &lt;strong&gt;web scraping&lt;/strong&gt;.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/UrszulaCzerwinska/db0aa37b1cb10ec94205d847f63ddc4f.js&quot;&gt;&lt;/script&gt;

&lt;div&gt;&lt;span class=&quot;image fit&quot;&gt;&lt;img src=&quot;/images/NER_img3.png&quot; alt=&quot;Table showing training data for language model&quot; /&gt;&lt;/span&gt;&lt;/div&gt;
&lt;p align=&quot;center&quot;&gt;&lt;sub&gt;&lt;sup&gt;An extract from the table containing the contents of the medium articles&lt;/sup&gt;&lt;/sub&gt;&lt;/p&gt;

&lt;p&gt;The text of each article was split into sentences for easier annotation.&lt;/p&gt;

&lt;p&gt;For NER annotation, there are tools like &lt;strong&gt;Prodigy&lt;/strong&gt;, but we opted for a simple spreadsheet where we manually marked the entities in dedicated columns.&lt;/p&gt;

&lt;div&gt;&lt;span class=&quot;image fit&quot;&gt;&lt;img src=&quot;/images/NER_img4.png&quot; alt=&quot;Table showing training data for language model and first predictions&quot; /&gt;&lt;/span&gt;&lt;/div&gt;

&lt;p&gt;With around twenty articles (~600 sentences), our model began to show promising performance, achieving over 0.78 accuracy on the test set. We separated the train and test data to evaluate the model effectively.&lt;/p&gt;

&lt;div&gt;&lt;span class=&quot;image fit&quot;&gt;&lt;img src=&quot;/images/NER_img5.png&quot; alt=&quot;Table showing training data for language model and first predictions&quot; /&gt;&lt;/span&gt;&lt;/div&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;TRAIN_DATA_ALL&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_table&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;apply&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;mark_targets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;ORG&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;PERSON&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;LOC&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;TOPIC&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;GPE&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;DATE&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;EVENT&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;WORK_OF_ART&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;sents&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;ORG&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;PERSON&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;LOC&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;TOPIC&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;GPE&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;DATE&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;EVENT&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;WORK_OF_ART&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;div&gt;&lt;span class=&quot;image fit&quot;&gt;&lt;img src=&quot;/images/NER_img6.png&quot; alt=&quot;Table showing training data for language model and first predictions&quot; /&gt;&lt;/span&gt;&lt;/div&gt;

&lt;p&gt;We fine-tuned the algorithm by adjusting parameters like the number of iterations, dropout rate, learning rate, and batch size.&lt;/p&gt;

&lt;h3 id=&quot;the-nlp-model-assesment&quot;&gt;The NLP model assesment&lt;/h3&gt;

&lt;p&gt;In addition to the model’s loss metric, we implemented precision, recall, and F1 score to measure performance more accurately.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/UrszulaCzerwinska/c23ce9e0edffe6f9790a2bbf8f018a4b.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;After training on the annotated data, the best model’s performance on our test set was quite impressive, especially considering the modest training data size (~3000 sentences).&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;precision :  0.9588053949903661
recall :  0.9211764705882353
f1_score :  0.9396221959858323

It is is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy.

TOPIC Python
TOPIC NumPy
TOPIC SciPy&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;In the &lt;strong&gt;Flow&lt;/strong&gt; on DSS, the process can be summarized by the graph:&lt;/p&gt;

&lt;div&gt;&lt;span class=&quot;image fit&quot;&gt;&lt;img src=&quot;/images/NER_img7.png&quot; alt=&quot;&quot; /&gt;&lt;/span&gt;&lt;/div&gt;
&lt;p align=&quot;center&quot;&gt;&lt;sub&gt;&lt;sup&gt;Flow on Dataiku&apos;s DSS platform: the annotated dataset is divided into train and test, the model learned on the train data is evaluated on the train and test batches.&lt;/sup&gt;&lt;/sub&gt;&lt;/p&gt;

&lt;p&gt;Returning to our Barack Obama example, our algorithm now detects the NLP algorithm entity as a TOPIC, in addition to the ORG (organization), LOC (location), GPE (geopolitical entity), and DATE categories.&lt;/p&gt;

&lt;p&gt;We have succeeded! 🚀&lt;/p&gt;

&lt;p&gt;The next step involves incorporating the model into our recommendation system, enhancing the customization of articles offered to users based on detected topics.&lt;/p&gt;

&lt;div&gt;&lt;span class=&quot;image fit&quot;&gt;&lt;img src=&quot;/images/NER_img8.png&quot; alt=&quot;&quot; /&gt;&lt;/span&gt;&lt;/div&gt;

&lt;p&gt;The finalized model can be compiled as an independent python library (instructions here) and installed with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pip&lt;/code&gt;. This is very practical for deploying the model in another environment and for production setup.&lt;/p&gt;

&lt;div&gt;&lt;span class=&quot;image fit&quot;&gt;&lt;img src=&quot;/images/NER_img9.jpg&quot; alt=&quot;&quot; /&gt;&lt;/span&gt;&lt;/div&gt;

&lt;h2 id=&quot;exploitation-of-the-model&quot;&gt;Exploitation of the model&lt;/h2&gt;

&lt;h3 id=&quot;analysis-of-an-article-medium&quot;&gt;Analysis of an article Medium&lt;/h3&gt;

&lt;p&gt;In our mini webapp, presented at the EGG, it is possible to display the most frequent entities of a Medium article.&lt;/p&gt;

&lt;p&gt;Thus, for the article: &lt;a href=&quot;https://towardsdatascience.com/cat-dog-or-elon-musk-145658489730&quot;&gt;https://towardsdatascience.com/cat-dog-or-elon-musk-145658489730&lt;/a&gt;, the most frequent entities were: model, MobileNet, Transfer learning, network, Python. We also detected people: Elon Musk, Marshal McLuhan and organizations: Google, Google Brain.&lt;/p&gt;

&lt;div&gt;&lt;span class=&quot;image fit&quot;&gt;&lt;img src=&quot;/images/NER_img10.png&quot; alt=&quot;&quot; /&gt;&lt;/span&gt;&lt;/div&gt;

&lt;p&gt;Inspired by &lt;a href=&quot;https://towardsdatascience.com/@bramblexu&quot;&gt;Xu LIANG’s&lt;/a&gt; &lt;a href=&quot;https://towardsdatascience.com/textrank-for-keyword-extraction-by-python-c0bae21bcec0&quot;&gt;post&lt;/a&gt;, we also used his way of representing the relationship between words in the form of a graph of linguistic dependencies. Unlike in his method, we did not use TextRank or TFIDF to detect keywords but we only applied our pre-trained NER model.&lt;/p&gt;

&lt;p&gt;Then, like &lt;a href=&quot;https://towardsdatascience.com/@bramblexu&quot;&gt;Xu LIANG&lt;/a&gt;, we used the capacity of Parts-of-Speech (PoS) Tagging, inherited by our model from the original model (&lt;a href=&quot;https://github.com/explosion/spacy-models/releases/tag/en_core_web_md-2.2.0&quot;&gt;en_core_web_md&lt;/a&gt;), to link the entities together with the edges, which forms the graph below.&lt;/p&gt;

&lt;div&gt;&lt;span class=&quot;image fit&quot;&gt;&lt;img src=&quot;/images/NER_img11.png&quot; alt=&quot;&quot; /&gt;&lt;/span&gt;&lt;/div&gt;
&lt;p&gt;&lt;sub&gt;&lt;sup&gt;The graph of dependencies between the entities detected in the article “Cat, Dog, or Elon Musk?”&lt;/sup&gt;&lt;/sub&gt;&lt;/p&gt;

&lt;p&gt;Thus, we get a graph where the keywords are placed around their category: Tech topic, Person and Organization.&lt;/p&gt;

&lt;p&gt;This gives a quick overview of the content of a Medium article.&lt;/p&gt;

&lt;p&gt;Here is how to get the graph from a Medium article url link:&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/UrszulaCzerwinska/d1d77f0bf8bd089103994eb3883db28f.js&quot;&gt;&lt;/script&gt;

&lt;h2 id=&quot;to-go-further&quot;&gt;To go further&lt;/h2&gt;
&lt;p&gt;Our Saegus Showroom including the functional webapp is coming soon. Feel free to follow our page &lt;a href=&quot;https://medium.com/data-by-saegus&quot;&gt;https://medium.com/data-by-saegus&lt;/a&gt; to be kept informed.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The project we have outlined here can easily be transposed into the various fields of industry: technical, legal and medical documents. It could be very interesting to analyse the civil, criminal and law… with this approach for a better efficiency in the research that all legal professionals do.&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt;

&lt;p&gt;To conclude, by recognizing topics within Medium articles, this solution represents a significant leap forward in content personalization. Whether for individual readers or professionals seeking articles on specific subjects, automatic keyword extraction offers a tailored experience. This model’s ability to classify articles based on finely-tuned NER allows for precise, relevant recommendations, improving overall user satisfaction and engagement.&lt;/p&gt;

&lt;p&gt;We invite you to explore this exciting field and consider how such technology could be adapted to your specific needs.&lt;/p&gt;

&lt;h2 id=&quot;disclaimer&quot;&gt;&lt;em&gt;Disclaimer&lt;/em&gt;&lt;/h2&gt;

&lt;p&gt;This article is a result of a teamwork realized at &lt;a href=&quot;http://saegus.com/fr/&quot;&gt;Saegus&lt;/a&gt;. Published originally in French at &lt;a href=&quot;https://medium.com/data-by-saegus/ner-medium-articles-saegus-7ffec0f3188c&quot;&gt;Medium&lt;/a&gt;.&lt;/p&gt;

&lt;section&gt;
&lt;!-- Go to www.addthis.com/dashboard to customize your tools --&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-584ec4ce89deed84&quot;&gt;&lt;/script&gt;
&lt;!-- Go to www.addthis.com/dashboard to customize your tools --&gt; &lt;div class=&quot;addthis_inline_share_toolbox&quot;&gt;&lt;/div&gt;
&lt;section&gt;

&lt;/section&gt;&lt;/section&gt;
</description>
        <pubDate>Mon, 11 Nov 2019 00:00:00 +0100</pubDate>
        <link>http://urszulaczerwinska.github.io/works/egg_ner</link>
        <guid isPermaLink="true">http://urszulaczerwinska.github.io/works/egg_ner</guid>
        
        <category>NLP</category>
        
        <category>Python</category>
        
        <category>featured</category>
        
        <category>machine learning</category>
        
        <category>NER</category>
        
        <category>language models</category>
        
        
        <category>works</category>
        
      </item>
    
      <item>
        <title>Women in Healthcare Analytics and Data Science (WiHADS)</title>
        <description>&lt;html&gt;
  &lt;head&gt;
    &lt;meta
      name=&quot;description&quot;
      content=&quot;Urszula Czerwinska orgnaiser of a meetup WiHADS | Join WiHADS, a pioneering community promoting women in data science, healthcare analytics, and deep learning AI. Explore opportunities to lead data science projects, engage with deep learning models, and shape the future of AI in healthcare.&quot;
    /&gt;
  &lt;/head&gt;

  &lt;body&gt;&lt;/body&gt;
  &lt;section&gt;
    &lt;h2&gt;
      Introduction: Empowering Women in Data Science and AI for Healthcare
    &lt;/h2&gt;
    &lt;p&gt;
      In today’s rapidly evolving world, data science is driving innovation
      across industries, particularly in healthcare where AI and deep learning
      models are transforming patient outcomes. The
      &lt;strong&gt;Women in Healthcare Analytics and Data Science (WiHADS)&lt;/strong&gt;
      initiative is leading the charge, empowering women in data science to
      spearhead crucial data science projects that are revolutionizing the
      healthcare sector.
    &lt;/p&gt;
    &lt;h2&gt;The Beginnings of WiHADS: A Movement in Data Science and Healthcare&lt;/h2&gt;
    &lt;p&gt;
      WiHADS began in 2017, spearheaded by
      &lt;a href=&quot;https://www.linkedin.com/in/sameh-m-8575a3179/&quot;&gt;Sameh Megrhi&lt;/a&gt;,
      with a vision to amplify the voices of women in data science and
      healthcare analytics.&lt;br /&gt;
    &lt;/p&gt;

    &lt;blockquote&gt;
      &lt;p&gt;
        The initiative was created to address the underrepresentation of women
        in fields like deep learning models, data science and engineering, and
        AI in healthcare. By highlighting the significant contributions of
        women, WiHADS aims to inspire more women to enter and thrive in these
        critical sectors.
      &lt;/p&gt;
      &lt;p&gt;
        Since its inception, WiHADS has brought together a healthcare data
        science community. The platform provides a space where women can share
        their experiences and insights, fostering a collaborative environment
        that encourages innovation in data science and machine learning. These
        meetups are not just about discussions—they are about solving real-world
        challenges using deep learning algorithms and data science
        methodologies.
      &lt;/p&gt;
    &lt;/blockquote&gt;
    &lt;h2&gt;WiHADS Meetups: A Hub for Innovation in Data Science and Healthcare&lt;/h2&gt;
    &lt;p&gt;
      WiHADS meetups have become a central platform for women in data science to
      exchange ideas, network, and discuss the latest trends in data science and
      machine learning. Since 2017, WiHADS has organized four major meetups in
      Paris, each focusing on the intersection of data science and healthcare.
      These events have garnered attention within the data science community and
      beyond, as they provide invaluable insights into how data science and deep
      learning AI are transforming the healthcare industry.
    &lt;/p&gt;
    &lt;blockquote&gt;
      &lt;b
        &gt;Join us at
        &lt;a
          href=&quot;https://www.meetup.com/fr-FR/Healthcare-Analytics-Data-Science/&quot;
          alt=&quot;The WiHADS meetups are place to exchange about woman in Data Science, Machine Learning and AI&quot;
          &gt;our meetup webpage&lt;/a
        &gt;&lt;/b
      &gt;
    &lt;/blockquote&gt;

    &lt;h2&gt;Key Components of WiHADS Meetups:&lt;/h2&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;strong&gt;Speakers:&lt;/strong&gt; The meetups feature accomplished women from
        top companies like
        &lt;a
          alt=&quot;IQVIA, a global leader in clinical research and healthcare data, drives innovation to enhance patient and public health. By integrating data, insights, technology, and expertise, IQVIA empowers clients and partners to advance and market cutting-edge therapies.&quot;
          href=&quot;https://www.iqvia.com/fr-fr/locations/france&quot;
          &gt;IQvia&lt;/a
        &gt;,
        &lt;a
          alt=&quot;Doctolib a leader in AI for healthcare.&quot;
          href=&quot;https://about.doctolib.fr/?utm_button=header&amp;utm_website=doctolib_career%2F&quot;
          &gt;Doctolib&lt;/a
        &gt;, and
        &lt;a
          alt=&quot;Democratizing expert cardiac care through medical-grade AI and cloud technology&quot;
          href=&quot;https://cardiologs.com/&quot;
          &gt;Cardiologs&lt;/a
        &gt;, who share their expertise on data science projects, deep learning
        models, and their applications in healthcare. These sessions provide
        attendees with a deeper understanding of how data science is applied in
        real-world healthcare settings.
      &lt;/li&gt;

      &lt;li&gt;
        &lt;strong&gt;Opportunities:&lt;/strong&gt; The events offer a unique chance for
        women in data science to connect, collaborate, and learn from each
        other. This networking is crucial for building a strong data science and
        engineering network that can support professional growth and innovation.
      &lt;/li&gt;

      &lt;li&gt;
        &lt;strong&gt;Roundtable Discussions:&lt;/strong&gt; After the presentations,
        participants engage in roundtable discussions, where they explore the
        ethical implications of AI in healthcare, the challenges of implementing
        deep learning algorithms in medical practice, and the future of data
        science in healthcare.
      &lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;
      WiHADS is committed to fostering a supportive environment where women in
      data science can thrive. The meetups are free to attend, thanks to the
      generous support of sponsors like
      &lt;a
        alt=&quot;Doctolib a leader in AI for healthcare.&quot;
        href=&quot;https://about.doctolib.fr/?utm_button=header&amp;utm_website=doctolib_career%2F&quot;
        &gt;Doctolib&lt;/a
      &gt;,
      &lt;a
        alt=&quot;IQVIA, a global leader in clinical research and healthcare data, drives innovation to enhance patient and public health. By integrating data, insights, technology, and expertise, IQVIA empowers clients and partners to advance and market cutting-edge therapies.&quot;
        href=&quot;https://www.iqvia.com/fr-fr/locations/france&quot;
        &gt;IQvia&lt;/a
      &gt;, and
      &lt;a
        alt=&quot;Saegus is supporting businesses in their transition to a combined intelligence model, where artificial intelligence serves humanity&quot;
        href=&quot;https://www.saegus.com/&quot;
        &gt;Saegus&lt;/a
      &gt;, who believe in the importance of promoting diversity in data science
      and AI.
    &lt;/p&gt;

    &lt;h2&gt;The Vision Behind WiHADS: Promoting Women in Data Science&lt;/h2&gt;
    &lt;p&gt;
      The driving force behind WiHADS is the belief that women have a critical
      role to play in the future of data science and healthcare. With a
      background in systems biology and extensive experience in machine learning
      within academia and the pharmaceutical industry, I joined WiHADS in 2018
      to promote the visibility of women in these fields. My goal was to build a
      platform where women could not only showcase their achievements in data
      science and AI in healthcare but also mentor and support one another.
      Working alongside
      &lt;a
        alt=&quot;Data scientist Sameh Megrhi is one of leaders in Machine Learning&quot;
        href=&quot;https://www.linkedin.com/in/sameh-m-8575a3179/&quot;
        &gt;Sameh Megrhi&lt;/a
      &gt;,&lt;a
        alt=&quot;Imen Helali working in Healthcare and Pharmaceutics is highly motivated promote Healthcare and Data Science fields&quot;
        href=&quot;https://www.linkedin.com/in/imen-helali-phd-a6a4222b/&quot;
      &gt;
        Imen Helali&lt;/a
      &gt;
      and
      &lt;a
        atl=&quot;Young data science professional Avani Tanna is promoting woman in tech importance&quot;
        href=&quot;https://www.linkedin.com/in/avani-tanna-b20b8a77/&quot;
        &gt;Avani Tanna&lt;/a
      &gt;, we crafted a unique meetup format that balances expert presentations
      with interactive, community-driven discussions. Our vision is to create a
      vibrant healthcare data science community where women can gain the skills
      and confidence needed to lead impactful data science projects.
    &lt;/p&gt;
    &lt;h2&gt;The Impact of WiHADS on the Healthcare Data Science Community&lt;/h2&gt;
    &lt;p&gt;
      WiHADS has had a profound impact on the healthcare data science community
      by providing a platform that not only highlights the achievements of women
      but also encourages ongoing education and professional development. The
      meetups have sparked important conversations about the future of data
      science in healthcare and the role of deep learning AI in improving
      patient care. Through WiHADS, we have seen an increase in the number of
      women pursuing careers in data science and machine learning, particularly
      in the healthcare sector. By offering opportunities for mentorship,
      networking, and skill development, WiHADS is helping to close the gender
      gap in data science and ensure that women are well-represented in the
      future of AI in healthcare.
    &lt;/p&gt;
    &lt;h2&gt;
      Get Involved with WiHADS: Join the Movement in Data Science and Healthcare
    &lt;/h2&gt;
    &lt;p&gt;
      Whether you are an experienced data scientist or just beginning your
      journey in data science and machine learning, WiHADS offers a welcoming
      community where you can grow, learn, and contribute. There are many ways
      to get involved with WiHADS and become part of this exciting movement.
    &lt;/p&gt;
    &lt;h2&gt;Ways to Participate:&lt;/h2&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;strong&gt;Attend a Meetup:&lt;/strong&gt;Join us at our next event to connect
        with fellow professionals and learn about the latest developments in
        data science and deep learning models for healthcare. Stay updated on
        upcoming events by visiting our
        &lt;a
          alt=&quot;Join WiHADS, a pioneering community promoting women in data science, healthcare analytics, and deep learning AI. Explore opportunities to lead data science projects, engage with deep learning models, and shape the future of AI in healthcare.&quot;
          href=&quot;https://www.meetup.com/fr-FR/Healthcare-Analytics-Data-Science/&quot;
          &gt;Meetup page&lt;/a
        &gt;.
      &lt;/li&gt;

      &lt;li&gt;
        &lt;strong&gt;Become a Speaker:&lt;/strong&gt; Share your knowledge and experience
        in data science projects or deep learning algorithms by speaking at one
        of our meetups. We are always looking for new voices to contribute to
        our discussions on AI in healthcare.
      &lt;/li&gt;

      &lt;li&gt;
        &lt;strong&gt;Sponsor an Event:&lt;/strong&gt; Help us continue to provide free,
        high-quality events by sponsoring a WiHADS meetup. Your support will
        enable us to reach more women in data science and expand our impact in
        the healthcare industry.
      &lt;/li&gt;
    &lt;/ul&gt;

    &lt;h2&gt;Conclusion: Shaping the Future of Data Science in Healthcare&lt;/h2&gt;
    &lt;p&gt;
      As we look to the future, data science and AI will continue to shape
      healthcare, and WiHADS is committed to ensuring that women in tech are at
      the forefront of this transformation. By joining us, you’re becoming part
      of a powerful movement dedicated to advancing deep learning algorithms and
      data science in healthcare. Together, we can lead the way in pioneering
      solutions that enhance patient care and drive innovation

      &lt;p&gt;&lt;strong&gt;Contact Us:&lt;/strong&gt; If you&apos;re interested in hosting a meetup or
      sponsoring an event, please reach out to us. We look forward to welcoming
      you to the WiHADS community.&lt;/p&gt;
    &lt;/p&gt;
    &lt;span class=&quot;image fit&quot;
      &gt;&lt;img
        src=&quot;/images/wihads3.jpeg&quot;
        alt=&quot;Group of women leaders in healthcare analytics and data science at a WiHADS meetup, fostering innovation in data science, deep learning AI, and healthcare.&quot;
        width=&quot;70%&quot;
    /&gt;&lt;/span&gt;

    &lt;!-- Go to www.addthis.com/dashboard to customize your tools --&gt;
    &lt;script
      type=&quot;text/javascript&quot;
      src=&quot;//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-584ec4ce89deed84&quot;
    &gt;&lt;/script&gt;
    &lt;!-- Go to www.addthis.com/dashboard to customize your tools --&gt;
    &lt;div class=&quot;addthis_inline_share_toolbox&quot;&gt;&lt;/div&gt;
  &lt;/section&gt;
&lt;/html&gt;
</description>
        <pubDate>Tue, 01 Oct 2019 00:00:00 +0200</pubDate>
        <link>http://urszulaczerwinska.github.io/works/women-healthcare-data-science-analytics-wihads</link>
        <guid isPermaLink="true">http://urszulaczerwinska.github.io/works/women-healthcare-data-science-analytics-wihads</guid>
        
        <category>data science</category>
        
        <category>events</category>
        
        <category>healthcare</category>
        
        <category>networking</category>
        
        
        <category>works</category>
        
      </item>
    
      <item>
        <title>Language gap between academia and business</title>
        <description>&lt;!DOCTYPE html&gt;
&lt;html&gt;
  &lt;head&gt;
    &lt;meta
      name=&quot;description&quot;
      content=&quot;An insider view on the language gap between academia and business, and how to bridge it from a data scientist perspective. | Transitioning from academia to industry.&quot;
    /&gt;
  &lt;/head&gt;

  &lt;body&gt;
    &lt;style&gt;
      .highlightme { background-color:#ccffe5; }
    &lt;/style&gt;

    &lt;section&gt;
      &lt;h2&gt;
        Transitioning from Academia to Industry: A Guide for Aspiring Data
        Scientists
      &lt;/h2&gt;
      &lt;p&gt;
        &lt;em
          &gt;Have you ever struggled to communicate your ideas effectively to
          business professionals when transitioning from academia to industry?
          If you&apos;re coming from an academic background and aiming for a career
          in the private sector, you may find the jargon and communication
          styles quite different.&lt;/em
        &gt;
      &lt;/p&gt;
      &lt;h3&gt;Understanding the Language Gap Between Academia and Business&lt;/h3&gt;
      &lt;p&gt;
        Academia and the private sector often speak different languages. In
        academia, the focus is on innovation and originality, leading to the use
        of terms like
        &lt;b&gt;novel&lt;/b&gt;, &lt;b&gt;insight&lt;/b&gt;, &lt;b&gt;paradigm&lt;/b&gt;, &lt;b&gt;robust&lt;/b&gt;, and
        &lt;b&gt;elucidate&lt;/b&gt;. Research often involves complex landscapes and
        frameworks, with a tendency to use words like &lt;b&gt;putative&lt;/b&gt;,
        &lt;b&gt;respectively&lt;/b&gt;, and &lt;b&gt;potentially&lt;/b&gt; to handle uncertainty and
        criticism.
      &lt;/p&gt;

      &lt;p&gt;
        On the other hand, the business world values clarity and efficiency.
        Terms such as &lt;b&gt;ballpark&lt;/b&gt;, &lt;b&gt;itemize&lt;/b&gt;, &lt;b&gt;prioritize&lt;/b&gt;,
        &lt;b&gt;agile&lt;/b&gt;, &lt;b&gt;incentivize&lt;/b&gt;, &lt;b&gt;proactive&lt;/b&gt;, and
        &lt;b&gt;empowerment&lt;/b&gt; reflect the need for quick solutions and strategic
        thinking. Business professionals focus on value, profit, and material
        gains, often using expressions like &lt;b&gt;push the envelope&lt;/b&gt; and
        &lt;b&gt;added value&lt;/b&gt;.
      &lt;/p&gt;
      &lt;div&gt;
        &lt;span class=&quot;image fit&quot;
          &gt;&lt;img
            src=&quot;/images/boss.jpg&quot;
            alt=&quot;Transitioning from academia to industry can be challenging but rewarding&quot;
        /&gt;&lt;/span&gt;
      &lt;/div&gt;

      &lt;h2&gt;The Critical Attitude Shift&lt;/h2&gt;

      &lt;p&gt;
        Academics are trained to be critical and challenge ideas rigorously.
        However, in the business world, being overly critical can be
        counterproductive. In business, you&apos;ll often hear terms like
        &lt;b&gt;opportunities for improvement&lt;/b&gt; rather than &lt;i&gt;pitfalls&lt;/i&gt;. Issues
        are described as &lt;b&gt;in development&lt;/b&gt; or &lt;b&gt;with growth potential&lt;/b&gt;,
        and even terminations are referred to as &lt;b&gt;restructuring&lt;/b&gt;. Adapting
        to this language and attitude is crucial for fitting into the corporate
        environment.
      &lt;/p&gt;

      &lt;h2&gt;Adapting to Corporate Culture&lt;/h2&gt;

      &lt;p&gt;
        Business professionals often prioritize efficiency and profit. Words
        like
        &lt;b&gt;ballpark&lt;/b&gt;, &lt;b&gt;itemize&lt;/b&gt;, &lt;b&gt;agile&lt;/b&gt;, and
        &lt;b&gt;takeaways&lt;/b&gt; emphasize task management and speed. Expressions such
        as &lt;b&gt;incentivize&lt;/b&gt;, &lt;b&gt;proactive&lt;/b&gt;, and &lt;b&gt;repurpose&lt;/b&gt; highlight
        the dynamic nature of corporate work. When discussing strategy, terms
        like &lt;b&gt;empowerment&lt;/b&gt; and &lt;b&gt;leverage&lt;/b&gt; are common, reflecting a
        focus on strategic impact and financial value.
      &lt;/p&gt;

      &lt;blockquote&gt;
        The difference in wording reflects different attitudes and methods used
        in business and research. However, the skills gained in research can be
        valuable in the business world.
        &lt;span class=&quot;highlightme&quot;
          &gt;&lt;b
            &gt;Mastering the art of communication can facilitate a smooth
            transition from academia to industry.&lt;/b
          &gt;&lt;/span
        &gt;
        Effective communication is crucial for team success and overall company
        performance.
      &lt;/blockquote&gt;

      &lt;p&gt;
        In academia, you often manage a project from start to finish,
        communicating with peers who are already familiar with your topic. In
        contrast, the corporate world involves working within a larger process,
        requiring clear communication and documentation. Corporate language
        often emphasizes organization and management.
      &lt;/p&gt;

      &lt;h2&gt;Crafting a Business-Ready CV&lt;/h2&gt;

      &lt;p&gt;
        When transitioning to a corporate role, your CV needs to shift from the
        academic format. Instead of a detailed list of projects and
        publications, focus on achievements and contributions that align with
        business needs. Follow
        &lt;a
          alt=&quot;Google&apos;s recommendations for tech jobs&quot;
          href=&quot;https://www.businessinsider.fr/us/google-exec-gives-key-to-perfect-resume-2014-9&quot;
          &gt;Google&apos;s recommendations&lt;/a
        &gt;
        for tech jobs, using a business tone and highlighting impact. For
        example, replace academic jargon with phrases like &apos;Achieved Z&apos; or
        &apos;Managed a team of N people&apos;.
      &lt;/p&gt;

      &lt;p&gt;
        In the business world, the emphasis is less on publications and more on
        the skills and logistics involved in achieving results. Break down your
        academic achievements into understandable and valuable skills for
        potential employers.
      &lt;/p&gt;

      &lt;div&gt;
        &lt;span class=&quot;image fit&quot;
          &gt;&lt;img
            src=&quot;/images/work.jpg&quot;
            alt=&quot;Effective CV writing is crucial for transitioning from academia to industry&quot;
        /&gt;&lt;/span&gt;
      &lt;/div&gt;

      &lt;h2&gt;Presenting Your Research&lt;/h2&gt;

      &lt;p&gt;
        When discussing your research, practice explaining it to non-academic
        audiences. Focus on the big picture and avoid technical jargon. Use
        everyday language and make parallels with business or everyday life.
        Consider preparing an
        &lt;a
          href=&quot;http://thepostdocway.com/content/elevator-pitches-scientists-what-when-where-and-how&quot;
          &gt;elevator pitch&lt;/a
        &gt;
        or reviewing
        &lt;a href=&quot;https://vimeo.com/threeminutethesis&quot;&gt;3MT&lt;/a&gt; videos for
        inspiration.
      &lt;/p&gt;

      &lt;h2&gt;Effective Presentations&lt;/h2&gt;

      &lt;p&gt;
        In the corporate world, presentations are crucial for persuading and
        informing. Unlike academic presentations, business presentations require
        a strong focus on the form and visual appeal. Ensure your slides are
        well-designed and that your presentation highlights the big picture
        before delving into details.
      &lt;/p&gt;

      &lt;h2&gt;Final Thoughts&lt;/h2&gt;

      &lt;p&gt;
        Although there is a significant gap between academia and business,
        bridging this divide can be rewarding. Adapting to business language and
        practices will open new doors and enhance your career opportunities.
        Engage with industry events, practice your communication skills, and be
        prepared for interviews to ensure a successful transition.
      &lt;/p&gt;

      &lt;div&gt;
        &lt;span class=&quot;image fit&quot;
          &gt;&lt;img
            src=&quot;/images/raising.jpg&quot;
            alt=&quot;Successfully transitioning from academia to industry opens new career opportunities&quot;
        /&gt;&lt;/span&gt;
      &lt;/div&gt;
	  &lt;p&gt;Also published on &lt;a alt=&quot;Discover more stories about Deep Learnig, Machine Learning and Data Science by Urszula Czerwinska&quot; href=&quot;https://medium.com/@ulalaparis/language-gap-between-academia-and-business-ef534ce71d10&quot;&gt;Medium&lt;/a&gt;&lt;/p&gt;
    &lt;/section&gt;
  &lt;/body&gt;
  &lt;!-- Go to www.addthis.com/dashboard to customize your tools --&gt;
  &lt;script
    type=&quot;text/javascript&quot;
    src=&quot;//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-584ec4ce89deed84&quot;
  &gt;&lt;/script&gt;

  &lt;!-- Go to www.addthis.com/dashboard to customize your tools --&gt;
  &lt;div class=&quot;addthis_inline_share_toolbox&quot;&gt;&lt;/div&gt;
&lt;/html&gt;
</description>
        <pubDate>Sat, 19 Jan 2019 00:00:00 +0100</pubDate>
        <link>http://urszulaczerwinska.github.io/thoughts/businesslang</link>
        <guid isPermaLink="true">http://urszulaczerwinska.github.io/thoughts/businesslang</guid>
        
        
        <category>thoughts</category>
        
      </item>
    
      <item>
        <title>PhD Thesis</title>
        <description>&lt;!DOCTYPE HTML&gt;
&lt;html lang=&quot;en-US&quot;&gt;
    &lt;head&gt;
        &lt;meta charset=&quot;UTF-8&quot;&gt;
        &lt;meta http-equiv=&quot;refresh&quot; content=&quot;1; url=&quot;&gt;
        &lt;script type=&quot;text/javascript&quot;&gt;
            window.location.href = &quot;https://urszulaczerwinska.github.io/UCzPhDThesis/&quot;
        &lt;/script&gt;
        &lt;title&gt;Redirecting to a page on NER project at Cour de Cassation webpage that explains in details the approach of NLP Engineering Data Science Project&lt;/title&gt;
    &lt;/head&gt;
    &lt;body&gt;
        &lt;!-- Note: don&apos;t tell people to `click` the link, just tell them that it is a link. --&gt;
        If you are not redirected automatically, follow this &lt;a href=&apos;https://urszulaczerwinska.github.io/UCzPhDThesis/&apos;&gt;link to example&lt;/a&gt;.
          &lt;!-- Go to www.addthis.com/dashboard to customize your tools --&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-584ec4ce89deed84&quot;&gt;&lt;/script&gt;
&lt;!-- Go to www.addthis.com/dashboard to customize your tools --&gt; &lt;div class=&quot;addthis_inline_share_toolbox&quot;&gt;&lt;/div&gt;

    &lt;/body&gt;
&lt;/html&gt;</description>
        <pubDate>Thu, 02 Aug 2018 00:00:00 +0200</pubDate>
        <link>http://urszulaczerwinska.github.io/works/PhDThesis.html</link>
        <guid isPermaLink="true">http://urszulaczerwinska.github.io/works/PhDThesis.html</guid>
        
        <category>Machine Learning</category>
        
        <category>R</category>
        
        <category>Data Science</category>
        
        <category>biomedical applications</category>
        
        
        <category>works</category>
        
      </item>
    
  </channel>
</rss>
