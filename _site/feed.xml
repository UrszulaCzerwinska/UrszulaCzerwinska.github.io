<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Urszula Czerwinska | Data Scientist &amp; Deep Learning Engineer</title>
    <description>Explore the journey of Urszula Czerwinska from PhD to Data Science, featuring insights on Data Science projects, Machine Learning, and Deep Learning. Discover how to become a Data Scientist or Machine Learning Engineer.</description>
    <link>http://urszulaczerwinska.github.io/</link>
    <atom:link href="http://urszulaczerwinska.github.io/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Tue, 27 Aug 2024 23:34:49 +0200</pubDate>
    <lastBuildDate>Tue, 27 Aug 2024 23:34:49 +0200</lastBuildDate>
    <generator>Jekyll v4.3.3</generator>
    
      <item>
        <title>Deep Dive in PaddleOCR inference</title>
        <description>&lt;head&gt;
  &lt;meta name=&quot;description&quot; content=&quot;A deep dive into the complexities of using PaddleOCR for text extraction from images and how the Cognition team improved the service. Learn about the challenges and solutions that enhanced user experience in OCR services.&quot; /&gt;
&lt;/head&gt;

&lt;p&gt;This article is a deep dive into part of our work as described in &lt;a href=&quot;/works/text-in-image-2-0-improving-ocr-service-with-paddleocr&quot;&gt;&lt;strong&gt;Article 1: Text in Image 2.0: improving OCR service with PaddleOCR&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We are Cognition, an &lt;a href=&quot;https://www.adevinta.com/&quot;&gt;Adevinta&lt;/a&gt; Computer Vision Machine Learning (ML) team working on solutions for our marketplaces. Adevinta is a global classifieds specialist with market-leading positions in key European markets that aims to find perfect matches between its users and the platforms’ goods. As a Global Team, our team, Cognition, provides image processing APIs to all of our marketplaces.&lt;/p&gt;

&lt;p&gt;In the process of improving our OCR API for text extraction from images, we updated our existing Text in Image service to the &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR&quot;&gt;PaddleOCR&lt;/a&gt; framework, which was the winner of our benchmarks. In order to test if this framework was the most suitable solution, we carried out a deeper analysis of their code base. This article shares the challenges we encountered and how we overcame them.&lt;/p&gt;

&lt;p&gt;We believe our code version is easier to work with, given the use case of text extraction from images. The different steps and pre-processing and post-processing parts are clearly separated so they can be called independently, which should make further community extensions easier to add. It also makes putting into production easier, as the simplified, modular code combines well with the structure of &lt;em&gt;inference.py&lt;/em&gt; for serving SageMaker endpoints. Our proposed code version does not alter predictions (compared to the 2.6 release) for images.&lt;/p&gt;

&lt;h2 id=&quot;understanding-the-paddleocr-framework&quot;&gt;Understanding the PaddleOCR framework&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/PaddlePaddle/Paddle&quot;&gt;PaddlePaddle&lt;/a&gt; (short for Parallel Distributed Deep Learning) is an open source deep learning platform developed by Baidu Research. It is written in C++ and Python, and is designed to be easy to use and efficient for large-scale machine learning tasks.&lt;/p&gt;

&lt;p&gt;PaddlePaddle provides a range of tools and libraries for building and training deep learning models, including support for convolutional neural networks (CNNs), recurrent neural networks (RNNs) and long short-term memory (LSTM) networks.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR&quot;&gt;PaddleOCR&lt;/a&gt; builds on PaddlePaddle, an unfamiliar framework that our team had not used before. To make things even more challenging, PaddleOCR is not just one algorithm, it includes a range of pre-trained models and tools for recognising text in images and documents, as well as for training custom OCR models.&lt;/p&gt;

&lt;p&gt;PaddleOCR is divided into two main sections:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;PP-OCR&lt;/strong&gt;, an OCR system used for text extraction from images&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;PP-Structure&lt;/strong&gt;, a document analysis system which aims to perform layout analysis and table recognition&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;PP-OCR exists in three different versions (V1, V2 and V3). In these different releases, major improvements were brought to the models’ architecture.&lt;/p&gt;

&lt;p&gt;For our Text in Image service update, we focused on the most recent and most performant PP-OCRv3 release.&lt;/p&gt;

&lt;h3 id=&quot;the-paddleocrv3-models-architecture&quot;&gt;The PaddleOCRv3 models architecture&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*1mI3YTIjAut_QMrl&quot; alt=&quot;PaddleOCRv3 Architecture&quot; /&gt;&lt;/p&gt;

&lt;p&gt;PP-OCRv3 is composed of three parts: detection, classification and recognition, all of which can be used independently. Each part has its own model trained with the PaddlePaddle framework. For those interested, model details can be found in this dedicated research article PP-OCRv3: &lt;a href=&quot;https://arxiv.org/abs/2206.03001v2&quot;&gt;More Attempts for the Improvement of Ultra Lightweight OCR System (Yanjun et al., 2022)&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;PP-OCRv3 text detection is made with the Differentiable Binarization algorithm (&lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.6/doc/doc_en/algorithm_det_db_en.md&quot;&gt;DB&lt;/a&gt;) trained using distillation strategy. The PP-OCRv3 recogniser is optimised based on the text recognition algorithm, Scene Text Recognition with a Single Visual Model (&lt;a href=&quot;https://arxiv.org/abs/2205.00159&quot;&gt;SVTR, Du et al. 2022)&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;PP-OCRv3 adopts the text recognition network SVTR_LCNet, and uses &lt;a href=&quot;https://arxiv.org/abs/2002.01276&quot;&gt;the guided training of Connectionist Temporal Classification (CTC&lt;/a&gt;, Z&lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Lin%2C+Z&quot;&gt;hiping&lt;/a&gt; et al., 2020) by the attention, data augmentation strategy, TextConAug, Unified Deep Mutual Learning and Unlabelled Images Mining (first introduced in &lt;a href=&quot;https://arxiv.org/abs/2109.03144&quot;&gt;PaddleOCRv2, Yanjun et al. 2021&lt;/a&gt;). The Text classifier is a simple binary classifier with classes 0 and 180°.&lt;/p&gt;

&lt;h3 id=&quot;paddleocr-inference-in-practice&quot;&gt;PaddleOCR inference in practice&lt;/h3&gt;

&lt;p&gt;While testing on our benchmarks, we used the PaddleOCR code for inference with default parameters and “latin” as a language (see their &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.6/doc/doc_en/quickstart_en.md&quot;&gt;QuickStart page&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Reading the documentation and looking into the class parameters, we saw lots of model combinations to test and therefore more opportunities to potentially improve our score.&lt;/p&gt;

&lt;p&gt;For instance, the &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.6/doc/doc_en/detection_en.md&quot;&gt;documentation&lt;/a&gt; suggests there is a choice between “DB” and “EAST” algorithms for detection, but it’s only the main inference &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.6/paddleocr.py&quot;&gt;script&lt;/a&gt; where the algorithm has to be “DB” — the &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/blob/23e034c40ecd5755af48d7b14dcc1bf6c5cf1128/tools/infer/predict_det.py#L62&quot;&gt;script&lt;/a&gt; of detection inference goes through a long list of algorithms. A similar situation occurs with text recognition where the pre-trained algorithm for Latin is “SVTR_LCNet”, but in &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/blob/23e034c40ecd5755af48d7b14dcc1bf6c5cf1128/paddleocr.py#L51&quot;&gt;theory&lt;/a&gt;, the accepted values are “‘CRNN’ and ‘SVTR_LCNet’ with the general &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/blob/18ddb6d5f9bdc2c1b0aa7f6e399ec0f76119dc87/doc/doc_en/inference_args_en.md&quot;&gt;documentation&lt;/a&gt; mentioning a plethora of models.&lt;/p&gt;

&lt;p&gt;Pre-trained English models are available in “‘CRNN’ and ‘SVTR_LCNet’ architectures. However, to find the information, the user would need to look into the pretrained model &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/blob/18ddb6d5f9bdc2c1b0aa7f6e399ec0f76119dc87/configs/rec/PP-OCRv3/en_PP-OCRv3_rec.yml#L39&quot;&gt;config&lt;/a&gt;. If the user does not specify the “rec_algorithm”, the default value, “SVTR_LCNet”, would be used, even if it isn’t correct. This doesn’t actually make any difference to the inference &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.6/tools/infer/predict_rec.py&quot;&gt;code&lt;/a&gt; as none of the “if” applies to ‘CRNN’ or ‘SVTR_LCNet’.&lt;/p&gt;

&lt;p&gt;In order to test a different architecture, we would need to train it ourselves and chain dedicated scripts.&lt;/p&gt;

&lt;h2 id=&quot;clarifying-paddleocr-inference&quot;&gt;Clarifying PaddleOCR inference&lt;/h2&gt;

&lt;p&gt;From digging into the code, we discovered several complexities, unnecessary for our use case. Firstly, the code seemed to grow organically, where the inference version is a limited choice entry to the multi-option code. This leaves us with numerous “factory patterns” and “if .. elses”, where the user has no choice at all. The English documentation was confusing and referenced different usage cases. We struggled to follow the logic as it neither explained parameters, nor clearly defined the limitations of the inference code.&lt;/p&gt;

&lt;p&gt;Despite these complexities, we managed to clarify the general way of working, calling the PaddleOCR.ocr() method from the ‘master’ file, &lt;em&gt;paddleocr.py&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*zwImfJ-4pOxDvrEI&quot; alt=&quot;PaddleOCR.ocr() Method&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The input image and parameters are entered into the PaddleOCR.ocr() method which calls TextSystem class in order: TextDetector, TextClassifier and TextRecogniser, with a selection of helper functions, including one that formats the outputs of TextDetector into a list of cropped images being input to TextClassifier and TextRecogniser.&lt;/p&gt;

&lt;p&gt;The PaddleOCR.ocr() method is parsing params, including the language, version, type of OCR (or structure), downloads inference models and imports actual image (with check_image).&lt;/p&gt;

&lt;p&gt;If we want our image to go through a full OCR process, the TextSystem class will sequentially call classes responsible for detection, classification and recognition.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*B-7pY0A4Xv7eNTcr&quot; alt=&quot;TextSystem Class Flow&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Each of the main classes has an &lt;em&gt;__init__&lt;/em&gt; method that initialises pre- &amp;amp; post- processing classes and loads the model (create_predictor), and &lt;em&gt;__call__&lt;/em&gt; method that executes (pre- &amp;amp;) post-processing on the image and performs the model inference for the input image(s).&lt;/p&gt;

&lt;p&gt;Most of the scripts used for inference can be found under ‘tools/infer/’. The pre-processing scripts are under “ppocr/data/imaug/operators.py”. The post-processing classes are under ‘ppocr/postprocess/’.&lt;/p&gt;

&lt;p&gt;This schema enables us to reduce the essential inference code to just a couple of files and better understand exactly how the code works. To make it easier to maintain, we decided to reformat the code, keeping only the essential parts for our use case.&lt;/p&gt;

&lt;h2 id=&quot;paddleocr-inference-code-caveats-andfixes&quot;&gt;PaddleOCR inference code caveats and fixes&lt;/h2&gt;

&lt;p&gt;Let’s walk you through the PaddleOCR features we didn’t like and suggestions on how they could be improved.&lt;/p&gt;

&lt;h3 id=&quot;spaghetti-code&quot;&gt;Spaghetti code&lt;/h3&gt;

&lt;p&gt;Overall, most of the code is in object oriented programming style where classes are not modular and most things happen in very long &lt;em&gt;__init__&lt;/em&gt; and &lt;em&gt;__call__&lt;/em&gt; methods. We have noticed (fig. 2 and fig. 3) that generally, three parts can be extracted: pre-processing, inference and post-processing. We have removed ‘create_operators’ and ‘build_post_process’ intermediate functions and called directly the class performing the task such as “DBPostprocess” and “NormalizeImage”. To make things more straightforward, we transformed them into simple functions, performing what their &lt;em&gt;__call__&lt;/em&gt; method was doing before. This leaves us with more modular code and direct logic that fits our needs.&lt;/p&gt;

&lt;h3 id=&quot;parameter-parsing&quot;&gt;&lt;em&gt;Parameter parsing&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;We found it problematic that the inference class requires 105 parameters, of which more than 70 were ignored.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*jPMJx-wOF-R5DsmqJFs5BA.png&quot; alt=&quot;PaddleOCR inference parameters are not all used&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/blob/18ddb6d5f9bdc2c1b0aa7f6e399ec0f76119dc87/doc/doc_en/inference_args_en.md&quot;&gt;English documentation&lt;/a&gt; lists the parameters and gives a succinct definition of them. In the code, they are defined in at least three different places: &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/blob/23e034c40ecd5755af48d7b14dcc1bf6c5cf1128/paddleocr.py#L307&quot;&gt;paddleocr.py&lt;/a&gt;, &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/blob/23e034c40ecd5755af48d7b14dcc1bf6c5cf1128/ppstructure/utility.py#L21&quot;&gt;utility.py&lt;/a&gt; and different &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/blob/34b9569800a38af41a27ed893b12567757ef6c89/tools/infer/utility.py#L34&quot;&gt;utility.py&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;However while executing the code, we found that only 20 parameters were useful in our refactored code:&lt;/p&gt;

&lt;p&gt;When rewriting the code, we cleaned the parameter list, leaving only the relevant parameters.&lt;/p&gt;

&lt;h3 id=&quot;parameter-impact-on-prediction&quot;&gt;&lt;em&gt;Parameter impact on prediction&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;Some of the parameter definitions and effect they would have when changed from default, were not clear to us. We built a &lt;a href=&quot;https://streamlit.io/&quot;&gt;Streamlit app&lt;/a&gt; to visualise the changes in params on the predictions. For instance, “unclip ratio” would impact the size of the box, and “threshold” would detect two bounding boxes instead of one. We advise you to play with your own data and model to see how different parameters affect the detection. Overall, we were not able to see a major improvement from changing defaults.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*B4uqn-7vcxfu5aPz&quot; alt=&quot;The illustration of PaddleOCR parameters impact on the machine learning model prediction&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;language-choice&quot;&gt;&lt;em&gt;Language choice&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;Normally in our role, we work with “‘PP-OCRv3”, the most recent version of the framework. As we are dealing with European languages, we would choose “fr”, “en”, “es” as the “lang” param, thinking that this means different models are being called. However, while looking into the paddleocr.py, we saw how the languages are interpreted:&lt;/p&gt;

&lt;p&gt;The first definition serves to define the recognition model name/path. But if we typed “fr” or “es”, it becomes lang = “latin”, yet “en” remains “en”. Then another simplification happens for the detection model.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;if lang in [“en”, “latin”]:&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;det_lang = “en”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;We are left with an English detection model and a Latin recognition model for any European language written with Latin characters except English, which has its own recognition model.&lt;/p&gt;

&lt;h3 id=&quot;downloading-models&quot;&gt;&lt;em&gt;Downloading models&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;Based on the language parameter and framework version, the first time we call the PaddleOCR class with those parameters, the model will be downloaded from the url encoded in paddleocr.py.&lt;/p&gt;

&lt;p&gt;Firstly, this could cause some issues when running the code in secure or offline environments.&lt;/p&gt;

&lt;p&gt;Secondly, we found inconsistencies between the model urls in the paddleocr.py and the models provided in the dedicated &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/blob/18ddb6d5f9bdc2c1b0aa7f6e399ec0f76119dc87/doc/doc_en/models_list_en.md&quot;&gt;documentation page&lt;/a&gt;. For instance, “en_PP-OCRv3_det_slim” is not an option when models are downloaded by the paddleocr.py script. In order to use some of the models from Model Zoo, a database of pre-trained models and code, you would need to download the model and provide the path to it manually.&lt;/p&gt;

&lt;p&gt;In order to remove this ambiguity and use the specific model we needed, we decided to pre-download the chosen model, then provide the path directly. In the original code, it is possible to provide det_model_dir, cls_model_dir and rec_model_dir. The language param will then be ignored and any pre-trained model with the accepted backbones can be used. After this process, we removed the model download functionality from our code.&lt;/p&gt;

&lt;h3 id=&quot;using-onnxmodels&quot;&gt;&lt;em&gt;Using ONNX models&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;PaddleOCR provides a &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/blob/18ddb6d5f9bdc2c1b0aa7f6e399ec0f76119dc87/deploy/paddle2onnx/readme.md&quot;&gt;handy way&lt;/a&gt; to export models to the &lt;a href=&quot;https://onnx.ai/&quot;&gt;ONNX framework&lt;/a&gt; that can serve or integrate in different pipelines. We exported the pre-trained models using PaddleOCR instructions. In the PaddleOCR class, there is a parameter “use_onnx”. If one sets “use_onnx” and provides a direct path to the ONNX models to PaddleOCR(), the model would use the ONNX model for prediction. However, there is a small bug that occurs while running ONNX with GPUs, described further in this &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/issues/8688&quot;&gt;issue&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We applied the modification suggested and tested the code with ONNX models, obtaining satisfactory results on both CPU and GPU (even though we noticed small numerical differences between the Paddle and ONNX model versions).&lt;/p&gt;

&lt;h3 id=&quot;documentation&quot;&gt;&lt;em&gt;Documentation&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;If you look at the &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/tree/18ddb6d5f9bdc2c1b0aa7f6e399ec0f76119dc87/doc&quot;&gt;documentation pages&lt;/a&gt;, you will find a lot of resources in both English and Chinese. However, when looking at &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/issues&quot;&gt;Issues&lt;/a&gt;, you will find most of them are in Chinese, Japanese or Korean. The same applies to blog posts and community resources online. We also found that some documentation is only partially translated to English and the Chinese version contains much more detail.&lt;/p&gt;

&lt;p&gt;We did not find a solution for this. We made sure to always check both the English and Chinese documentation (translated to English by an automatic translator) to ensure that we have all the possible information.&lt;/p&gt;

&lt;h3 id=&quot;tests--pylint-typing&quot;&gt;&lt;em&gt;Tests &amp;amp;&lt;/em&gt; &lt;a href=&quot;https://pylint.pycqa.org/en/latest/&quot;&gt;&lt;em&gt;pylint&lt;/em&gt;&lt;/a&gt; &lt;em&gt;&amp;amp;&lt;/em&gt; &lt;a href=&quot;https://docs.python.org/3/library/typing.html&quot;&gt;&lt;em&gt;typing&lt;/em&gt;&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;In general, as the original code is not modular, it was not tested according to the standards of our team. Once we cleaned and simplified the code, we worked on linting and variable typing. Our next step will be to write meaningful unit tests to secure the code base.&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;PaddleOCR is a powerful and optimised library for the extraction of text from images. However, we found that the code doesn’t fit the standards of our team as it is too complex to maintain and understand. In this article, we pointed out some of the pain points for us that other PaddleOCR users may experience when working with this framework. The fixes we proposed made our lives easier and the code more transparent for any team member and the wider community, without compromising the speed or the original model accuracy.&lt;/p&gt;

&lt;footer&gt;
  &lt;p&gt;Exported from &lt;a href=&quot;https://medium.com&quot;&gt;Medium&lt;/a&gt; on June 06,
    2023.&lt;/p&gt;
  &lt;p&gt;&lt;a href=&quot;https://medium.com/adevinta-tech-blog/deep-dive-in-paddleocr-inference-e86f618a0937&quot;&gt;View
      the original&lt;/a&gt;&lt;/p&gt;
&lt;/footer&gt;
&lt;script type=&quot;text/javascript&quot; src=&quot;//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-584ec4ce89deed84&quot;&gt;&lt;/script&gt;

&lt;div class=&quot;addthis_inline_share_toolbox&quot;&gt;&lt;/div&gt;

</description>
        <pubDate>Mon, 06 Mar 2023 00:00:00 +0100</pubDate>
        <link>http://urszulaczerwinska.github.io/works/deep-dive-in-paddleocr-inference</link>
        <guid isPermaLink="true">http://urszulaczerwinska.github.io/works/deep-dive-in-paddleocr-inference</guid>
        
        <category>data science</category>
        
        <category>OCR</category>
        
        <category>deep learning</category>
        
        <category>computer vision</category>
        
        <category>machine learning</category>
        
        <category>featured</category>
        
        
        <category>works</category>
        
      </item>
    
      <item>
        <title>Text in Image 2.0 - improving OCR service with PaddleOCR</title>
        <description>&lt;head&gt;
  &lt;meta name=&quot;description&quot; content=&quot;Discover how the Cognition team at Adevinta enhanced the Text in Image service using PaddleOCR, leading to significant improvements in OCR accuracy and performance.&quot; /&gt;
&lt;/head&gt;

&lt;h2 id=&quot;understanding-ocr-what-is-optical-character-recognition&quot;&gt;Understanding OCR: What is Optical Character Recognition?&lt;/h2&gt;

&lt;p&gt;Optical Character Recognition (OCR) is a popular topic for both industry and personal use. In this article, we share how we tested and used an existing open source library, PaddleOCR, to extract text from an image. This read is for anyone who would like to find out more about OCR, the needs of our customers at &lt;a href=&quot;https://www.adevinta.com/&quot;&gt;Adevinta&lt;/a&gt;, and the challenges we face in attending to them. You’ll find out how we upgraded an existing service, benchmarked different solutions and delivered the selected one to satisfy our customers.&lt;/p&gt;

&lt;h2 id=&quot;key-ocr-applications-how-ocr-transforms-business-and-daily-operations&quot;&gt;Key OCR applications: How OCR transforms business and daily operations&lt;/h2&gt;

&lt;p&gt;OCR stands for “Optical Character Recognition” and is a technology that allows computers to recognise and extract text from images and scanned documents. OCR software uses optical recognition algorithms to interpret the text in images and convert it into machine-readable text that can be edited, searched and stored electronically.&lt;/p&gt;

&lt;p&gt;There are numerous use-cases where OCR can be used:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Digitising paper documents&lt;/strong&gt;: to convert scanned images of text into digital text. This is useful for organisations that want to reduce their reliance on paper and improve their document management processes.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Extracting data from images&lt;/strong&gt;: eg from documents such as invoices, receipts and forms. This can be useful for automating data entry tasks and reducing the need for manual data entry.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Translating documents&lt;/strong&gt;: to extract text from images of documents written in foreign languages and translate them into a different language.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Archiving&lt;/strong&gt;: to create digital copies of important documents that need to be preserved for long periods of time.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Improving accessibility&lt;/strong&gt;: to make scanned documents more accessible to people with disabilities by converting the text into a format that can be read by assistive technologies such as screen readers.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Searching documents&lt;/strong&gt;: to make scanned documents searchable, allowing users to easily find specific information within a large collection of documents.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;the-adevinta-context-why-ocr-matters-in-global-marketplace&quot;&gt;The Adevinta context: Why OCR matters in global marketplace&lt;/h2&gt;

&lt;p&gt;Within &lt;a href=&quot;https://www.adevinta.com/&quot;&gt;Adevinta&lt;/a&gt;, a global classifieds specialist with market-leading positions in key European markets, there is space for all of the cited use cases. However, for this article, we focus specifically on “extracting data from images.”&lt;/p&gt;

&lt;p&gt;Applying deep learning to images is the main expertise of our team, Cognition. We are Data Scientists and Machine Learning (ML) Engineers that work together to develop image-based ML solutions at scale, helping Adevinta’s marketplaces build better products and experiences for their customers. Adevinta’s mission is to connect buyers and sellers, enabling people to find jobs, homes, cars, consumer goods and more. By making an accessible ML API with features tailored to our different marketplaces’ needs, Adevinta’s marketplaces are empowered with ML tools at a reasonable cost.&lt;/p&gt;

&lt;h2 id=&quot;text-extraction-in-images-why-its-crucial-for-adevintas-services&quot;&gt;Text Extraction in Images: Why It’s Crucial for Adevinta’s Services&lt;/h2&gt;
&lt;p&gt;Text extraction from images enables us to:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Detect unwanted content in ads (e.g., insults, hidden messages).&lt;/li&gt;
  &lt;li&gt;Better understand image content to improve search capabilities.&lt;/li&gt;
  &lt;li&gt;Support more efficient searches using visible text on items.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With over 100 million requests per month and growing, our existing Text in Image service was ripe for enhancement. We aimed to improve accuracy and performance, leading to the development of Text in Image 2.0.&lt;/p&gt;

&lt;h2 id=&quot;why-we-chose-paddleocr-benchmarking-the-best-ocr-solution&quot;&gt;Why we chose PaddleOCR: Benchmarking the best OCR solution&lt;/h2&gt;

&lt;p&gt;The existing service was based on &lt;a href=&quot;https://arxiv.org/abs/1801.01671&quot;&gt;Fast Oriented Text Spotting with a Unified Network (Yan et al., 2018)&lt;/a&gt;. Despite being state of the art in 2018, the algorithm achieved 0.4 accuracy on our internal benchmark of 200 marketplace images. Nevertheless, accuracy was not the sole criteria of choice for the Text in Image 2.0, so we compiled a list of edge cases where our partner marketplaces require high-performing algorithms.&lt;/p&gt;

&lt;p&gt;After reviewing different open source OCR frameworks (including &lt;a href=&quot;https://github.com/open-mmlab/mmocr&quot;&gt;MMOCR&lt;/a&gt;, &lt;a href=&quot;https://github.com/JaidedAI/EasyOCR&quot;&gt;EASY OCR&lt;/a&gt;, &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR&quot;&gt;PaddleOCR&lt;/a&gt; and &lt;a href=&quot;https://thehive.ai/apis/ocr&quot;&gt;HiveOCR&lt;/a&gt;) and different combinations of proposed models on our internal benchmark and on the edge cases, a indisputable winner was PaddleOCR with an average accuracy of 0.8 and an acceptable performance on our edge cases. This result competes with the paid &lt;a href=&quot;https://cloud.google.com/vision/docs/ocr&quot;&gt;Google Cloud Vision OCR API&lt;/a&gt; on the best accuracy we measured.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*UUEf-TKs1Lfn7_wx&quot; alt=&quot;Graph showing benchmark results for various OCR frameworks&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;how-we-validated-paddleocr-building-a-comprehensive-benchmark&quot;&gt;How We Validated PaddleOCR: Building a Comprehensive Benchmark&lt;/h2&gt;

&lt;p&gt;In order to construct our independent benchmark and validate the choice of PaddleOCR at scale, we built a “Text in Image generator” that uses open source images from &lt;a href=&quot;https://unsplash.com/license&quot;&gt;Unsplash&lt;/a&gt; and &lt;a href=&quot;https://pikwizard.com/free-license&quot;&gt;Pikwizard&lt;/a&gt; and adds randomly generated text on top of them. The created tool is highly customisable in order to simulate a wide variety of cases that combine factors such as font type, rotation, text length, background type, image resolution etc. Using a simulated benchmark of 20k images with a distribution of cases matching business needs, we obtained an improvement factor of x1.4.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*sWpBlrJtdxsRlqj4&quot; alt=&quot;Sample of Text in Image generator output showing simulated text scenarios&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;challenges-with-paddleocr-identifying-and-mitigating-issues&quot;&gt;Challenges with PaddleOCR: Identifying and mitigating issues&lt;/h2&gt;

&lt;p&gt;We identified several cases where PaddleOCR fails. This is mostly when there are different angles of rotated text, some alternative fonts and differing colour/contrast. We also observed that in some cases, the correct words are detected but the spaces between them are not placed correctly. This may or may not be an issue depending on the way the extracted text is used further.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*3CO2dWUYPpVPPBZJDpx4EA.png&quot; alt=&quot;Example of OCR results with incorrectly spaced text&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;deep-dive-how-we-optimized-paddleocr-for-production&quot;&gt;Deep Dive: How We Optimized PaddleOCR for Production&lt;/h2&gt;

&lt;p&gt;In order to evaluate the potential for improvement and mitigation of these errors, in addition to defining the serving strategy, we had to deep dive into the PaddleOCR framework.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR&quot;&gt;PaddleOCR&lt;/a&gt; builds on &lt;a href=&quot;https://github.com/PaddlePaddle/Paddle&quot;&gt;PaddlePaddle.&lt;/a&gt; Our team had no previous experience with this and it’s less popular in our community than other frameworks such as Tensorflow, Keras or Pytorch.&lt;/p&gt;

&lt;p&gt;From a technical point of view, PaddleOCR is composed of three distinct models:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Detection&lt;/strong&gt;, for detecting a bounding box where possible text is&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Classification&lt;/strong&gt;, rotating the text 180° if necessary&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Recognition&lt;/strong&gt;, translating the detected image frame to raw text&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Pre-trained models in different languages are &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/blob/18ddb6d5f9bdc2c1b0aa7f6e399ec0f76119dc87/doc/doc_en/models_list_en.md&quot;&gt;provided by authors&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;refactoring-paddleocr-creating-a-clean-production-ready-codebase&quot;&gt;Refactoring PaddleOCR: Creating a Clean, Production-Ready Codebase&lt;/h3&gt;

&lt;p&gt;Whilst exploring the code base of PaddleOCR for inference, we were faced with convoluted code, which was difficult to read and understand. As we wanted to use the PaddleOCR solution in production, we decided to refactor the code, keeping in mind to preserve the performance and the speed of the original code. You can read about the details of that process and the PaddleOCR model in the complementary article of this series. After refactoring the code, we had created a clean and readable code base.&lt;/p&gt;

&lt;p&gt;We believe our code version is easier to work with, given the use case of text extraction from images, and are working on making the code available open source. The different steps and pre-processing and post-processing parts are clearly separated, so they can be called independently, which should make further community extensions easier to add. It also makes putting into production easier, as the simplified, modular code combines well with the structure of inference.py for serving SageMaker endpoints. Our proposed code version does not alter predictions (compared to the 2.6 release) for images.&lt;/p&gt;

&lt;h2 id=&quot;deploying-text-in-image-20-achieving-superior-performance-with-paddleocr&quot;&gt;Deploying Text in Image 2.0: Achieving Superior Performance with PaddleOCR&lt;/h2&gt;

&lt;p&gt;Using the refactored code, we made the model available as an API. To help our customers’ transition, we maintained the same API contract used in the previous service.&lt;/p&gt;

&lt;p&gt;Serving PaddleOCR can be done in multiple ways. The straightforward approach is calling its own Python API (provided by the &lt;a href=&quot;https://pypi.org/project/paddleocr/&quot;&gt;PaddleOCR&lt;/a&gt; package) from within a well-known framework. We selected Multi Model Server, Flask and FastAPI to conduct our benchmark. All our proposed solutions are served by AWS SageMaker Endpoint, building our own container (BYOC) from the same Docker base image.&lt;/p&gt;

&lt;p&gt;MultiModel Server uses its own JAVA ModelServer, while for Flask and FastAPI, we use nginx+gunicorn (combined with &lt;a href=&quot;https://fastapi.tiangolo.com/deployment/server-workers/&quot;&gt;uvicorn workers for the ASGI FastAPI&lt;/a&gt;). The frontend for our customers is served by an API Gateway, which is out of the scope of this article.&lt;/p&gt;

&lt;h2 id=&quot;benchmarking-deployment-options-multi-model-server-flask-and-fastapi&quot;&gt;Benchmarking Deployment Options: Multi-Model Server, Flask, and FastAPI&lt;/h2&gt;

&lt;p&gt;For the performance testing, we recreated a number of requests with a controlled amount of text and different image sizes, mimicking the expected distribution from our customers. We used &lt;a href=&quot;https://locust.io/&quot;&gt;Locust&lt;/a&gt; as the testing framework, and stimulated heavy bursts in the &lt;a href=&quot;https://docs.locust.io/en/stable/writing-a-locustfile.html#wait-time-attribute&quot;&gt;waiting time&lt;/a&gt; as a stress test.&lt;/p&gt;

&lt;p&gt;With the data gathered from the performance tests, we were able to define our infrastructure (type of instance and autoscaling policy) in relation to the Service Level Agreement (SLA) terms, while balancing the risk of a sudden shift from the observed distribution (the service is sensitive to the amount of text per image).&lt;/p&gt;

&lt;p&gt;Currently, we deal with 330 million requests per month, and we have estimated that next year, more Adevinta marketplaces will onboard a Text in Image service, resulting in a 400% growth.&lt;/p&gt;

&lt;h2 id=&quot;results-and-impact-transforming-text-in-image-service-with-paddleocr&quot;&gt;Results and impact: Transforming Text in Image service with PaddleOCR&lt;/h2&gt;

&lt;p&gt;The new API resulted in an improved latency 7.5x compared to the FOTS-based solution, while providing a 7% cost reduction in serving. Also, since the new API being 12x cheaper than a typical external solution, such as GCP OCR, we received positive feedback from our users about both the speed and the accuracy of the Text in Image 2.0.&lt;/p&gt;

&lt;h2 id=&quot;key-takeaways-enhancing-ocr-with-paddleocr&quot;&gt;Key Takeaways: Enhancing OCR with PaddleOCR&lt;/h2&gt;

&lt;p&gt;As a computer vision team working for an international company serving millions of people every day, we aimed to improve our OCR API for text extraction from classified ads. After testing numerous frameworks, we built an image simulator in order to find the algorithm matching the needs of our users. The selected framework, PaddleOCR, went through our internal review and revamp. (There were challenges along the way and you can read more about them in &lt;a href=&quot;/works/deep-dive-in-paddleocr-inference&quot;&gt;&lt;strong&gt;Article 2: Deep Dive in PaddleOCR inference&lt;/strong&gt;&lt;/a&gt;). Now, we’re pleased to say we’re providing a more accurate, faster and cheaper API using the PaddleOCR framework.&lt;/p&gt;

&lt;footer&gt;
  &lt;p&gt;Exported from &lt;a href=&quot;https://medium.com&quot;&gt;Medium&lt;/a&gt; on June 06,
    2023.&lt;/p&gt;
  &lt;p&gt;&lt;a href=&quot;https://medium.com/adevinta-tech-blog/text-in-image-2-0-improving-ocr-service-with-paddleocr-61614c886f93&quot;&gt;This article was orignally co-authored by Cognition team members, special credits to Joaquin Cabezas&lt;/a&gt;&lt;/p&gt;
&lt;/footer&gt;
&lt;script type=&quot;text/javascript&quot; src=&quot;//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-584ec4ce89deed84&quot;&gt;&lt;/script&gt;

&lt;div class=&quot;addthis_inline_share_toolbox&quot;&gt;&lt;/div&gt;

</description>
        <pubDate>Mon, 06 Mar 2023 00:00:00 +0100</pubDate>
        <link>http://urszulaczerwinska.github.io/works/text-in-image-2-0-improving-ocr-service-with-paddleocr</link>
        <guid isPermaLink="true">http://urszulaczerwinska.github.io/works/text-in-image-2-0-improving-ocr-service-with-paddleocr</guid>
        
        <category>data science</category>
        
        <category>OCR</category>
        
        <category>deep learning</category>
        
        <category>computer vision</category>
        
        <category>machine learning</category>
        
        <category>API</category>
        
        
        <category>works</category>
        
      </item>
    
      <item>
        <title>Named Entity Recognition Tool by Cour de Cassation</title>
        <description>&lt;!DOCTYPE HTML&gt;
&lt;html lang=&quot;en-US&quot;&gt;
    &lt;head&gt;
        &lt;meta charset=&quot;UTF-8&quot;&gt;
        &lt;meta http-equiv=&quot;refresh&quot; content=&quot;0; url=https://github.com/Cour-de-cassation/moteurNER&quot;&gt;
        &lt;meta name=&quot;description&quot; content=&quot;Redirecting to the Named Entity Recognition (NER) tool repository by Cour de Cassation. It is a page about building deep learning NLP application for French justice&quot;&gt;
        &lt;script type=&quot;text/javascript&quot;&gt;
            window.location.href = &quot;https://github.com/Cour-de-cassation/moteurNER&quot;
        &lt;/script&gt;
        &lt;title&gt;Redirecting to Named Entity Recognition Tool by Cour de Cassation, building NER, NLP deep learning applications for French Supreme Court.&lt;/title&gt;
    &lt;/head&gt;
    &lt;body&gt;
        &lt;p&gt;You are being redirected to the Named Entity Recognition tool repository by Cour de Cassation. If you are not redirected automatically, &lt;a rel=&quot;canonical&quot; href=&apos;https://github.com/Cour-de-cassation/moteurNER&apos;&gt;click here to proceed to the repository.&lt;/a&gt;&lt;/p&gt;
    &lt;/body&gt;

      &lt;!-- Go to www.addthis.com/dashboard to customize your tools --&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-584ec4ce89deed84&quot;&gt;&lt;/script&gt;
&lt;!-- Go to www.addthis.com/dashboard to customize your tools --&gt; &lt;div class=&quot;addthis_inline_share_toolbox&quot;&gt;&lt;/div&gt;

&lt;/html&gt;</description>
        <pubDate>Wed, 01 Sep 2021 00:00:00 +0200</pubDate>
        <link>http://urszulaczerwinska.github.io/works/ner_cc</link>
        <guid isPermaLink="true">http://urszulaczerwinska.github.io/works/ner_cc</guid>
        
        <category>Machine Learning</category>
        
        <category>Python</category>
        
        <category>Deep Learning</category>
        
        <category>NER</category>
        
        <category>NLP</category>
        
        <category>Justice</category>
        
        <category>Flair</category>
        
        <category>featured</category>
        
        
        <category>works</category>
        
      </item>
    
      <item>
        <title>Push the limits of machine learning explainability</title>
        <description>&lt;head&gt;
  &lt;meta
    name=&quot;description&quot;
    content=&quot;Explore the power of SHAP in enhancing model interpretability in data science and machine learning. This guide provides detailed insights and practical examples for data professionals.&quot;
  /&gt;
&lt;/head&gt;

&lt;section&gt;
  &lt;h1&gt;
    Summary - A Comprehensive Guide to SHAP: Enhancing Machine Learning
    Interpretability
  &lt;/h1&gt;
  &lt;p&gt;
    This article is a guide to the advanced and lesser-known features of the
    python SHAP library. It is based on an example of tabular data
    classification.
  &lt;/p&gt;
  &lt;p&gt;
    But first, let’s talk about the motivation and interest in explainability at
    Saegus that motivated and financed my explorations.
  &lt;/p&gt;

  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img src=&quot;/images/engineering.jpg&quot; alt=&quot;&quot; width=&quot;70%&quot;
  /&gt;&lt;/span&gt;

  &lt;h3&gt;The Theory Behind Explainability in AI and Machine Learning&lt;/h3&gt;
  &lt;p&gt;
    The explainability of algorithms is taking more and more place in the
    discussions about Data Science. We know that algorithms are powerful, we
    know that they can assist us in many tasks: price prediction, document
    classification, video recommendation.
  &lt;/p&gt;
  &lt;p&gt;
    From now on, more and more questions are being asked about this
    prediction:&lt;br /&gt;- Is it ethical?&lt;br /&gt;- Is it affected by bias?&lt;br /&gt;- Is
    it used for the right reasons?
  &lt;/p&gt;
  &lt;p&gt;
    In many domains such as medicine, banking or insurance, algorithms can be
    used if, and only if, it is possible to trace and explain (or better,
    interpret) the decisions of these algorithms.
  &lt;/p&gt;
  &lt;h4&gt;Key Terminology in Machine Learning Interpretability and SHAP&lt;/h4&gt;
  &lt;p&gt;In this article we would like to distinguish the terms:&lt;/p&gt;
  &lt;p&gt;
    &lt;strong&gt;Explainability&lt;/strong&gt;: possibility to explain from a technical
    point of view the prediction of an algorithm.
  &lt;/p&gt;
  &lt;p&gt;
    &lt;strong&gt;Interpretability&lt;/strong&gt;: the ability to explain or provide meaning
    in terms that are understandable by a human being.
  &lt;/p&gt;
  &lt;p&gt;
    &lt;strong&gt;Transparency&lt;/strong&gt;: a model is considered transparent if it is
    understandable on its own.
  &lt;/p&gt;
  &lt;h4&gt;Why Explainability Matters in Data Science ?&lt;/h4&gt;
  &lt;p&gt;
    Interpretability helps to ensure impartiality in decision-making, i.e. to
    detect and therefore correct biases in the training data set. In addition,
    it facilitates robustness by highlighting potential adverse disturbances
    that could change the prediction. It can also act as an assurance that only
    significant features infer the outcome.
  &lt;/p&gt;
  &lt;p&gt;
    Sometimes, it would be more advisable to abandon the machine learning
    approach, and use deterministic algorithms based on rules justified by
    industry knowledge or legislation [1].
  &lt;/p&gt;
  &lt;p&gt;
    Nevertheless, it is too tempting to access the capabilities of machine
    learning algorithms that can offer high accuracy. We can talk about the
    trade-off between accuracy and explainability. This trade-off consists in
    discarding more complex models such as neural networks for simpler
    algorithms that can be explained.
  &lt;/p&gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img src=&quot;/images/model-interpret.png&quot; alt=&quot;&quot; /&gt;&lt;em
      &gt;As described in [2] relation between interpretability and accuracy of the
      model. For some models improvements can be made towards a more
      interpretable or more relevant model.&lt;/em
    &gt;&lt;/span
  &gt;
  &lt;p&gt;
    To achieve these goals, a new field has emerged: XAI (Explainable Artificial
    Intelligence), which aims to produce algorithms that are both powerful and
    explainable.
  &lt;/p&gt;
  &lt;p&gt;
    Many frameworks have been proposed to help explain non-transparent
    algorithms. A very good presentation of these methods can be found in the
    Cloudera white paper [3].
  &lt;/p&gt;
  &lt;p&gt;
    In this article we will deal with one of the most used frameworks: SHAP.
  &lt;/p&gt;
  &lt;h4&gt;Exploring the Audience for Explainable AI in Data Science&lt;/h4&gt;
  &lt;p&gt;
    Different profiles interested in expainability or interpretability have been
    identified:
  &lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;
      Business expert/model user — in order to trust the model, understand the
      causality of the prediction
    &lt;/li&gt;
    &lt;li&gt;
      Regulatory bodies to certify compliance with the legislation, auditing
    &lt;/li&gt;
    &lt;li&gt;
      Managers and executive board to assess regulatory compliance, understand
      enterprise AI applications
    &lt;/li&gt;
    &lt;li&gt;
      Users impacted by model decisions in order to understand the situation,
      verify decisions
    &lt;/li&gt;
    &lt;li&gt;
      Data scientist, developer, PO to ensure/improve product performance, find
      new features, explain functioning/predictions to superiors
    &lt;/li&gt;
  &lt;/ul&gt;
  &lt;p&gt;
    In order to make explainability accessible to people with low technical
    skills, first of all, the creator: a data scientist/developer must be
    comfortable with the tools of explainability.
  &lt;/p&gt;
  &lt;p&gt;
    The data scientist will use them above all to understand and improve his
    model and then to communicate with his superiors and regulatory bodies.&lt;br /&gt;Recently,
    explainability tools have become more and more accessible.
  &lt;/p&gt;
  &lt;p&gt;
    For example,
    &lt;a
      href=&quot;https://www.dataiku.com/&quot;
      data-href=&quot;https://www.dataiku.com/&quot;
      class=&quot;markup--anchor markup--p-anchor&quot;
      rel=&quot;noopener&quot;
      target=&quot;_blank&quot;
      &gt;Dataiku &lt;/a
    &gt;— ML’s platform — has added in its latest version 7.0 published on March 2,
    2020 explainability tools: Shapley values and “The Individual Conditional
    Expectation” (ICE).
  &lt;/p&gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img src=&quot;/images/dataiku.png&quot; alt=&quot;&quot; /&gt;&lt;em
      &gt;Dataiku prediction studio&lt;/em
    &gt;&lt;/span
  &gt;
  &lt;p&gt;
    &lt;a href=&quot;https://azure.microsoft.com/en-us/services/machine-learning/&quot;
      &gt;Azure ML&lt;/a
    &gt;
    proposes its own version of Shap and alternative tools adding interactive
    &lt;a
      href=&quot;https://docs.microsoft.com/en-us/azure/machine-learning/how-to-machine-learning-interpretability-aml&quot;
      &gt;dashboards&lt;/a
    &gt;.
  &lt;/p&gt;

  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img src=&quot;/images/azure.png&quot; alt=&quot;&quot; /&gt;&lt;em
      &gt;Azure ML interpretability dashboard&lt;/em
    &gt;&lt;/span
  &gt;

  &lt;p&gt;
    There are also open-source webapps such as this one described in the medium
    article [4] that facilitate the exploration of the SHAP library.
  &lt;/p&gt;
  &lt;div&gt;
    &lt;a
      href=&quot;https://towardsdatascience.com/understand-the-machine-learning-blackbox-with-ml-interpreter-7b0f9a2d8e9f&quot;
      &gt;&lt;strong
        &gt;Understand the machine learning Blackbox with ML interpreter&lt;/strong
      &gt;&lt;br /&gt;&lt;em class=&quot;markup--em markup--mixtapeEmbed-em&quot;
        &gt;There are dangers in having models running the world and making
        decisions from hiring to criminal justice&lt;/em
      &gt;towardsdatascience.com&lt;/a
    &gt;&lt;a
      href=&quot;https://towardsdatascience.com/understand-the-machine-learning-blackbox-with-ml-interpreter-7b0f9a2d8e9f&quot;
    &gt;&lt;/a&gt;
  &lt;/div&gt;
  &lt;div&gt;&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;
  &lt;p&gt;
    These tools, very interesting to get a quick overview of interpretation, do
    not necessarily give an understanding of the full potential of the SHAP
    library. Few allow to explore interaction values or to use different
    background or display sets.
  &lt;/p&gt;
  &lt;p&gt;
    I investigated the SHAP framework and I present you my remarks and the usage
    of less known features, available in the official version of the library in
    open source. I also propose some interactive visualizations easy to
    integrate in your projects.
  &lt;/p&gt;
  &lt;h3&gt;Step-by-Step Guide: Using SHAP for Machine Learning Models&lt;/h3&gt;
  &lt;p&gt;
    Most data scientists have already heard of the SHAP framework.&lt;br /&gt;In this
    post, we won’t explain in detail how the calculations behind the library are
    done. Many resources are available online such as the SHAP documentation
    [5], publications by authors of the library [6,7], the great book
    “Interpretable Machine Learning” [8] and multiple medium articles [9,10,11].
  &lt;/p&gt;
  &lt;p&gt;
    In summary, Shapley’s values calculate the importance of a feature by
    comparing what a model predicts with and without this feature. However,
    since the order in which a model sees the features can affect its
    predictions, this is done in all possible ways, so that the features are
    compared fairly. This approach is inspired by game theory.
  &lt;/p&gt;
  &lt;p&gt;
    Having worked with many clients, for example in the banking and insurance
    sectors, one can see that their data scientists are struggling to exploit
    the full potential of SHAP. They don’t know how this tool could really be
    useful for understanding a model and how to use it to go beyond simply
    extracting the importance of features.
  &lt;/p&gt;
  &lt;blockquote&gt;The devil is in the detail&lt;/blockquote&gt;
  &lt;p&gt;
    SHAP comes with a set of visualizations that are quite complex and not
    always intuitive, even for a data scientist.
  &lt;/p&gt;
  &lt;p&gt;
    On top of that, there are several technical nuances to be able to use SHAP
    with your data. Francesco Porchetti’s blog article [12] expresses some of
    these frustrations by exploring the SHAP,
    &lt;a href=&quot;https://github.com/marcotcr/lime&quot;&gt;LIME&lt;/a&gt;,
    &lt;a
      href=&quot;https://github.com/SauceCat/PDPbox&quot;
      data-href=&quot;https://github.com/SauceCat/PDPbox&quot;
      &gt;PDPbox &lt;/a
    &gt;(PDP and ICE) and
    &lt;a
      href=&quot;https://eli5.readthedocs.io/en/latest/autodocs/sklearn.html#module-eli5.sklearn.permutation_importance&quot;
      &gt;ELI5 &lt;/a
    &gt;libraries.
  &lt;/p&gt;
  &lt;p&gt;
    &lt;strong
      &gt;&lt;em
        &gt;At Saegus, I worked on a course which aims to give more clarity to the
        SHAP framework and to facilitate the use of this tool.&lt;/em
      &gt;&lt;/strong
    &gt;
  &lt;/p&gt;
  &lt;p&gt;
    In this post I would like to share with you some observations collected
    during that process.
  &lt;/p&gt;
  &lt;p&gt;
    SHAP is used to explain an existing model. Taking a binary classification
    case built with a sklearn model. We train, tune and test our model. Then we
    can use our data and the model to create an additional SHAP model that
    explains our classification model.
  &lt;/p&gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/shap.png&quot;
      alt=&quot;SHAP plot demonstrating deep learning AI model interpretability in data science.&quot;
    /&gt;&lt;em&gt;Image source: SHAP github&lt;/em&gt;&lt;/span
  &gt;
  &lt;h4&gt;Vocabulary&lt;/h4&gt;
  &lt;p&gt;
    It is important to understand all the bricks that make up a SHAP
    explanation.
  &lt;/p&gt;
  &lt;p&gt;
    Often, by using default values for parameters, the complexity of the choices
    we make remains obscure.
  &lt;/p&gt;
  &lt;p&gt;
    &lt;strong&gt;global explanations&lt;br /&gt;&lt;/strong&gt;explanations of how the model
    works from a general point of view
  &lt;/p&gt;
  &lt;p&gt;
    &lt;strong&gt;local explanations&lt;/strong&gt;&lt;br /&gt;explanations of the model for a
    sample (a data point)
  &lt;/p&gt;
  &lt;p&gt;
    &lt;strong&gt;explainer &lt;/strong&gt;(shap.explainer_type(params))&lt;br /&gt;type of
    explainability algorithm to be chosen according to the model used.
  &lt;/p&gt;
  &lt;p&gt;
    The parameters are different for each type of model. Usually, the model and
    training data must be provided, at a minimum.
  &lt;/p&gt;
  &lt;p&gt;
    &lt;strong&gt;base value&lt;/strong&gt; (explainer.expected_value)&lt;br /&gt;&lt;em
      &gt;E(y_hat)&lt;/em
    &gt;
    is “the value that would be predicted if we didn’t know any features of the
    current output” is the &lt;em&gt;mean(y_hat)&lt;/em&gt; prediction for the training data
    set or the background set. We can call it “reference value”, it’s a scalar
    (&lt;em&gt;n&lt;/em&gt;).
  &lt;/p&gt;
  &lt;p&gt;
    It’s important to choose your background set carefully — if we have the
    unbalanced training set this will result in a base value placed among the
    majority of samples. This can also be a desired effect: for example if for a
    bank loan we want to answer the question: “how is the customer in question
    different from customers who have been approved for the loan” or “how is my
    false positive different from the true positives”.
  &lt;/p&gt;
  &lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt; &lt;span class=&quot;c1&quot;&gt;# equilibrated case background = X.sample(1000) #X is
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;equilibrated&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# background used in explainer defines base value explainer =
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;shap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;TreeExplainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xgb_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model_output&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;raw&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shap_values&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;explainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;shap_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# background used in the plot, the points that are
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;visible&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;on&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;summary_plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shap_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;feature_names&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt; &lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt; &lt;span class=&quot;c1&quot;&gt;#
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;base&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shifted&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;class1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;class1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,:]&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#X is equilibrated # background
&lt;/span&gt;  &lt;span class=&quot;k&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;1 &lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;used&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;explainer&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;defines&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;base&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;explainer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;shap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;TreeExplainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xgb_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;class1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model_output&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;raw&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shap_values&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;explainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;shap_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# points from class 0 is used in the plot, the points
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;that&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;are&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;visible&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;on&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;summary_plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shap_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;class0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,:],&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;feature_names&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img src=&quot;/images/background-shap.png&quot; alt=&quot;&quot; /&gt;&lt;em
      &gt;Selecting the background dataset changes the question answered by
      shap values.&lt;/em
    &gt;&lt;/span
  &gt;
  &lt;p&gt;
    &lt;strong&gt;SHAPley values&lt;/strong&gt; (explainer.shap_values(x))&lt;br /&gt;the average
    contribution of each feature to each prediction for each sample based on all
    possible features. It is a (&lt;em&gt;n,m&lt;/em&gt;) &lt;em&gt;n &lt;/em&gt;— samples,
    &lt;em&gt;m &lt;/em&gt;— features matrix that represents the contribution of each
    feature to each sample.
  &lt;/p&gt;
  &lt;p&gt;
    &lt;strong&gt;output value&lt;/strong&gt; (for a sample)&lt;br /&gt;the value predicted by the
    algorithm (the probability, logit or raw output values of the model)
  &lt;/p&gt;
  &lt;p&gt;
    &lt;strong&gt;display features&lt;/strong&gt; (&lt;em&gt;n &lt;/em&gt;x &lt;em&gt;m&lt;/em&gt;)&lt;br /&gt;a matrix of
    original values — before transformation/encoding/engineering of features
    etc. — that can be provided to some graphs to improve interpretation. Often
    overlooked and essential for interpretation.
  &lt;/p&gt;
  &lt;p&gt;____&lt;/p&gt;
  &lt;p&gt;&lt;strong&gt;SHAPley values&lt;/strong&gt;&lt;/p&gt;
  &lt;p&gt;
    Shapley values remain the central element. Once we realize that this is
    simply a matrix with the same dimensions as our input data and that we can
    analyze it in different ways to explain the model and not only. We can
    reduce its dimensions, we can cluster it, we can use it to create new
    features. An interesting exploration described in the article [12] aims at
    improving anomaly detection using auto encoders and SHAP. The SHAP library
    proposes a rich but not exchaustive exploration through visualizations.
  &lt;/p&gt;
  &lt;h4&gt;Visualizing SHAP: Enhancing Interpretability in Deep Learning Models&lt;/h4&gt;
  &lt;p&gt;
    The SHAP library offers different visualizations. A good explanation on how
    to read the colors of the summary plot can be found in this medium article
    [14].
  &lt;/p&gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img src=&quot;/images/shap-global.png&quot; alt=&quot;&quot; /&gt;&lt;em
      &gt;A summary of graphical visualizations to analyze global explanations&lt;/em
    &gt;&lt;/span
  &gt;
  &lt;p&gt;
    &lt;strong&gt;The summary plot&lt;/strong&gt; shows the most important features and the
    magnitude of their impact on the model. It can take several graphical forms
    and for the models explained by TreeExplainer we can also observe the&lt;strong
      &gt;&lt;em&gt; interaction values &lt;/em&gt;&lt;/strong
    &gt;using the “compact dot” with shap_interaction_values in input.
  &lt;/p&gt;
  &lt;p&gt;
    &lt;strong&gt;The dependency plot&lt;/strong&gt; allows to analyze the features two by
    two by suggesting a possibility to observe the interactions. The scatter
    plot represents a dependency between a feature(x) and the shapley values (y)
    colored by a second feature(hue).
  &lt;/p&gt;
  &lt;p&gt;
    On a personal note, I find that an observation of a three-factor
    relationship at the same time is not intuitive for the human brain (at least
    mine). I also doubt that an observation of dependency by observing colours
    can be scientifically accurate. Shap can give us an interaction relationship
    that is calculated as a correlation between the shapley values of the first
    feature and the values of the second feature. If possible (for
    TreeExplainer) it makes more sense to use the shapley interaction values to
    observe interactions.
  &lt;/p&gt;
  &lt;figure&gt;
    &lt;script src=&quot;https://gist.github.com/UrszulaCzerwinska/b7853f5f0b209f2f89e2a3590dd6f329.js&quot;&gt;&lt;/script&gt;
    &lt;figcaption class=&quot;imageCaption&quot;&gt;
      Snippet code to reproduce my dependence plot variant.
    &lt;/figcaption&gt;
  &lt;/figure&gt;
  &lt;p&gt;
    I propose an interactive variant of dependency plot that allows to observe
    the relationship between a feature(x), the shapley values (y) and the
    prediction (histogram colors). What seems important to me in this version is
    the possibility to display on the graph the original values (Income in k
    USD) instead of the normalized space used by the model.
  &lt;/p&gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/my-dep-plot.png&quot;
      alt=&quot;Histogram visualizing SHAP interaction values in a data science model analysis.&quot;
    /&gt;&lt;em&gt;My variant of dependence plot&lt;/em&gt;&lt;/span
  &gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/shap-local.png&quot;
      alt=&quot;Histogram visualizing SHAP interaction values in a data science model analysis.&quot;
    /&gt;&lt;em
      &gt;a summary of graphical visualizations to analyze local explanations&lt;/em
    &gt;&lt;/span
  &gt;
  &lt;p&gt;
    There are three alternatives for the visualization of explanations of a
    sample: force plot, decision plot and waterfall plot.
  &lt;/p&gt;
  &lt;p&gt;
    For a sample, these three representations are redundant, they represent the
    information in a very similar way. At the same time, some elements of these
    graphs are complementary. By putting the three side by side, I have the
    impression to understand the result in a more intuitive way. The force plot
    is good to see where the “output value” fits in relation to the “base
    value”. We also see which features have a positive (red) or negative (blue)
    impact on the prediction and the magnitude of this impact. The water plot
    also allows us to see the amplitude and the nature of the impact of a
    feature with its quantification. It also allows to see the order of
    importance of the features and the values taken by each feature for the
    studied sample. The Decision plot makes it possible to observe the amplitude
    of each change, “a trajectory” taken by a samplefor the values of the
    displayed features.
  &lt;/p&gt;
  &lt;p&gt;
    By using force plot and decision plot we can represent several samples at
    the same time.
  &lt;/p&gt;
  &lt;p&gt;
    The force plot for a set of samples can be compared to the last level of a
    dendrogram. The samples are grouped by similarity or by selected feature. In
    my opinion, this graph is difficult to read for a random sample. It is much
    more meaningful if we represent the contrasting cases or with a hypothesis
    behind.
  &lt;/p&gt;
  &lt;p&gt;
    The decision plot, for a set of samples, quickly becomes cumbersome if we
    select too many samples. It is very useful to observe a ‘trajectory
    deviation’ or ‘diverging/converging trajectories’ of a limited group of
    samples.
  &lt;/p&gt;
  &lt;h4&gt;
    Explainers in SHAP: Understanding Different Model Interpretability
    Approaches
  &lt;/h4&gt;
  &lt;p&gt;
    Explainers are the models used to calculate shapley values. The diagram
    below shows different types of Explainers.
  &lt;/p&gt;
  &lt;p&gt;
    The choice of Explainers depends mainly on the selected learning model. For
    linear models, the “Linear Explainer” is used, for decision trees and “set”
    type models — “TreeExplainer”. “Kernel Explainer” is slower than the above
    mentioned explainers.
  &lt;/p&gt;
  &lt;p&gt;
    In addition the “Tree Explainer” allows to display the interaction values
    (see next section). It also allows to transform the model output into
    probabilities or logloss, which is useful for a better understanding of the
    model or to compare several models.
  &lt;/p&gt;
  &lt;p&gt;
    The Kernel Explainer creates a model that substitutes the closest to our
    model. Kernel Explainer can be used to explain neural networks. For deep
    learning models, there are the Deep Explainer and the Grandient Explainer.
    For this paper we have not investigated the explainability of neural
    networks.
  &lt;/p&gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/explainer.png&quot;
      alt=&quot;Force plot visualization of SHAP values enhancing transparency in AI model predictions; Waterfall plot depicting SHAP value contributions to machine learning predictions.&quot;
    /&gt;&lt;em&gt;a summary of Explainer types in the SHAP library&lt;/em&gt;&lt;/span
  &gt;
  &lt;h4&gt;Shapley values of interactions&lt;/h4&gt;
  &lt;p&gt;
    One of the properties that allows to go further in the analysis of a model
    that can be explained with the “Tree Explainer” is the calculation of
    shapley values of interactions.
  &lt;/p&gt;
  &lt;p&gt;
    These values make it possible to quantify the impact of an interaction
    between two features on the prediction for each sample. As the matrix of
    shapley values has two dimensions (samples x features), the interactions are
    a tensor with three dimensions (samples x features x features).
  &lt;/p&gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/interactions-1.png&quot;
      alt=&quot;Interactive SHAP dependence plot showing feature impact in deep learning models.&quot;
    /&gt;&lt;em
      &gt;a summary of graphical visualizations to analyze local explanations&lt;/em
    &gt;&lt;/span
  &gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/interactions-2.png&quot;
      alt=&quot;Interactive SHAP dependence plot showing feature impact in deep learning models.&quot;
    /&gt;&lt;em
      &gt;a summary of graphical visualizations to analyze local explanations&lt;/em
    &gt;&lt;/span
  &gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/interactions-3.png&quot;
      alt=&quot;Interactive SHAP dependence plot showing feature impact in deep learning models.&quot;
    /&gt;&lt;em
      &gt;a summary of graphical visualizations to analyze local explanations&lt;/em
    &gt;&lt;/span
  &gt;

  &lt;p&gt;
    &lt;strong
      &gt;Here’s how interaction values help interpret a binary classification
      model.&lt;/strong
    &gt;
  &lt;/p&gt;
  &lt;p&gt;
    I used a Kaggle [15] dataset that represents a client base and the binary
    dependent feature: did the client accept the personal loan? NO/YES (0/1).
  &lt;/p&gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/table.png&quot;
      alt=&quot;A summary of SHAP graphical
      visualizations to analyze local explanations of machine learning model&quot;
    /&gt;&lt;em
      &gt;a summary of graphical visualizations to analyze local explanations&lt;/em
    &gt;&lt;/span
  &gt;

  &lt;p&gt;
    I’ve trained several models, including an xgboost model that we treated with
    the Tree Explainer.
  &lt;/p&gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/sklearn-model.png&quot;
      alt=&quot;A summary of SHAP graphical
        visualizations to analyze local explanations of machine learning model&quot;
    /&gt;&lt;em
      &gt;a summary of graphical visualizations to analyze local explanations&lt;/em
    &gt;&lt;/span
  &gt;

  &lt;p&gt;The background dataset was balanced and represented 40% of the dataset.&lt;/p&gt;
  &lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt; &lt;span class=&quot;c1&quot;&gt;# xgb - traned model # X_background - background
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;explainer_raw&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;TreeExplainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xgb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;model_output&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;raw&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;feature_perturbation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;tree_path_dependent&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# project
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;point&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;background&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datasetshap_values&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;explainer_raw&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;shap_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# obtain interaction values
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;shap_interaction_values&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;explainer_raw&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;shap_interaction_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;# dimensions shap_values.shape &amp;gt;&amp;gt;&amp;gt;(2543, 16) shap_interaction_values.shape
&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2543&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;summary_plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shap_interaction_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;plot_type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;compact_dot&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/summary-plot.png&quot;
      alt=&quot;Summary plot showcasing SHAP values and feature interactions in machine learning models.&quot;
    /&gt;&lt;em&gt;Summary plot with interactions&lt;/em&gt;&lt;/span
  &gt;

  &lt;p&gt;To better explore interactions, a heatmap can be very useful.&lt;/p&gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/histogram.png&quot;
      alt=&quot;Heatmap illustrating SHAP values and feature interactions in machine learning models.&quot;
    /&gt;&lt;em&gt;Histogram of interaction values&lt;/em&gt;&lt;/span
  &gt;
  &lt;p&gt;
    In the Summary_plot one can observe the importance of features and the
    importance the interactions. The interactions appear in double which
    confuses a little the reading.
  &lt;/p&gt;
  &lt;p&gt;
    In the histogram, we observe directly the interactions. The strongests of
    them of being: Income-Education, Income — Family, Income — CCAvg and
    Family-Education, Income-Age.
  &lt;/p&gt;
  &lt;p&gt;
    Then I investigated the interactions two by two.&lt;br /&gt;To understand the
    difference between a dependency_plot and a dependency_plot of interactions
    here are the two:
  &lt;/p&gt;

  &lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt; &lt;span class=&quot;c1&quot;&gt;# dependence_plot classique shap.dependence_plot(&quot;Age&quot;,
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;shap_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;features&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;display_features&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_background_display&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;interaction_index&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Income&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/dep-plot.png&quot;
      alt=&quot;SHAP dependency plot illustrating interaction effects in machine learning algorithm&quot;
  /&gt;&lt;/span&gt;
  &lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt; &lt;span class=&quot;c1&quot;&gt;# dependence_plot des interactions
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;shap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;dependence_plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Age&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Income&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shap_interaction_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;features&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;X_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;display_features&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_background_display&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/interaction-dep-plot.png&quot;
      alt=&quot;SHAP dependency plot illustrating interaction effects in machine learning algorithm&quot;
  /&gt;&lt;/span&gt;

  &lt;p&gt;
    Even when using the ‘display_features’ parameter, the Age and Income values
    are displayed in the transformed space.
  &lt;/p&gt;
  &lt;p&gt;
    For this reasons I offer an interactive version, which displays the
    non-transformed values.
  &lt;/p&gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/interactive-dep-plot.png&quot;
      alt=&quot;SHAP dependency plot illustrating interaction effects in machine learning algorithm&quot;
  /&gt;&lt;/span&gt;

  &lt;p&gt;And here is the code to reproduce this plot:&lt;/p&gt;
  &lt;figure&gt;
    &lt;script src=&quot;https://gist.github.com/UrszulaCzerwinska/b78e85c6e1795d949321209cbe41587a.js&quot;&gt;&lt;/script&gt;
  &lt;/figure&gt;
  &lt;p&gt;Here we have the strongest interactions:&lt;/p&gt;
  &lt;p&gt;Income — Education&lt;/p&gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/income-education.png&quot;
      alt=&quot;Shapley interaction plot for better visualization of features interaction in machine learning model&quot;
  /&gt;&lt;/span&gt;

  &lt;p&gt;
    In this graph, we notice that with an Education level 1 (undergrad), low
    income (under 100 k USD) is an encouraging factor to take a credit, and high
    income (over 120 k USD) is an inhibiting interaction.&lt;br /&gt;For individuals
    with Education 2 &amp;amp; 3 (graduated &amp;amp; advanced/professional), the
    interaction effect is slightly lower and opposite to that for Education ==
    1.
  &lt;/p&gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/income-family.png&quot;
      alt=&quot;Shapley interaction plot for better visualization of features interaction in machine learning model&quot;
  /&gt;&lt;/span&gt;

  &lt;p&gt;
    For the features “Family” and “number of people” in the household, the
    interaction is positive when income is low (below USD 100k) and the family
    has 1–2 members. For higher incomes (&amp;gt; 120 k USD), for family with 1–2
    members has a negative effect. The opposite is true for families of 3–4
    people.
  &lt;/p&gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/income-ccavg.png&quot;
      alt=&quot;Shapley interaction plot for better visualization of features interaction in machine learning model&quot;
  /&gt;&lt;/span&gt;

  &lt;p&gt;
    The interaction between income and credit card average spending is more
    complex. For low income (&amp;lt;100 k USD) and low CCAvg (&amp;lt;4 k USD) the
    interaction has a negative effect, for income between 50 and 110 k USD and
    CCAvg 2–6 k USD the effect is strongly positive, this could define a
    potential target for credit canvassing along these two axes. For high
    incomes (&amp;gt; 120 k USD), the low CCAvg has a positive impact on the
    prediction of class 1, high CCAvg has a small negative effect on the
    predictions, the medium CCAvg has a stronger negative impact.
  &lt;/p&gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/family-education.png&quot;
      alt=&quot;Shapley interaction plot for better visualization of features interaction in machine learning model&quot;
  /&gt;&lt;/span&gt;
  &lt;p&gt;
    The interaction between two features is a little less readible. For a family
    of 1 and 2 members with “undergrad” education, the interaction has a
    negative impact. For a family of 3–4 members the effect is the opposite.
  &lt;/p&gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/income-age.png&quot;
      alt=&quot;Shapley interaction plot for better visualization of features interaction in machine learning model&quot;
  /&gt;&lt;/span&gt;

  &lt;p&gt;
    For low incomes (&amp;lt; 70k USD), impact changes linearly with age, the higher
    the age, the more the impact varies positively. For high incomes (&amp;gt;120k
    USD), the interaction impact is lower, at middle age (~40 years) the impact
    is slightly positive, at low age the impact is negative and for age &amp;gt;45
    the impact is neutral.
  &lt;/p&gt;
  &lt;p&gt;
    These findings would be more complicated to interpret if the values of the
    features had not corresponded to original values. For instance, speaking of
    age or income in negative in units. Therefore, representing explanations in
    an understandable dimension facilitates interpretation.
  &lt;/p&gt;
  &lt;h4&gt;Comparing Models: How SHAP Improves Machine Learning Interpretability&lt;/h4&gt;
  &lt;p&gt;
    In some situations, we may want to compare the predictions of different
    models for the same samples. Understand why one model classifies the sample
    correctly and the other one does not.&lt;br /&gt;To start, we can display the
    summary plots for each model, look at the importance of features and the
    shapley value distributions. This gives a first general idea.
  &lt;/p&gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/tree-explainer.png&quot;
      alt=&quot;SHAP tree explainer illustrating how to compare the predictions of different models for the same samples&quot;
  /&gt;&lt;/span&gt;

  &lt;p&gt;
    Decision plot allows to compare on the same graph the predictions of
    different models for the same sample.&lt;br /&gt;You just have to create an object
    that simulates multiclass classification.
  &lt;/p&gt;
  &lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt; &lt;span class=&quot;c1&quot;&gt;# we have tree models : xgb, gbt, rf # for each model
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;we&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;an&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;explainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_explainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;probabilities &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_probs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shapley&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#xgb xgb_explainer =
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;shap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;TreeExplainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xgb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model_output&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;probability&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;feature_perturbation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;interventional&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xgb_values&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;xgb_explainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;shap_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xgb_probs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;xgb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;predict_proba&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pipeline_trans&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xgb_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;xgb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pipeline_trans&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# rf rf_explainer =
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;shap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;TreeExplainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model_output&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;probability&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;feature_perturbation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;interventional&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rf_values&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;rf_explainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;shap_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rf_probs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;rf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;predict_proba&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pipeline_trans&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rf_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;rf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pipeline_trans&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# gbt gbt_explainer =
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;shap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;TreeExplainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gbt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model_output&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;probability&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;feature_perturbation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;interventional&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gbt_values&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;gbt_explainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;shap_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gbt_probs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;gbt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;predict_proba&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pipeline_trans&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gbt_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;gbt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pipeline_trans&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;######## # we make a list
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;explaners&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;base_values&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xgb_explainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;expected_value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;rf_explainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;expected_value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gbt_explainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;expected_value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shap_values&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xgb_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rf_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gbt_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xgb_probs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rf_probs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gbt_probs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;xgb&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;rf&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;gbt&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# index of a
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Plot idx = 100 shap.multioutput_decision_plot(base_values,
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;shap_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;feature_names&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;to_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;legend_labels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;legend_location&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;lower right&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/model-comparison.png&quot;
      alt=&quot;Comparative analysis of machine learning models using SHAP values for data science insights.&quot;
  /&gt;&lt;/span&gt;

  &lt;p&gt;
    The only difficulty consists in checking the dimensions of shapley values
    because for some models, shapley values are calculated for each class, in
    the case of the binary classification (class 0 and 1), while for others we
    obtain a single matrix which corresponds to class 1. In our example, we
    select a second matrix (index 1) for random forest.
  &lt;/p&gt;
  &lt;h4&gt;
    Running Simulations with SHAP: Enhancing Model Predictions in Data Science
  &lt;/h4&gt;
  &lt;p&gt;
    By default SHAP does not contain functions that make it easier to answer the
    “What if?” question. “What if I could earn an extra 10K USD a year, would my
    credit be extended?”&lt;br /&gt;Nevertheless, it is possible to run the
    simulations by varying a feature and calculating hypothetical shapley
    values.
  &lt;/p&gt;
  &lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt; &lt;span class=&quot;n&quot;&gt;explainer_margin_i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;shap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;TreeExplainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xgb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model_output&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;raw&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;feature_perturbation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;interventional&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rg&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;202&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;R&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;hypothetical_shap_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hypothetical_predictions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_r&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
  &lt;span class=&quot;nf&quot;&gt;simulate_with_shap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Income&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;explainer_margin_i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;pipeline_trans&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pipeline_trans&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
  &lt;p&gt;
    I created a `simulate_with_shap` function that simulates different values of
    the feature and calculates the hypothetical shapley values.
  &lt;/p&gt;
  &lt;figure&gt;
    &lt;script src=&quot;https://gist.github.com/UrszulaCzerwinska/c324a8ec8e37ebe4758f4affe52456be.js&quot;&gt;&lt;/script&gt;
  &lt;/figure&gt;
  &lt;span class=&quot;image fit&quot;
    &gt;&lt;img
      src=&quot;/images/simulation.png&quot;
      alt=&quot;The SHAP simulation allows to see how we could change the prediction using new values and what the shapley values would be for these&quot;
  /&gt;&lt;/span&gt;

  &lt;p&gt;
    This simulation allows us to see for the selected sample, if we freeze all
    the features apart from Income, how we could change the prediction and what
    the shapley values would be for these new values.
  &lt;/p&gt;
  &lt;p&gt;
    It is possible to simulate the changes ‘feature by feature’, it would be
    interesting to be able to make several changes simultaneously.
  &lt;/p&gt;
  &lt;h3&gt;
    Future Trends in Data Science: The Growing Role of SHAP and Interpretability
  &lt;/h3&gt;
  &lt;p&gt;
    AI algorithms are taking up more and more space in our lives. The
    explanability of predictions is an important topic for data scientists,
    decision-makers and individuals who are impacted by predictions.
  &lt;/p&gt;
  &lt;p&gt;
    Several frameworks have been proposed in order to transform non-explainable
    models into explainable ones. One of the best known and most widely used
    frameworks is SHAP.
  &lt;/p&gt;
  &lt;p&gt;
    Despite very good documentation, it is not clear how to exploit all its
    features in depth.
  &lt;/p&gt;
  &lt;p&gt;
    I have proposed some simple graphical enhancements and tried to demonstrate
    the usefulness of less known and not understood features in most standard
    uses of SHAP.
  &lt;/p&gt;
  &lt;h3&gt;Acknowledgements&lt;/h3&gt;
  &lt;p&gt;
    I would like to thank the Saegus DATA team who participated in this work
    with good advice, in particular Manager Fréderic Brajon and Senior
    Consultant Manager Clément Moutard.
  &lt;/p&gt;
  &lt;h3&gt;Bibliography&lt;/h3&gt;
  &lt;p&gt;
    [1] Stop Explaining Black Box Machine Learning Models for High Stakes
    Decisions and Use Interpretable Models Instead; Cynthia Rudin
    &lt;a
      href=&quot;https://arxiv.org/pdf/1811.10154.pdf&quot;
      data-href=&quot;https://arxiv.org/pdf/1811.10154.pdf&quot;
      target=&quot;_blank&quot;
      &gt;https://arxiv.org/pdf/1811.10154.pdf&lt;/a
    &gt;
  &lt;/p&gt;
  &lt;p&gt;
    [2] Explainable Artificial Intelligence (XAI): Concepts, Taxonomies,
    Opportunities and Challenges toward Responsible AI; Arrietaa et al.
    &lt;a
      href=&quot;https://arxiv.org/pdf/1910.10045.pdf&quot;
      data-href=&quot;https://arxiv.org/pdf/1910.10045.pdf&quot;
      target=&quot;_blank&quot;
      &gt;https://arxiv.org/pdf/1910.10045.pdf&lt;/a
    &gt;
  &lt;/p&gt;
  &lt;p&gt;
    [3] Cloudera Fast Forward Interpretability:
    &lt;a
      href=&quot;https://ff06-2020.fastforwardlabs.com/?utm_campaign=Data_Elixir&amp;amp;utm_source=Data_Elixir_282&quot;
      data-href=&quot;https://ff06-2020.fastforwardlabs.com/?utm_campaign=Data_Elixir&amp;amp;utm_source=Data_Elixir_282&quot;
      target=&quot;_blank&quot;
      &gt;https://ff06-2020.fastforwardlabs.com/?utm_campaign=Data_Elixir&amp;amp;utm_source=Data_Elixir_282&lt;/a
    &gt;
  &lt;/p&gt;
  &lt;p&gt;
    [4]
    &lt;a
      href=&quot;https://towardsdatascience.com/understand-the-machine-learning-blackbox-with-ml-interpreter-7b0f9a2d8e9f&quot;
      data-href=&quot;https://towardsdatascience.com/understand-the-machine-learning-blackbox-with-ml-interpreter-7b0f9a2d8e9f&quot;
      target=&quot;_blank&quot;
      &gt;https://towardsdatascience.com/understand-the-machine-learning-blackbox-with-ml-interpreter-7b0f9a2d8e9f&lt;/a
    &gt;
  &lt;/p&gt;
  &lt;p&gt;
    [5]
    &lt;a
      href=&quot;https://github.com/slundberg/shap&quot;
      data-href=&quot;https://github.com/slundberg/shap&quot;
      target=&quot;_blank&quot;
      &gt;https://github.com/slundberg/shap&lt;/a
    &gt;
  &lt;/p&gt;
  &lt;p&gt;
    [6]
    &lt;a
      href=&quot;http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions&quot;
      data-href=&quot;http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions&quot;
      target=&quot;_blank&quot;
      &gt;http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions&lt;/a
    &gt;
  &lt;/p&gt;
  &lt;p&gt;
    [7]
    &lt;a
      href=&quot;https://www.nature.com/articles/s42256-019-0138-9&quot;
      data-href=&quot;https://www.nature.com/articles/s42256-019-0138-9&quot;
      target=&quot;_blank&quot;
      &gt;https://www.nature.com/articles/s42256-019-0138-9&lt;/a
    &gt;
  &lt;/p&gt;
  &lt;p&gt;
    [8]
    &lt;a
      href=&quot;https://christophm.github.io/interpretable-ml-book/&quot;
      data-href=&quot;https://christophm.github.io/interpretable-ml-book/&quot;
      target=&quot;_blank&quot;
      &gt;https://christophm.github.io/interpretable-ml-book/&lt;/a
    &gt;
  &lt;/p&gt;
  &lt;p&gt;
    [9]
    &lt;a
      href=&quot;https://towardsdatascience.com/shap-explained-the-way-i-wish-someone-explained-it-to-me-ab81cc69ef30&quot;
      data-href=&quot;https://towardsdatascience.com/shap-explained-the-way-i-wish-someone-explained-it-to-me-ab81cc69ef30&quot;
      target=&quot;_blank&quot;
      &gt;https://towardsdatascience.com/shap-explained-the-way-i-wish-someone-explained-it-to-me-ab81cc69ef30&lt;/a
    &gt;
  &lt;/p&gt;
  &lt;p&gt;
    [10]
    &lt;a
      href=&quot;https://medium.com/@gabrieltseng/interpreting-complex-models-with-shap-values-1c187db6ec83&quot;
      data-href=&quot;https://medium.com/@gabrieltseng/interpreting-complex-models-with-shap-values-1c187db6ec83&quot;
      target=&quot;_blank&quot;
      &gt;https://medium.com/@gabrieltseng/interpreting-complex-models-with-shap-values-1c187db6ec83&lt;/a
    &gt;
  &lt;/p&gt;
  &lt;p&gt;
    [11]
    &lt;a
      href=&quot;https://medium.com/@stanleyg1/a-detailed-walk-through-of-shap-example-for-interpretable-machine-learning-d265c693ac22&quot;
      data-href=&quot;https://medium.com/@stanleyg1/a-detailed-walk-through-of-shap-example-for-interpretable-machine-learning-d265c693ac22&quot;
      target=&quot;_blank&quot;
      &gt;https://medium.com/@stanleyg1/a-detailed-walk-through-of-shap-example-for-interpretable-machine-learning-d265c693ac22&lt;/a
    &gt;
  &lt;/p&gt;
  &lt;p&gt;
    [12]
    &lt;a
      href=&quot;https://francescopochetti.com/whitening-a-black-box-how-to-interpret-a-ml-model/&quot;
      data-href=&quot;https://francescopochetti.com/whitening-a-black-box-how-to-interpret-a-ml-model/&quot;
      target=&quot;_blank&quot;
      &gt;https://francescopochetti.com/whitening-a-black-box-how-to-interpret-a-ml-model/&lt;/a
    &gt;
  &lt;/p&gt;
  &lt;p&gt;
    [13] Explaining Anomalies Detected by Autoencoders Using SHAP; Antwarg et
    al.
    &lt;a
      href=&quot;https://arxiv.org/pdf/1903.02407.pdf&quot;
      data-href=&quot;https://arxiv.org/pdf/1903.02407.pdf&quot;
      target=&quot;_blank&quot;
      &gt;https://arxiv.org/pdf/1903.02407.pdf&lt;/a
    &gt;
  &lt;/p&gt;
  &lt;p&gt;
    [14]
    &lt;a
      href=&quot;https://medium.com/fiddlerlabs/case-study-explaining-credit-modeling-predictions-with-shap-2a7b3f86ec12&quot;
      data-href=&quot;https://medium.com/fiddlerlabs/case-study-explaining-credit-modeling-predictions-with-shap-2a7b3f86ec12&quot;
      target=&quot;_blank&quot;
      &gt;https://medium.com/fiddlerlabs/case-study-explaining-credit-modeling-predictions-with-shap-2a7b3f86ec12&lt;/a
    &gt;
  &lt;/p&gt;
  &lt;p&gt;
    [15]
    &lt;a
      href=&quot;https://www.kaggle.com/itsmesunil/bank-loan-modelling&quot;
      data-href=&quot;https://www.kaggle.com/itsmesunil/bank-loan-modelling&quot;
      target=&quot;_blank&quot;
      &gt;https://www.kaggle.com/itsmesunil/bank-loan-modelling&lt;/a
    &gt;
  &lt;/p&gt;

  &lt;footer&gt;
    This blog post was originally published with
    &lt;a
      href=&quot;https://medium.com/swlh&quot;
      alt=&quot;Get smarter at building your thing. Follow to join The Startup’s +8 million monthly readers &amp; +778K followers.&quot;
      &gt;The Startup&lt;/a
    &gt;
    at
    &lt;a
      href=&quot;https://medium.com/swlh/push-the-limits-of-explainability-an-ultimate-guide-to-shap-library-a110af566a02&quot;
      alt=&quot;Discover how to push the boundaries of explainability in data science and machine learning. This comprehensive guide to SHAP (SHapley Additive exPlanations) covers everything from deep learning algorithms to model interpretability, making complex AI models more transparent and trustworthy. Ideal for data science professionals and enthusiasts looking to enhance their understanding of machine learning interpretability with cutting-edge tools and techniques.&quot;
      &gt;Medium&lt;/a
    &gt;. &lt;br /&gt;&lt;br /&gt;
  &lt;/footer&gt;
  &lt;script
    type=&quot;text/javascript&quot;
    src=&quot;//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-584ec4ce89deed84&quot;
  &gt;&lt;/script&gt;
  &lt;div class=&quot;addthis_inline_share_toolbox&quot;&gt;&lt;/div&gt;
&lt;/section&gt;
</description>
        <pubDate>Sun, 01 Mar 2020 00:00:00 +0100</pubDate>
        <link>http://urszulaczerwinska.github.io/works/interpretability-shap</link>
        <guid isPermaLink="true">http://urszulaczerwinska.github.io/works/interpretability-shap</guid>
        
        <category>data science</category>
        
        <category>interpretability</category>
        
        <category>XAI</category>
        
        <category>machine learning</category>
        
        <category>featured</category>
        
        <category>explainable AI</category>
        
        <category>Python</category>
        
        
        <category>works</category>
        
      </item>
    
      <item>
        <title>Mastering Named Entity Recognition (NER) in Data Science</title>
        <description>&lt;h2 id=&quot;context-of-development-of-a-keyword-extraction-application-using-nlp-language-model&quot;&gt;Context of development of a keyword extraction application using NLP language model&lt;/h2&gt;

&lt;p&gt;Named Entity Recognition, often abbreviated as NER, has gained traction as a critical tool for extracting meaningful insights from text data. Whether you’re diving into data science projects or exploring the cutting edge of AI applied to language, understanding how to utilize NER is essential. In this post, I’ll walk you through a practical example of using SpaCy, a go-to library for NLP, to detect keywords from Medium articles. But first, let’s explore why NER is becoming a must-have skill in the data science and engineering toolbox.&lt;/p&gt;

&lt;p&gt;Inspired by a solution developed for a customer in the Pharmaceutical industry, we presented at the &lt;a href=&quot;https://paris.egg.dataiku.com/&quot;&gt;EGG PARIS 2019&lt;/a&gt; conference an application based on NLP (Natural Language Processing) and developed on a &lt;a href=&quot;https://www.dataiku.com/&quot;&gt;Dataiku&lt;/a&gt; &lt;a href=&quot;https://www.dataiku.com/dss/&quot;&gt;DSS&lt;/a&gt; environment.&lt;/p&gt;

&lt;p&gt;More precisely, we trained a deep learning model to recognize the keywords of a blog article, precisely from &lt;a href=&quot;https://medium.com/&quot;&gt;Medium blogging platform&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;By &lt;strong&gt;automatically generate tags and/or keywords&lt;/strong&gt;, this approach enables personalized content recommendations, improving user experience by aligning content with reader expectations. The method holds significant potential, particularly for automated text analysis of complex documents, including scientific papers and legal texts.&lt;/p&gt;

&lt;p&gt;To showcase its functionality, we integrated a voice command feature using &lt;a href=&quot;https://azure.microsoft.com/en-us/services/cognitive-services/&quot;&gt;Azure’s cognitive services API&lt;/a&gt;. The &lt;em&gt;speech to text&lt;/em&gt; module translates spoken queries into text, which is then processed by the algorithm. The output is a recommendation of articles, classified by relevance according to the field of research.&lt;/p&gt;

&lt;p&gt;In this article, I’ll walk you through our approach to creating the underlying NLP model.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/zg0pTe-GyF0&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;sub&gt;&lt;sup&gt;[To view the comments, please enable subtitles] A video that illustrates our web application created for the EGG Dataiku 2019 conference&lt;/sup&gt;&lt;/sub&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;why-extract-keywords-from-medium-blog-articles-with-ai-&quot;&gt;Why Extract Keywords from Medium Blog Articles with AI ?&lt;/h2&gt;

&lt;p&gt;Medium has two categorization systems: &lt;strong&gt;tags&lt;/strong&gt; and &lt;strong&gt;topics&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Topics&lt;/strong&gt; are predefined by the platform and correspond to broad categories like data science or machine learning. Authors have no control over these.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Tags&lt;/strong&gt;, on the other hand, are keywords selected by the author, with a maximum of five tags per article. These tags help increase the visibility of the article but often may not accurately reflect the content. For instance, tags like “TECHNOLOGY,” “MINDFULNESS,” or “LIFE LESSONS” might make an article easier to find but can complicate the reader’s search for specific content.&lt;/p&gt;

&lt;p&gt;Our approach aims to improve this by automatically tagging articles, increasing their relevance. With these “new tags” or “keywords,” searching for articles becomes more efficient.&lt;/p&gt;

&lt;p&gt;Going further, this method could be used to build a recommendation system that suggests related articles based on the one you’re currently reading or aligned with your reading habits.&lt;/p&gt;

&lt;h2 id=&quot;the-ner-named-entity-recognition-approach&quot;&gt;The NER (Named Entity Recognition) approach&lt;/h2&gt;

&lt;p&gt;Using the NER (Named Entity Recognition) approach, we can extract entities across various categories. Several pre-trained models, like &lt;a href=&quot;https://github.com/explosion/spacy-models/releases/tag/en_core_web_md-2.2.0&quot;&gt;en_core_web_md&lt;/a&gt; can recognize entities like people, places, dates, etc.&lt;/p&gt;

&lt;p&gt;For example, in the sentence &lt;em&gt;“I think Barack Obama met founder of Facebook at occasion of a release of a new NLP algorithm.”&lt;/em&gt;, the en_core_web_md model detects “Facebook” and “Barack Obama” as entities.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/UrszulaCzerwinska/11a8fab0cc4c936b67e374e2b55e0fa0.js&quot;&gt;&lt;/script&gt;

&lt;div&gt;&lt;span class=&quot;image fit&quot;&gt;&lt;img src=&quot;/images/NER_img1.png&quot; alt=&quot;NER process using SpaCy in data science&quot; /&gt;&lt;/span&gt;&lt;/div&gt;
&lt;p align=&quot;center&quot;&gt;&lt;sub&gt;&lt;sup&gt;Dependency graph: result of line 9 (# 1)&lt;/sup&gt;&lt;/sub&gt;&lt;/p&gt;

&lt;div&gt;&lt;span class=&quot;image fit&quot;&gt;&lt;img src=&quot;/images/NER_img2.png&quot; alt=&quot;NER process using SpaCy in data science&quot; /&gt;&lt;/span&gt;&lt;/div&gt;
&lt;p align=&quot;center&quot;&gt;&lt;sub&gt;&lt;sup&gt;Entity detection: result of line 10 (# 2)&lt;/sup&gt;&lt;/sub&gt;&lt;/p&gt;

&lt;p&gt;With some annotated data, we trained the algorithm to detect this new entity type.&lt;/p&gt;

&lt;p&gt;The concept is straightforward: an article tagged with “Data Science,” “AI,” “Machine Learning,” or “Python” might still cover vastly different technologies. Our algorithm is designed to detect specific technologies mentioned in the article, such as GANs, reinforcement learning, or Python libraries, while still recognizing places, organizations, and people.&lt;/p&gt;

&lt;p&gt;During training, the model learns to identify keywords without prior knowledge. For example, it might recognize “random forest” as a topic, even if it wasn’t in the training data. By analyzing other algorithms discussed in articles, the NER model can identify phrase patterns that indicate a specific topic.&lt;/p&gt;

&lt;h2 id=&quot;the-machine-learning-language-model-behind&quot;&gt;The machine learning language model behind&lt;/h2&gt;

&lt;h3 id=&quot;spacy-framework-for-nlp&quot;&gt;SpaCy Framework for NLP&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://spacy.io/&quot;&gt;SpaCy&lt;/a&gt; is an open-source library tailored for advanced natural language processing in Python. It’s built for production use and helps create applications that process large volumes of text. SpaCy can be used to build information extraction systems, natural language understanding systems, or text preprocessing pipelines for deep learning. Among its features are tokenization, parts-of-speech (PoS) tagging, text classification, and named entity recognition.&lt;/p&gt;

&lt;p&gt;SpaCy offers an efficient, statistical system for NER in Python. Beyond the default entities, SpaCy allows us to add custom classes to the NER model and train it with new examples.&lt;/p&gt;

&lt;p&gt;SpaCy’s NER model is based on &lt;strong&gt;Convolutional Neural Networks (CNNs)&lt;/strong&gt;. For those interested, more details on how SpaCy’s NER model works can be found in the video below:&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/sqDHBH9IjRU&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/p&gt;

&lt;h3 id=&quot;training-data&quot;&gt;Training data&lt;/h3&gt;

&lt;p&gt;To train our model to recognize tech keywords, we scraped some Medium articles through &lt;strong&gt;web scraping&lt;/strong&gt;.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/UrszulaCzerwinska/db0aa37b1cb10ec94205d847f63ddc4f.js&quot;&gt;&lt;/script&gt;

&lt;div&gt;&lt;span class=&quot;image fit&quot;&gt;&lt;img src=&quot;/images/NER_img3.png&quot; alt=&quot;Table showing training data for language model&quot; /&gt;&lt;/span&gt;&lt;/div&gt;
&lt;p align=&quot;center&quot;&gt;&lt;sub&gt;&lt;sup&gt;An extract from the table containing the contents of the medium articles&lt;/sup&gt;&lt;/sub&gt;&lt;/p&gt;

&lt;p&gt;The text of each article was split into sentences for easier annotation.&lt;/p&gt;

&lt;p&gt;For NER annotation, there are tools like &lt;strong&gt;Prodigy&lt;/strong&gt;, but we opted for a simple spreadsheet where we manually marked the entities in dedicated columns.&lt;/p&gt;

&lt;div&gt;&lt;span class=&quot;image fit&quot;&gt;&lt;img src=&quot;/images/NER_img4.png&quot; alt=&quot;Table showing training data for language model and first predictions&quot; /&gt;&lt;/span&gt;&lt;/div&gt;

&lt;p&gt;With around twenty articles (~600 sentences), our model began to show promising performance, achieving over 0.78 accuracy on the test set. We separated the train and test data to evaluate the model effectively.&lt;/p&gt;

&lt;div&gt;&lt;span class=&quot;image fit&quot;&gt;&lt;img src=&quot;/images/NER_img5.png&quot; alt=&quot;Table showing training data for language model and first predictions&quot; /&gt;&lt;/span&gt;&lt;/div&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;TRAIN_DATA_ALL&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_table&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;apply&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;mark_targets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;ORG&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;PERSON&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;LOC&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;TOPIC&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;GPE&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;DATE&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;EVENT&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;WORK_OF_ART&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;sents&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;ORG&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;PERSON&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;LOC&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;TOPIC&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;GPE&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;DATE&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;EVENT&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;WORK_OF_ART&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;div&gt;&lt;span class=&quot;image fit&quot;&gt;&lt;img src=&quot;/images/NER_img6.png&quot; alt=&quot;Table showing training data for language model and first predictions&quot; /&gt;&lt;/span&gt;&lt;/div&gt;

&lt;p&gt;We fine-tuned the algorithm by adjusting parameters like the number of iterations, dropout rate, learning rate, and batch size.&lt;/p&gt;

&lt;h3 id=&quot;the-nlp-model-assesment&quot;&gt;The NLP model assesment&lt;/h3&gt;

&lt;p&gt;In addition to the model’s loss metric, we implemented precision, recall, and F1 score to measure performance more accurately.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/UrszulaCzerwinska/c23ce9e0edffe6f9790a2bbf8f018a4b.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;After training on the annotated data, the best model’s performance on our test set was quite impressive, especially considering the modest training data size (~3000 sentences).&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;precision :  0.9588053949903661
recall :  0.9211764705882353
f1_score :  0.9396221959858323

It is is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy.

TOPIC Python
TOPIC NumPy
TOPIC SciPy&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;In the &lt;strong&gt;Flow&lt;/strong&gt; on DSS, the process can be summarized by the graph:&lt;/p&gt;

&lt;div&gt;&lt;span class=&quot;image fit&quot;&gt;&lt;img src=&quot;/images/NER_img7.png&quot; alt=&quot;&quot; /&gt;&lt;/span&gt;&lt;/div&gt;
&lt;p align=&quot;center&quot;&gt;&lt;sub&gt;&lt;sup&gt;Flow on Dataiku&apos;s DSS platform: the annotated dataset is divided into train and test, the model learned on the train data is evaluated on the train and test batches.&lt;/sup&gt;&lt;/sub&gt;&lt;/p&gt;

&lt;p&gt;Returning to our Barack Obama example, our algorithm now detects the NLP algorithm entity as a TOPIC, in addition to the ORG (organization), LOC (location), GPE (geopolitical entity), and DATE categories.&lt;/p&gt;

&lt;p&gt;We have succeeded! 🚀&lt;/p&gt;

&lt;p&gt;The next step involves incorporating the model into our recommendation system, enhancing the customization of articles offered to users based on detected topics.&lt;/p&gt;

&lt;div&gt;&lt;span class=&quot;image fit&quot;&gt;&lt;img src=&quot;/images/NER_img8.png&quot; alt=&quot;&quot; /&gt;&lt;/span&gt;&lt;/div&gt;

&lt;p&gt;The finalized model can be compiled as an independent python library (instructions here) and installed with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pip&lt;/code&gt;. This is very practical for deploying the model in another environment and for production setup.&lt;/p&gt;

&lt;div&gt;&lt;span class=&quot;image fit&quot;&gt;&lt;img src=&quot;/images/NER_img9.jpg&quot; alt=&quot;&quot; /&gt;&lt;/span&gt;&lt;/div&gt;

&lt;h2 id=&quot;exploitation-of-the-model&quot;&gt;Exploitation of the model&lt;/h2&gt;

&lt;h3 id=&quot;analysis-of-an-article-medium&quot;&gt;Analysis of an article Medium&lt;/h3&gt;

&lt;p&gt;In our mini webapp, presented at the EGG, it is possible to display the most frequent entities of a Medium article.&lt;/p&gt;

&lt;p&gt;Thus, for the article: &lt;a href=&quot;https://towardsdatascience.com/cat-dog-or-elon-musk-145658489730&quot;&gt;https://towardsdatascience.com/cat-dog-or-elon-musk-145658489730&lt;/a&gt;, the most frequent entities were: model, MobileNet, Transfer learning, network, Python. We also detected people: Elon Musk, Marshal McLuhan and organizations: Google, Google Brain.&lt;/p&gt;

&lt;div&gt;&lt;span class=&quot;image fit&quot;&gt;&lt;img src=&quot;/images/NER_img10.png&quot; alt=&quot;&quot; /&gt;&lt;/span&gt;&lt;/div&gt;

&lt;p&gt;Inspired by &lt;a href=&quot;https://towardsdatascience.com/@bramblexu&quot;&gt;Xu LIANG’s&lt;/a&gt; &lt;a href=&quot;https://towardsdatascience.com/textrank-for-keyword-extraction-by-python-c0bae21bcec0&quot;&gt;post&lt;/a&gt;, we also used his way of representing the relationship between words in the form of a graph of linguistic dependencies. Unlike in his method, we did not use TextRank or TFIDF to detect keywords but we only applied our pre-trained NER model.&lt;/p&gt;

&lt;p&gt;Then, like &lt;a href=&quot;https://towardsdatascience.com/@bramblexu&quot;&gt;Xu LIANG&lt;/a&gt;, we used the capacity of Parts-of-Speech (PoS) Tagging, inherited by our model from the original model (&lt;a href=&quot;https://github.com/explosion/spacy-models/releases/tag/en_core_web_md-2.2.0&quot;&gt;en_core_web_md&lt;/a&gt;), to link the entities together with the edges, which forms the graph below.&lt;/p&gt;

&lt;div&gt;&lt;span class=&quot;image fit&quot;&gt;&lt;img src=&quot;/images/NER_img11.png&quot; alt=&quot;&quot; /&gt;&lt;/span&gt;&lt;/div&gt;
&lt;p&gt;&lt;sub&gt;&lt;sup&gt;The graph of dependencies between the entities detected in the article “Cat, Dog, or Elon Musk?”&lt;/sup&gt;&lt;/sub&gt;&lt;/p&gt;

&lt;p&gt;Thus, we get a graph where the keywords are placed around their category: Tech topic, Person and Organization.&lt;/p&gt;

&lt;p&gt;This gives a quick overview of the content of a Medium article.&lt;/p&gt;

&lt;p&gt;Here is how to get the graph from a Medium article url link:&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/UrszulaCzerwinska/d1d77f0bf8bd089103994eb3883db28f.js&quot;&gt;&lt;/script&gt;

&lt;h2 id=&quot;to-go-further&quot;&gt;To go further&lt;/h2&gt;
&lt;p&gt;Our Saegus Showroom including the functional webapp is coming soon. Feel free to follow our page &lt;a href=&quot;https://medium.com/data-by-saegus&quot;&gt;https://medium.com/data-by-saegus&lt;/a&gt; to be kept informed.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The project we have outlined here can easily be transposed into the various fields of industry: technical, legal and medical documents. It could be very interesting to analyse the civil, criminal and law… with this approach for a better efficiency in the research that all legal professionals do.&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt;

&lt;p&gt;To conclude, by recognizing topics within Medium articles, this solution represents a significant leap forward in content personalization. Whether for individual readers or professionals seeking articles on specific subjects, automatic keyword extraction offers a tailored experience. This model’s ability to classify articles based on finely-tuned NER allows for precise, relevant recommendations, improving overall user satisfaction and engagement.&lt;/p&gt;

&lt;p&gt;We invite you to explore this exciting field and consider how such technology could be adapted to your specific needs.&lt;/p&gt;

&lt;h2 id=&quot;disclaimer&quot;&gt;&lt;em&gt;Disclaimer&lt;/em&gt;&lt;/h2&gt;

&lt;p&gt;This article is a result of a teamwork realized at &lt;a href=&quot;http://saegus.com/fr/&quot;&gt;Saegus&lt;/a&gt;. Published originally in French at &lt;a href=&quot;https://medium.com/data-by-saegus/ner-medium-articles-saegus-7ffec0f3188c&quot;&gt;Medium&lt;/a&gt;.&lt;/p&gt;

&lt;section&gt;
&lt;!-- Go to www.addthis.com/dashboard to customize your tools --&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-584ec4ce89deed84&quot;&gt;&lt;/script&gt;
&lt;!-- Go to www.addthis.com/dashboard to customize your tools --&gt; &lt;div class=&quot;addthis_inline_share_toolbox&quot;&gt;&lt;/div&gt;
&lt;section&gt;

&lt;/section&gt;&lt;/section&gt;
</description>
        <pubDate>Mon, 11 Nov 2019 00:00:00 +0100</pubDate>
        <link>http://urszulaczerwinska.github.io/works/egg_ner</link>
        <guid isPermaLink="true">http://urszulaczerwinska.github.io/works/egg_ner</guid>
        
        <category>NLP</category>
        
        <category>Python</category>
        
        <category>featured</category>
        
        <category>machine learning</category>
        
        <category>NER</category>
        
        <category>language models</category>
        
        
        <category>works</category>
        
      </item>
    
      <item>
        <title>Women in Healthcare Analytics and Data Science (WiHADS)</title>
        <description>&lt;html&gt;
  &lt;head&gt;
    &lt;meta
      name=&quot;description&quot;
      content=&quot;Urszula Czerwinska orgnaiser of a meetup WiHADS | Join WiHADS, a pioneering community promoting women in data science, healthcare analytics, and deep learning AI. Explore opportunities to lead data science projects, engage with deep learning models, and shape the future of AI in healthcare.&quot;
    /&gt;
  &lt;/head&gt;

  &lt;body&gt;&lt;/body&gt;
  &lt;section&gt;
    &lt;h2&gt;
      Introduction: Empowering Women in Data Science and AI for Healthcare
    &lt;/h2&gt;
    &lt;p&gt;
      In today’s rapidly evolving world, data science is driving innovation
      across industries, particularly in healthcare where AI and deep learning
      models are transforming patient outcomes. The
      &lt;strong&gt;Women in Healthcare Analytics and Data Science (WiHADS)&lt;/strong&gt;
      initiative is leading the charge, empowering women in data science to
      spearhead crucial data science projects that are revolutionizing the
      healthcare sector.
    &lt;/p&gt;
    &lt;h2&gt;The Beginnings of WiHADS: A Movement in Data Science and Healthcare&lt;/h2&gt;
    &lt;p&gt;
      WiHADS began in 2017, spearheaded by
      &lt;a href=&quot;https://www.linkedin.com/in/sameh-m-8575a3179/&quot;&gt;Sameh Megrhi&lt;/a&gt;,
      with a vision to amplify the voices of women in data science and
      healthcare analytics.&lt;br /&gt;
    &lt;/p&gt;

    &lt;blockquote&gt;
      &lt;p&gt;
        The initiative was created to address the underrepresentation of women
        in fields like deep learning models, data science and engineering, and
        AI in healthcare. By highlighting the significant contributions of
        women, WiHADS aims to inspire more women to enter and thrive in these
        critical sectors.
      &lt;/p&gt;
      &lt;p&gt;
        Since its inception, WiHADS has brought together a healthcare data
        science community. The platform provides a space where women can share
        their experiences and insights, fostering a collaborative environment
        that encourages innovation in data science and machine learning. These
        meetups are not just about discussions—they are about solving real-world
        challenges using deep learning algorithms and data science
        methodologies.
      &lt;/p&gt;
    &lt;/blockquote&gt;
    &lt;h2&gt;WiHADS Meetups: A Hub for Innovation in Data Science and Healthcare&lt;/h2&gt;
    &lt;p&gt;
      WiHADS meetups have become a central platform for women in data science to
      exchange ideas, network, and discuss the latest trends in data science and
      machine learning. Since 2017, WiHADS has organized four major meetups in
      Paris, each focusing on the intersection of data science and healthcare.
      These events have garnered attention within the data science community and
      beyond, as they provide invaluable insights into how data science and deep
      learning AI are transforming the healthcare industry.
    &lt;/p&gt;
    &lt;blockquote&gt;
      &lt;b
        &gt;Join us at
        &lt;a
          href=&quot;https://www.meetup.com/fr-FR/Healthcare-Analytics-Data-Science/&quot;
          alt=&quot;The WiHADS meetups are place to exchange about woman in Data Science, Machine Learning and AI&quot;
          &gt;our meetup webpage&lt;/a
        &gt;&lt;/b
      &gt;
    &lt;/blockquote&gt;

    &lt;h2&gt;Key Components of WiHADS Meetups:&lt;/h2&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;strong&gt;Speakers:&lt;/strong&gt; The meetups feature accomplished women from
        top companies like
        &lt;a
          alt=&quot;IQVIA, a global leader in clinical research and healthcare data, drives innovation to enhance patient and public health. By integrating data, insights, technology, and expertise, IQVIA empowers clients and partners to advance and market cutting-edge therapies.&quot;
          href=&quot;https://www.iqvia.com/fr-fr/locations/france&quot;
          &gt;IQvia&lt;/a
        &gt;,
        &lt;a
          alt=&quot;Doctolib a leader in AI for healthcare.&quot;
          href=&quot;https://about.doctolib.fr/?utm_button=header&amp;utm_website=doctolib_career%2F&quot;
          &gt;Doctolib&lt;/a
        &gt;, and
        &lt;a
          alt=&quot;Democratizing expert cardiac care through medical-grade AI and cloud technology&quot;
          href=&quot;https://cardiologs.com/&quot;
          &gt;Cardiologs&lt;/a
        &gt;, who share their expertise on data science projects, deep learning
        models, and their applications in healthcare. These sessions provide
        attendees with a deeper understanding of how data science is applied in
        real-world healthcare settings.
      &lt;/li&gt;

      &lt;li&gt;
        &lt;strong&gt;Opportunities:&lt;/strong&gt; The events offer a unique chance for
        women in data science to connect, collaborate, and learn from each
        other. This networking is crucial for building a strong data science and
        engineering network that can support professional growth and innovation.
      &lt;/li&gt;

      &lt;li&gt;
        &lt;strong&gt;Roundtable Discussions:&lt;/strong&gt; After the presentations,
        participants engage in roundtable discussions, where they explore the
        ethical implications of AI in healthcare, the challenges of implementing
        deep learning algorithms in medical practice, and the future of data
        science in healthcare.
      &lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;
      WiHADS is committed to fostering a supportive environment where women in
      data science can thrive. The meetups are free to attend, thanks to the
      generous support of sponsors like
      &lt;a
        alt=&quot;Doctolib a leader in AI for healthcare.&quot;
        href=&quot;https://about.doctolib.fr/?utm_button=header&amp;utm_website=doctolib_career%2F&quot;
        &gt;Doctolib&lt;/a
      &gt;,
      &lt;a
        alt=&quot;IQVIA, a global leader in clinical research and healthcare data, drives innovation to enhance patient and public health. By integrating data, insights, technology, and expertise, IQVIA empowers clients and partners to advance and market cutting-edge therapies.&quot;
        href=&quot;https://www.iqvia.com/fr-fr/locations/france&quot;
        &gt;IQvia&lt;/a
      &gt;, and
      &lt;a
        alt=&quot;Saegus is supporting businesses in their transition to a combined intelligence model, where artificial intelligence serves humanity&quot;
        href=&quot;https://www.saegus.com/&quot;
        &gt;Saegus&lt;/a
      &gt;, who believe in the importance of promoting diversity in data science
      and AI.
    &lt;/p&gt;

    &lt;h2&gt;The Vision Behind WiHADS: Promoting Women in Data Science&lt;/h2&gt;
    &lt;p&gt;
      The driving force behind WiHADS is the belief that women have a critical
      role to play in the future of data science and healthcare. With a
      background in systems biology and extensive experience in machine learning
      within academia and the pharmaceutical industry, I joined WiHADS in 2018
      to promote the visibility of women in these fields. My goal was to build a
      platform where women could not only showcase their achievements in data
      science and AI in healthcare but also mentor and support one another.
      Working alongside
      &lt;a
        alt=&quot;Data scientist Sameh Megrhi is one of leaders in Machine Learning&quot;
        href=&quot;https://www.linkedin.com/in/sameh-m-8575a3179/&quot;
        &gt;Sameh Megrhi&lt;/a
      &gt;,&lt;a
        alt=&quot;Imen Helali working in Healthcare and Pharmaceutics is highly motivated promote Healthcare and Data Science fields&quot;
        href=&quot;https://www.linkedin.com/in/imen-helali-phd-a6a4222b/&quot;
      &gt;
        Imen Helali&lt;/a
      &gt;
      and
      &lt;a
        atl=&quot;Young data science professional Avani Tanna is promoting woman in tech importance&quot;
        href=&quot;https://www.linkedin.com/in/avani-tanna-b20b8a77/&quot;
        &gt;Avani Tanna&lt;/a
      &gt;, we crafted a unique meetup format that balances expert presentations
      with interactive, community-driven discussions. Our vision is to create a
      vibrant healthcare data science community where women can gain the skills
      and confidence needed to lead impactful data science projects.
    &lt;/p&gt;
    &lt;h2&gt;The Impact of WiHADS on the Healthcare Data Science Community&lt;/h2&gt;
    &lt;p&gt;
      WiHADS has had a profound impact on the healthcare data science community
      by providing a platform that not only highlights the achievements of women
      but also encourages ongoing education and professional development. The
      meetups have sparked important conversations about the future of data
      science in healthcare and the role of deep learning AI in improving
      patient care. Through WiHADS, we have seen an increase in the number of
      women pursuing careers in data science and machine learning, particularly
      in the healthcare sector. By offering opportunities for mentorship,
      networking, and skill development, WiHADS is helping to close the gender
      gap in data science and ensure that women are well-represented in the
      future of AI in healthcare.
    &lt;/p&gt;
    &lt;h2&gt;
      Get Involved with WiHADS: Join the Movement in Data Science and Healthcare
    &lt;/h2&gt;
    &lt;p&gt;
      Whether you are an experienced data scientist or just beginning your
      journey in data science and machine learning, WiHADS offers a welcoming
      community where you can grow, learn, and contribute. There are many ways
      to get involved with WiHADS and become part of this exciting movement.
    &lt;/p&gt;
    &lt;h2&gt;Ways to Participate:&lt;/h2&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;strong&gt;Attend a Meetup:&lt;/strong&gt;Join us at our next event to connect
        with fellow professionals and learn about the latest developments in
        data science and deep learning models for healthcare. Stay updated on
        upcoming events by visiting our
        &lt;a
          alt=&quot;Join WiHADS, a pioneering community promoting women in data science, healthcare analytics, and deep learning AI. Explore opportunities to lead data science projects, engage with deep learning models, and shape the future of AI in healthcare.&quot;
          href=&quot;https://www.meetup.com/fr-FR/Healthcare-Analytics-Data-Science/&quot;
          &gt;Meetup page&lt;/a
        &gt;.
      &lt;/li&gt;

      &lt;li&gt;
        &lt;strong&gt;Become a Speaker:&lt;/strong&gt; Share your knowledge and experience
        in data science projects or deep learning algorithms by speaking at one
        of our meetups. We are always looking for new voices to contribute to
        our discussions on AI in healthcare.
      &lt;/li&gt;

      &lt;li&gt;
        &lt;strong&gt;Sponsor an Event:&lt;/strong&gt; Help us continue to provide free,
        high-quality events by sponsoring a WiHADS meetup. Your support will
        enable us to reach more women in data science and expand our impact in
        the healthcare industry.
      &lt;/li&gt;
    &lt;/ul&gt;

    &lt;h2&gt;Conclusion: Shaping the Future of Data Science in Healthcare&lt;/h2&gt;
    &lt;p&gt;
      As we look to the future, data science and AI will continue to shape
      healthcare, and WiHADS is committed to ensuring that women in tech are at
      the forefront of this transformation. By joining us, you’re becoming part
      of a powerful movement dedicated to advancing deep learning algorithms and
      data science in healthcare. Together, we can lead the way in pioneering
      solutions that enhance patient care and drive innovation

      &lt;p&gt;&lt;strong&gt;Contact Us:&lt;/strong&gt; If you&apos;re interested in hosting a meetup or
      sponsoring an event, please reach out to us. We look forward to welcoming
      you to the WiHADS community.&lt;/p&gt;
    &lt;/p&gt;
    &lt;span class=&quot;image fit&quot;
      &gt;&lt;img
        src=&quot;/images/wihads3.jpeg&quot;
        alt=&quot;Group of women leaders in healthcare analytics and data science at a WiHADS meetup, fostering innovation in data science, deep learning AI, and healthcare.&quot;
        width=&quot;70%&quot;
    /&gt;&lt;/span&gt;

    &lt;!-- Go to www.addthis.com/dashboard to customize your tools --&gt;
    &lt;script
      type=&quot;text/javascript&quot;
      src=&quot;//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-584ec4ce89deed84&quot;
    &gt;&lt;/script&gt;
    &lt;!-- Go to www.addthis.com/dashboard to customize your tools --&gt;
    &lt;div class=&quot;addthis_inline_share_toolbox&quot;&gt;&lt;/div&gt;
  &lt;/section&gt;
&lt;/html&gt;
</description>
        <pubDate>Tue, 01 Oct 2019 00:00:00 +0200</pubDate>
        <link>http://urszulaczerwinska.github.io/works/women-healthcare-data-science-analytics-wihads</link>
        <guid isPermaLink="true">http://urszulaczerwinska.github.io/works/women-healthcare-data-science-analytics-wihads</guid>
        
        <category>data science</category>
        
        <category>events</category>
        
        <category>healthcare</category>
        
        <category>networking</category>
        
        
        <category>works</category>
        
      </item>
    
      <item>
        <title>Language gap between academia and business</title>
        <description>&lt;!DOCTYPE html&gt;
&lt;html&gt;
  &lt;head&gt;
    &lt;meta
      name=&quot;description&quot;
      content=&quot;An insider view on the language gap between academia and business, and how to bridge it from a data scientist perspective. | Transitioning from academia to industry.&quot;
    /&gt;
  &lt;/head&gt;

  &lt;body&gt;
    &lt;style&gt;
      .highlightme { background-color:#ccffe5; }
    &lt;/style&gt;

    &lt;section&gt;
      &lt;h2&gt;
        Transitioning from Academia to Industry: A Guide for Aspiring Data
        Scientists
      &lt;/h2&gt;
      &lt;p&gt;
        &lt;em
          &gt;Have you ever struggled to communicate your ideas effectively to
          business professionals when transitioning from academia to industry?
          If you&apos;re coming from an academic background and aiming for a career
          in the private sector, you may find the jargon and communication
          styles quite different.&lt;/em
        &gt;
      &lt;/p&gt;
      &lt;h3&gt;Understanding the Language Gap Between Academia and Business&lt;/h3&gt;
      &lt;p&gt;
        Academia and the private sector often speak different languages. In
        academia, the focus is on innovation and originality, leading to the use
        of terms like
        &lt;b&gt;novel&lt;/b&gt;, &lt;b&gt;insight&lt;/b&gt;, &lt;b&gt;paradigm&lt;/b&gt;, &lt;b&gt;robust&lt;/b&gt;, and
        &lt;b&gt;elucidate&lt;/b&gt;. Research often involves complex landscapes and
        frameworks, with a tendency to use words like &lt;b&gt;putative&lt;/b&gt;,
        &lt;b&gt;respectively&lt;/b&gt;, and &lt;b&gt;potentially&lt;/b&gt; to handle uncertainty and
        criticism.
      &lt;/p&gt;

      &lt;p&gt;
        On the other hand, the business world values clarity and efficiency.
        Terms such as &lt;b&gt;ballpark&lt;/b&gt;, &lt;b&gt;itemize&lt;/b&gt;, &lt;b&gt;prioritize&lt;/b&gt;,
        &lt;b&gt;agile&lt;/b&gt;, &lt;b&gt;incentivize&lt;/b&gt;, &lt;b&gt;proactive&lt;/b&gt;, and
        &lt;b&gt;empowerment&lt;/b&gt; reflect the need for quick solutions and strategic
        thinking. Business professionals focus on value, profit, and material
        gains, often using expressions like &lt;b&gt;push the envelope&lt;/b&gt; and
        &lt;b&gt;added value&lt;/b&gt;.
      &lt;/p&gt;
      &lt;div&gt;
        &lt;span class=&quot;image fit&quot;
          &gt;&lt;img
            src=&quot;/images/boss.jpg&quot;
            alt=&quot;Transitioning from academia to industry can be challenging but rewarding&quot;
        /&gt;&lt;/span&gt;
      &lt;/div&gt;

      &lt;h2&gt;The Critical Attitude Shift&lt;/h2&gt;

      &lt;p&gt;
        Academics are trained to be critical and challenge ideas rigorously.
        However, in the business world, being overly critical can be
        counterproductive. In business, you&apos;ll often hear terms like
        &lt;b&gt;opportunities for improvement&lt;/b&gt; rather than &lt;i&gt;pitfalls&lt;/i&gt;. Issues
        are described as &lt;b&gt;in development&lt;/b&gt; or &lt;b&gt;with growth potential&lt;/b&gt;,
        and even terminations are referred to as &lt;b&gt;restructuring&lt;/b&gt;. Adapting
        to this language and attitude is crucial for fitting into the corporate
        environment.
      &lt;/p&gt;

      &lt;h2&gt;Adapting to Corporate Culture&lt;/h2&gt;

      &lt;p&gt;
        Business professionals often prioritize efficiency and profit. Words
        like
        &lt;b&gt;ballpark&lt;/b&gt;, &lt;b&gt;itemize&lt;/b&gt;, &lt;b&gt;agile&lt;/b&gt;, and
        &lt;b&gt;takeaways&lt;/b&gt; emphasize task management and speed. Expressions such
        as &lt;b&gt;incentivize&lt;/b&gt;, &lt;b&gt;proactive&lt;/b&gt;, and &lt;b&gt;repurpose&lt;/b&gt; highlight
        the dynamic nature of corporate work. When discussing strategy, terms
        like &lt;b&gt;empowerment&lt;/b&gt; and &lt;b&gt;leverage&lt;/b&gt; are common, reflecting a
        focus on strategic impact and financial value.
      &lt;/p&gt;

      &lt;blockquote&gt;
        The difference in wording reflects different attitudes and methods used
        in business and research. However, the skills gained in research can be
        valuable in the business world.
        &lt;span class=&quot;highlightme&quot;
          &gt;&lt;b
            &gt;Mastering the art of communication can facilitate a smooth
            transition from academia to industry.&lt;/b
          &gt;&lt;/span
        &gt;
        Effective communication is crucial for team success and overall company
        performance.
      &lt;/blockquote&gt;

      &lt;p&gt;
        In academia, you often manage a project from start to finish,
        communicating with peers who are already familiar with your topic. In
        contrast, the corporate world involves working within a larger process,
        requiring clear communication and documentation. Corporate language
        often emphasizes organization and management.
      &lt;/p&gt;

      &lt;h2&gt;Crafting a Business-Ready CV&lt;/h2&gt;

      &lt;p&gt;
        When transitioning to a corporate role, your CV needs to shift from the
        academic format. Instead of a detailed list of projects and
        publications, focus on achievements and contributions that align with
        business needs. Follow
        &lt;a
          alt=&quot;Google&apos;s recommendations for tech jobs&quot;
          href=&quot;https://www.businessinsider.fr/us/google-exec-gives-key-to-perfect-resume-2014-9&quot;
          &gt;Google&apos;s recommendations&lt;/a
        &gt;
        for tech jobs, using a business tone and highlighting impact. For
        example, replace academic jargon with phrases like &apos;Achieved Z&apos; or
        &apos;Managed a team of N people&apos;.
      &lt;/p&gt;

      &lt;p&gt;
        In the business world, the emphasis is less on publications and more on
        the skills and logistics involved in achieving results. Break down your
        academic achievements into understandable and valuable skills for
        potential employers.
      &lt;/p&gt;

      &lt;div&gt;
        &lt;span class=&quot;image fit&quot;
          &gt;&lt;img
            src=&quot;/images/work.jpg&quot;
            alt=&quot;Effective CV writing is crucial for transitioning from academia to industry&quot;
        /&gt;&lt;/span&gt;
      &lt;/div&gt;

      &lt;h2&gt;Presenting Your Research&lt;/h2&gt;

      &lt;p&gt;
        When discussing your research, practice explaining it to non-academic
        audiences. Focus on the big picture and avoid technical jargon. Use
        everyday language and make parallels with business or everyday life.
        Consider preparing an
        &lt;a
          href=&quot;http://thepostdocway.com/content/elevator-pitches-scientists-what-when-where-and-how&quot;
          &gt;elevator pitch&lt;/a
        &gt;
        or reviewing
        &lt;a href=&quot;https://vimeo.com/threeminutethesis&quot;&gt;3MT&lt;/a&gt; videos for
        inspiration.
      &lt;/p&gt;

      &lt;h2&gt;Effective Presentations&lt;/h2&gt;

      &lt;p&gt;
        In the corporate world, presentations are crucial for persuading and
        informing. Unlike academic presentations, business presentations require
        a strong focus on the form and visual appeal. Ensure your slides are
        well-designed and that your presentation highlights the big picture
        before delving into details.
      &lt;/p&gt;

      &lt;h2&gt;Final Thoughts&lt;/h2&gt;

      &lt;p&gt;
        Although there is a significant gap between academia and business,
        bridging this divide can be rewarding. Adapting to business language and
        practices will open new doors and enhance your career opportunities.
        Engage with industry events, practice your communication skills, and be
        prepared for interviews to ensure a successful transition.
      &lt;/p&gt;

      &lt;div&gt;
        &lt;span class=&quot;image fit&quot;
          &gt;&lt;img
            src=&quot;/images/raising.jpg&quot;
            alt=&quot;Successfully transitioning from academia to industry opens new career opportunities&quot;
        /&gt;&lt;/span&gt;
      &lt;/div&gt;
	  &lt;p&gt;Also published on &lt;a alt=&quot;Discover more stories about Deep Learnig, Machine Learning and Data Science by Urszula Czerwinska&quot; href=&quot;https://medium.com/@ulalaparis/language-gap-between-academia-and-business-ef534ce71d10&quot;&gt;Medium&lt;/a&gt;&lt;/p&gt;
    &lt;/section&gt;
  &lt;/body&gt;
  &lt;!-- Go to www.addthis.com/dashboard to customize your tools --&gt;
  &lt;script
    type=&quot;text/javascript&quot;
    src=&quot;//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-584ec4ce89deed84&quot;
  &gt;&lt;/script&gt;

  &lt;!-- Go to www.addthis.com/dashboard to customize your tools --&gt;
  &lt;div class=&quot;addthis_inline_share_toolbox&quot;&gt;&lt;/div&gt;
&lt;/html&gt;
</description>
        <pubDate>Sat, 19 Jan 2019 00:00:00 +0100</pubDate>
        <link>http://urszulaczerwinska.github.io/thoughts/businesslang</link>
        <guid isPermaLink="true">http://urszulaczerwinska.github.io/thoughts/businesslang</guid>
        
        
        <category>thoughts</category>
        
      </item>
    
      <item>
        <title>PhD Thesis</title>
        <description>&lt;!DOCTYPE HTML&gt;
&lt;html lang=&quot;en-US&quot;&gt;
    &lt;head&gt;
        &lt;meta charset=&quot;UTF-8&quot;&gt;
        &lt;meta http-equiv=&quot;refresh&quot; content=&quot;1; url=&quot;&gt;
        &lt;script type=&quot;text/javascript&quot;&gt;
            window.location.href = &quot;https://urszulaczerwinska.github.io/UCzPhDThesis/&quot;
        &lt;/script&gt;
        &lt;title&gt;Redirecting to a page on NER project at Cour de Cassation webpage that explains in details the approach of NLP Engineering Data Science Project&lt;/title&gt;
    &lt;/head&gt;
    &lt;body&gt;
        &lt;!-- Note: don&apos;t tell people to `click` the link, just tell them that it is a link. --&gt;
        If you are not redirected automatically, follow this &lt;a href=&apos;https://urszulaczerwinska.github.io/UCzPhDThesis/&apos;&gt;link to example&lt;/a&gt;.
          &lt;!-- Go to www.addthis.com/dashboard to customize your tools --&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-584ec4ce89deed84&quot;&gt;&lt;/script&gt;
&lt;!-- Go to www.addthis.com/dashboard to customize your tools --&gt; &lt;div class=&quot;addthis_inline_share_toolbox&quot;&gt;&lt;/div&gt;

    &lt;/body&gt;
&lt;/html&gt;</description>
        <pubDate>Thu, 02 Aug 2018 00:00:00 +0200</pubDate>
        <link>http://urszulaczerwinska.github.io/works/PhDThesis.html</link>
        <guid isPermaLink="true">http://urszulaczerwinska.github.io/works/PhDThesis.html</guid>
        
        <category>Machine Learning</category>
        
        <category>R</category>
        
        <category>Data Science</category>
        
        <category>biomedical applications</category>
        
        
        <category>works</category>
        
      </item>
    
      <item>
        <title>DeconICA</title>
        <description>&lt;!DOCTYPE HTML&gt;
&lt;html lang=&quot;en-US&quot;&gt;
    &lt;head&gt;
        &lt;meta charset=&quot;UTF-8&quot;&gt;
        &lt;meta http-equiv=&quot;refresh&quot; content=&quot;1; url=&quot;&gt;
        &lt;script type=&quot;text/javascript&quot;&gt;
            window.location.href = &quot;https://urszulaczerwinska.github.io/DeconICA/&quot;
        &lt;/script&gt;
        &lt;title&gt;Redirecting to the project page on Github that details the usage and application of the R library deconica&lt;/title&gt;
    &lt;/head&gt;
    &lt;body&gt;
        &lt;!-- Note: don&apos;t tell people to `click` the link, just tell them that it is a link. --&gt;
        If you are not redirected automatically, follow this &lt;a href=&apos;https://urszulaczerwinska.github.io/DeconICA/&apos;&gt;link to example&lt;/a&gt;.
          &lt;!-- Go to www.addthis.com/dashboard to customize your tools --&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-584ec4ce89deed84&quot;&gt;&lt;/script&gt;
&lt;!-- Go to www.addthis.com/dashboard to customize your tools --&gt; &lt;div class=&quot;addthis_inline_share_toolbox&quot;&gt;&lt;/div&gt;

    &lt;/body&gt;
&lt;/html&gt;</description>
        <pubDate>Tue, 22 May 2018 00:00:00 +0200</pubDate>
        <link>http://urszulaczerwinska.github.io/works/deconica</link>
        <guid isPermaLink="true">http://urszulaczerwinska.github.io/works/deconica</guid>
        
        <category>Machine Learning</category>
        
        <category>R</category>
        
        <category>Data Science</category>
        
        <category>Deconvolution</category>
        
        
        <category>works</category>
        
      </item>
    
      <item>
        <title>Big Dive</title>
        <description>&lt;!DOCTYPE html&gt;
&lt;html&gt;
  &lt;head&gt;
    &lt;meta
      name=&quot;description&quot;
      content=&quot;Join the Big Dive course to gain hands-on experience with big data, distributed computing, and advanced data visualization techniques. Perfect for those transitioning from academia to industry, this course offers practical projects, expert insights, and valuable networking opportunities in the data science field.&quot;
    /&gt;
  &lt;/head&gt;
  &lt;body&gt;
    &lt;style&gt;
      .scalable {
          overflow: hidden;
      }

      .scalable iframe {
          height: 100%;
          left: 0;
          position: absolute;
          top: 0;
          width: 100%;
      }

      .scalable .scalable-content {
          height: 0;
          position: relative;
      }

      .scalable-16-9 .scalable-content {
          /* Percentage from 9 divided by 16 for ratio of 16:9. */
          padding-bottom: 56.25%;
      }

      iframe {display:block;margin:auto;width: 100%}
    &lt;/style&gt;
  &lt;/body&gt;
  &lt;section&gt;
    &lt;p&gt;
      &lt;i&gt;&lt;strong&gt;Big data - big unknown&lt;/strong&gt;&lt;/i&gt;
    &lt;/p&gt;
    &lt;h2&gt;When we play with data that are not so big...&lt;/h2&gt;
    &lt;p&gt;
      Experience with big data is among top skills of a data scientist profile.
      However, it is not trivial to learn big data tools without necessary
      infrastructure and proper tutoring. While pursuing my PhD in Data science
      dealing with medium size data frames (&lt; 1G), usually all can be processed
      on one machine. In order to get some taste of methods and challenges of
      Big Data, I completed
      &lt;a href=&quot;http://www.bigdive.eu/&quot; target=&quot;_blank&quot;&gt;Big Dive&lt;/a&gt; course
      organized by
      &lt;a href=&quot;https://www.top-ix.org/en/home-eng/&quot; target=&quot;_blank&quot;&gt;TOP-IX&lt;/a&gt;.
    &lt;/p&gt;
    &lt;h3&gt;... but we want to get a taste of &lt;strong&gt;BIG data&lt;/strong&gt;&lt;/h3&gt;
    &lt;p&gt;
      &lt;span class=&quot;image right&quot;
        &gt;&lt;img src=&quot;/images/companies.png&quot; alt=&quot;&quot; /&gt;&lt;em
          &gt;Image rights TOP-IX&lt;/em
        &gt;&lt;/span
      &gt;
      During the intense 5-week course I put in practice my Python - pandas
      skills and learned new libraries for &lt;b&gt;distributed computing&lt;/b&gt;, as well
      as &lt;i&gt;Py&lt;b&gt;Spark&lt;/b&gt;&lt;/i&gt; basics. We also spent a significant amount of
      time setting up the infrastructure on
      &lt;a href=&quot;https://aws.amazon.com/fr/?nc2=h_lg&quot; target=&quot;_blank&quot;
        &gt;amazon aws&lt;/a
      &gt;
      and &lt;a href=&quot;https://www.mongodb.com/&quot; target=&quot;_blank&quot;&gt;&lt;b&gt;MongoDB&lt;/b&gt;&lt;/a
      &gt;. Invited keynote speakers showed us important aspects and challenges of
      the data science for &lt;b&gt;business and science&lt;/b&gt;. A bonus was an
      introduction to beautiful &lt;b&gt;visualization D3.js&lt;/b&gt;. Finally, for me the
      most interesting and important part of the course was a
      &lt;b&gt;project&lt;/b&gt; realized for one of 3 companies:
      &lt;a
        href=&quot;http://www.ifc.org/wps/wcm/connect/corp_ext_content/ifc_external_corporate_site/home&quot;
        target=&quot;_blank&quot;
        &gt;IFC&lt;/a
      &gt;, &lt;a href=&quot;https://eduscopio.it/&quot; target=&quot;_blank&quot;&gt;Eduscopio&lt;/a&gt; and
      &lt;a href=&quot;https://www.tesisquare.com/en/&quot; target=&quot;_blank&quot;&gt;TesiSquare&lt;/a&gt;
      where in just a few days we put our best efforts to make sense of hundreds
      of gigabytes of data!
    &lt;/p&gt;

    &lt;h2&gt;Introduction to the Big Dive Course&lt;/h2&gt;
    &lt;p&gt;
      The Big Dive course is an intensive program designed to provide
      participants with hands-on experience in data science, with a particular
      focus on big data, distributed computing, and data visualization. Held
      over five weeks, this course serves as an essential stepping stone for
      those looking to transition from academia to industry or deepen their
      understanding of big data tools and techniques. This article will walk you
      through the key aspects of the course, its importance in the data science
      landscape, and the personal and professional growth opportunities it
      offers.
    &lt;/p&gt;
    &lt;h2&gt;The Growing Importance of Big Data&lt;/h2&gt;
    &lt;p&gt;
      In today&apos;s data-driven world, big data has become a cornerstone of
      decision-making processes across various industries. The ability to
      process and analyze large volumes of data efficiently is a top skill for
      any data scientist. However, working with big data comes with its own set
      of challenges, such as the need for specialized tools and infrastructure.
      The Big Dive course provides a comprehensive introduction to these
      challenges, helping participants understand the significance of big data
      and equipping them with the necessary skills to tackle it.
    &lt;/p&gt;
    &lt;h2&gt;Learning Big Data Tools and Infrastructure&lt;/h2&gt;
    &lt;p&gt;
      One of the key highlights of the Big Dive course is the hands-on training
      in big data tools and infrastructure. Participants work extensively with
      Python and Pandas for data manipulation and learn the basics of PySpark
      for distributed computing. Setting up infrastructure on platforms like AWS
      and MongoDB is another critical component of the course, giving
      participants a solid foundation in managing and processing large data
      sets.
    &lt;/p&gt;
    &lt;h2&gt;Learning Big Data Tools and Infrastructure&lt;/h2&gt;
    &lt;p&gt;
      One of the key highlights of the Big Dive course is the hands-on training
      in big data tools and infrastructure. Participants work extensively with
      &lt;strong&gt;Python&lt;/strong&gt; and &lt;strong&gt;Pandas&lt;/strong&gt; for data manipulation
      and learn the basics of PySpark for distributed computing. Setting up
      infrastructure on platforms like AWS and
      &lt;a
        alt=&quot;MongoDB is a no sql database, extensively used in Data Science&quot;
        href=&quot;https://www.mongodb.com/&quot;
        target=&quot;_blank&quot;
        &gt;&lt;b&gt;MongoDB&lt;/b&gt;&lt;/a
      &gt;
      is another critical component of the course, giving participants a solid
      foundation in managing and processing large data sets.
    &lt;/p&gt;
    &lt;h2&gt;Deep Dive into Data Science Projects&lt;/h2&gt;
    &lt;p&gt;
      &lt;span class=&quot;image left&quot;
        &gt;&lt;img
          src=&quot;/images/bigdive6-WELCOME_def.jpg&quot;
          alt=&quot;Data Science Bootcamp Big Dive mix all elements of data science&quot;
        /&gt;&lt;em&gt;Image rights TOP-IX&lt;/em&gt;&lt;/span
      &gt;
      The practical application of knowledge is at the heart of the Big Dive
      course. Participants engage in real-world projects that simulate the
      challenges faced by data scientists in the industry. For instance, one
      project involved analyzing hundreds of gigabytes of data for the
      &lt;a
        alt=&quot;IFC a member of the World Bank Group — is the largest global development institution focused on the private sector in emerging markets.&quot;
        href=&quot;http://www.ifc.org/wps/wcm/connect/corp_ext_content/ifc_external_corporate_site/home&quot;
        target=&quot;_blank&quot;
        &gt;International Finance Corporation (IFC)&lt;/a
      &gt;. These projects not only reinforce the technical skills learned during
      the course but also highlight the importance of applying data science in
      solving complex business and scientific problems.
    &lt;/p&gt;
    &lt;p&gt;&lt;/p&gt;

    &lt;h2&gt;The Role of Visualization in Data Science&lt;/h2&gt;
    &lt;p&gt;
      Data visualization is a crucial aspect of data science, allowing data
      scientists to present complex data in a more understandable and actionable
      format. The Big Dive course introduces participants to advanced data
      visualization techniques, with a particular focus on D3.js, a powerful
      JavaScript library for creating interactive graphs. The course emphasizes
      the importance of visualization in data science projects, helping
      participants understand how to effectively communicate their findings.
    &lt;/p&gt;

    &lt;h2&gt;Interactive Graphs with D3.js&lt;/h2&gt;
    &lt;p&gt;
      D3.js is known for its ability to create dynamic, interactive graphs that
      can bring data to life. However, mastering D3.js comes with a steep
      learning curve. Participants in the Big Dive course learn how to overcome
      these challenges, create effective data visualizations, and explore
      alternatives like RShiny for those who prefer less complex tools.
      Understanding the intricacies of D3.js not only enhances the visual appeal
      of data presentations but also improves the interpretability of the data
      itself.
    &lt;/p&gt;
    &lt;h2&gt;Data Science Bootcamps: Are They Worth It?&lt;/h2&gt;
    &lt;p&gt;
      Data science bootcamps, like the Big Dive, are becoming increasingly
      popular as a fast track to acquiring the skills needed in the industry.
      But are they worth the investment? The answer is a resounding yes. At
      least Big Dive course made a significant touchpoint in my career. The
      course offers a unique opportunity to gain practical experience, build a
      professional network, and explore the world of big data in depth. For
      those looking to accelerate their career in data science, a bootcamp like
      the Big Dive is an excellent starting point.
    &lt;/p&gt;

    &lt;div class=&quot;scalable scalable-16-9&quot;&gt;
      &lt;div class=&quot;scalable-content&quot;&gt;
        &lt;iframe
          src=&quot;https://www.youtube.com/embed/y1Zj_pOx_iE&quot;
          allowfullscreen
        &gt;&lt;/iframe
        &gt;&lt;br /&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;em&gt;Big Dive Yearbook 2017&lt;/em&gt;
    &lt;h2&gt;Key Skills Acquired During the Big Dive&lt;/h2&gt;
    &lt;p&gt;
      Participants leave the Big Dive course with a wealth of new skills. These
      include distributed computing, which is essential for handling large
      datasets, and the basics of PySpark, a tool widely used in big data
      processing. The course also offers a refresher on machine learning and
      network science, with practical mini-projects that can be used as
      portfolio pieces. The importance of understanding the data science cycle
      and applying it to real-world scenarios is another critical takeaway from
      the course.
    &lt;/p&gt;
    &lt;h2&gt;Challenges Faced During the Course&lt;/h2&gt;
    &lt;p&gt;
      Learning new technologies and tools is never without its challenges. The
      Big Dive course participants face several hurdles, such as the steep
      learning curve of D3.js and the complexities of distributed computing with
      Dask. Additionally, handling large datasets often requires substantial
      computational resources, which can be a significant challenge. However,
      overcoming these challenges is a key part of the learning process,
      preparing participants for the demands of the industry.
    &lt;/p&gt;
    &lt;h2&gt;Networking Opportunities and Professional Growth&lt;/h2&gt;
    &lt;p&gt;
      One of the most valuable aspects of the Big Dive course is the networking
      opportunities it offers. Participants build connections with industry
      professionals, course instructors, and peers, all of whom can play a
      crucial role in their professional development. The course also provides
      resources for career development, such as a CV repository and job postings
      from partner companies. Leveraging these connections on platforms like
      LinkedIn can significantly boost one’s career in data science.
    &lt;/p&gt;

    &lt;h3&gt;Practical aspect of the big dive data science bootcamp&lt;/h3&gt;
    &lt;p&gt;
      The course took place in a group of 20 students, with
      STEM/linguistic/design background, mostly from Italy but there were also a
      few foreign folks like me. Our diversity helped a lot during the group
      project when each of us could bring different perspective and skillset.
    &lt;/p&gt;

    &lt;h3&gt;How to apply&lt;/h3&gt;
    &lt;p&gt;
      In order to apply, a candidate needs to send a video explaining why he or
      she is a good fit for the big dive as well as state level of her/his
      prerequisites.
    &lt;/p&gt;

    &lt;h3&gt;Detailed content of the big dive bootcamp&lt;/h3&gt;
    &lt;p&gt;
      If you want to read more about the content of the course you can check out
      &lt;a href=&quot;http://www.bigdive.eu/&quot; target=&quot;_blank&quot;&gt;Big Dive website&lt;/a&gt; as
      well as read their great
      &lt;a
        href=&quot;https://www.linkedin.com/in/christianracca/detail/recent-activity/posts/&quot;
        target=&quot;_blank&quot;
        &gt;posts on LinkedIn&lt;/a
      &gt;
      of
      &lt;a href=&quot;https://www.linkedin.com/in/christianracca/&quot; target=&quot;_blank&quot;
        &gt;Christian Racca&lt;/a
      &gt;,
      &lt;a href=&quot;https://www.facebook.com/bigdive.eu/&quot; target=&quot;_blank&quot;
        &gt;Facebook page&lt;/a
      &gt;
      and
      &lt;a href=&quot;https://twitter.com/bigdive_eu&quot; target=&quot;_blank&quot;
        &gt;twitter @bigdive_eu&lt;/a
      &gt;.
    &lt;/p&gt;

    &lt;h3&gt;Who are the participants of the data science bootcamp ?&lt;/h3&gt;
    &lt;p&gt;
      For most of the participants and for me it was the first time with Big
      data. I found the possibility to face the problems and tools I don&apos;t
      encounter in my everyday work. The content of the course was quite densely
      packed and I will need some time after the course to practice new skills
      and read all material. Thanks to great teachers like among others
      &lt;a href=&quot;https://www.linkedin.com/in/alexcomu/&quot; target=&quot;_blank&quot;
        &gt;Alex Comunian&lt;/a
      &gt;
      and
      &lt;a href=&quot;https://www.linkedin.com/in/abusedmedia/&quot; target=&quot;_blank&quot;
        &gt;Fabio Franchino&lt;/a
      &gt;
      classes were very interactive and easy to follow.
    &lt;/p&gt;
    &lt;h3&gt;Who are the instructors of the data science bootcamp ?&lt;/h3&gt;
    &lt;p&gt;
      The instructors of the Big Dive course bring a wealth of industry
      experience to the table. Their insights into the practical applications of
      data science, the importance of domain knowledge, and the latest trends in
      the field are invaluable. Participants benefit from their interactive
      teaching style and the real-world examples they provide. The course also
      emphasizes the importance of having a field expert guide the analysis,
      ensuring that the data science work is truly impactful in its application.
    &lt;/p&gt;

    &lt;h2&gt;The Future of Data Science and Engineering&lt;/h2&gt;
    &lt;p&gt;
      As data science and engineering continue to evolve, staying updated with
      the latest trends and technologies is crucial. The Big Dive course
      prepares participants for future challenges by introducing them to
      emerging fields like deep learning and artificial intelligence.
      Understanding these trends and how they will shape the future of data
      science is essential for anyone looking to stay competitive in the
      industry.
    &lt;/p&gt;

    &lt;h3&gt;Personal highlights&lt;/h3&gt;
    &lt;p&gt;
      &lt;li&gt;
        &lt;b&gt;D3.js&lt;/b&gt; is a very powerful tool for data visualization but its
        learning curve is quite steep. Learning jquery and communication with a
        remote data server, basics of web design are necessary to create a
        serious final product. So if you don&apos;t want to spend a significant
        amount of time learning all the toolkit better go for pre-set solutions
        like RShiny if you know R for instance.
      &lt;/li&gt;
      &lt;li&gt;
        &lt;b&gt;Dask&lt;/b&gt; library seems to be an interesting alternative to Spark for
        smaller projects, the big plus is that it is using pandas vocabulary. It
        can be also misleading as we encountered quite a lot of problems
        building a set up for our data, lots of ram memory seems to be necessary
        and understanding how dask is dealing with our objects differently than
        pandas is crucial for a successful application.
      &lt;/li&gt;
      &lt;li&gt;
        &lt;b&gt;Spark&lt;/b&gt; contains important differences in algorithms from R or
        scikit-learn implementations. Therefore testing locally and comparing to
        the distributed version is not a good idea and difference in performance
        can be significant. It is important to keep it in mind from the
        beginning of the project.
      &lt;/li&gt;
      &lt;li&gt;
        &lt;b&gt;Github&lt;/b&gt; has some really cool project management features like
        issues system and wiki. Usually, it is not extensively used in academia
        or only as a code repository while for us the management part was really
        useful and handy to organize work. If you are a student you can get a
        &lt;a href=&quot;https://education.github.com/pack&quot; target=&quot;_blank&quot;
          &gt;Student Developer Pack for free&lt;/a
        &gt;.
      &lt;/li&gt;
      &lt;li&gt;
        A &lt;b&gt;field expert&lt;/b&gt; is an important part of the team. His needs should
        guide the analysis, it helps to make data scientist work really useful
        in the domain of its application
      &lt;/li&gt;
      &lt;span class=&quot;image fit&quot;
        &gt;&lt;img src=&quot;/images/dataring.png&quot; alt=&quot;&quot; /&gt;&lt;img
          src=&quot;/images/copyrights.png&quot;
          alt=&quot;&quot;
      /&gt;&lt;/span&gt;

      &lt;li&gt;
        &lt;b&gt;Data ring&lt;/b&gt; - a tool proposed by top-ix and IFC helps to plan the
        project and communicate within data science team and with the business
        partner. You can find more information about in the comprehensive
        handbook
        &lt;a
          href=&quot;http://www.ifc.org/wps/wcm/connect/22ca3a7a-4ee6-444a-858e-374d88354d97/IFC+Data+HandBook+FINAL.pdf?MOD=AJPERES&quot;
          target=&quot;_blank&quot;
          &gt;DATA ANALYTICS AND DIGITAL FINANCIAL SERVICES&lt;/a
        &gt;
      &lt;/li&gt;
      &lt;li&gt;
        In my project, I worked with e-money transaction data and I found the
        problematic pretty interesting, it encouraged me to read finance and
        economy articles.
      &lt;/li&gt;
      &lt;span class=&quot;image fit&quot;
        &gt;&lt;img src=&quot;/images/final.png&quot; alt=&quot;&quot; /&gt;&lt;em
          &gt;Image rights TOP-IX&lt;/em
        &gt;&lt;/span
      &gt;
    &lt;/p&gt;

    &lt;h2&gt;Conclusion: Personal Reflections on the Big Dive&lt;/h2&gt;
    &lt;p&gt;
      The Big Dive course offers a unique and intense learning experience that
      is both challenging and rewarding. Participants gain valuable skills,
      build professional networks, and explore the world of big data in depth.
      While it’s impossible to become an expert in big data in just five weeks,
      the course provides a strong foundation and sparks a passion for continued
      learning in data science. For anyone considering a career in data science,
      the Big Dive is an excellent starting point.
    &lt;/p&gt;
    &lt;span class=&quot;image fit&quot;
      &gt;&lt;img src=&quot;/images/bigdive.jpg&quot; alt=&quot;&quot; /&gt;&lt;em
        &gt;Image rights TOP-IX&lt;/em
      &gt;&lt;/span
    &gt;
    &lt;h2&gt;Frequently Asked Questions (FAQs)&lt;/h2&gt;
    &lt;p&gt;&lt;strong&gt;1. What is the Big Dive Course About?&lt;/strong&gt;&lt;/p&gt;
    &lt;p&gt;
      The Big Dive course is a five-week intensive program designed to equip
      participants with practical skills in big data, distributed computing, and
      data visualization. It includes hands-on projects, industry insights, and
      networking opportunities.
    &lt;/p&gt;
    &lt;p&gt;&lt;strong&gt;2. Is a Data Science Bootcamp Worth It?&lt;/strong&gt;&lt;/p&gt;
    &lt;p&gt;
      Yes, data science bootcamps like the Big Dive are worth it for those
      looking to gain practical experience and industry connections quickly.
      They offer a fast track to acquiring in-demand skills and provide a solid
      foundation for a career in data science.
    &lt;/p&gt;

    &lt;p&gt;&lt;strong&gt;3. How Can I Transition from Academia to Industry?&lt;/strong&gt;&lt;/p&gt;
    &lt;p&gt;
      The Big Dive course is an excellent way to transition from academia to
      industry. It bridges the gap by offering real-world projects, networking
      opportunities, and exposure to industry-standard tools and practices.
    &lt;/p&gt;

    &lt;p&gt;&lt;strong&gt; 4. What Tools Should I Learn for Data Science?&lt;/strong&gt;&lt;/p&gt;
    &lt;p&gt;
      Key tools include Python, Pandas, PySpark, D3.js, and AWS. Understanding
      distributed computing, data visualization, and machine learning is also
      crucial for success in data science.
    &lt;/p&gt;

    &lt;p&gt;&lt;strong&gt; 5. How Important is Data Visualization?&lt;/strong&gt;&lt;/p&gt;
    &lt;p&gt;
      Data visualization is vital for effectively communicating complex data
      insights. Tools like D3.js are powerful for creating interactive and
      dynamic visualizations that make data more accessible and actionable.
    &lt;/p&gt;

    &lt;p&gt;&lt;strong&gt; 6. Where Can I Learn More About Data Science?&lt;/strong&gt;&lt;/p&gt;
    &lt;p&gt;
      Apart from bootcamps like Big Dive, you can learn more about data science
      through online courses, books, articles, and by joining data science
      communities. Continuing education is key to staying competitive in the
      field.
    &lt;/p&gt;

    &lt;h2&gt;Resources and Further Reading&lt;/h2&gt;
    &lt;p&gt;
      Continuing education is essential in the fast-paced field of data science.
      The Big Dive course provides participants with a list of recommended
      books, articles, and websites to further their learning. Joining data
      science communities and participating in online forums are also encouraged
      as ways to stay updated and connected with the industry. Resources like
      these ensure that participants continue to grow their skills long after
      the course has ended.
    &lt;/p&gt;
    &lt;h3&gt;Useful links&lt;/h3&gt;
    &lt;p&gt;
      &lt;li&gt;
        &lt;a href=&quot;http://www.bigdive.eu/&quot; target=&quot;_blank&quot;
          &gt;&lt;b&gt;Big Dive website&lt;/b&gt;: http://www.bigdive.eu/&lt;/a
        &gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;a
          href=&quot;https://www.linkedin.com/in/christianracca/detail/recent-activity/posts/&quot;
          target=&quot;_blank&quot;
          &gt;&lt;b&gt;posts on LinkedIn&lt;/b&gt;:
          https://www.linkedin.com/in/christianracca/detail/recent-activity/posts/&lt;/a
        &gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;a href=&quot;https://www.facebook.com/bigdive.eu/&quot; target=&quot;_blank&quot;
          &gt;&lt;b&gt;Facebook page&lt;/b&gt;: https://www.facebook.com/bigdive.eu/&lt;/a
        &gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;a href=&quot;https://twitter.com/bigdive_eu&quot; target=&quot;_blank&quot;
          &gt;&lt;b&gt;twitter&lt;/b&gt;: @bigdive_eu&lt;/a
        &gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;a
          href=&quot;http://www.kdnuggets.com/2016/04/data-science-comprehensive-guide-transition.html&quot;
          target=&quot;_blank&quot;
          &gt;post that inspired me to participate in Big DIVE : From Science to
          Data Science, a Comprehensive Guide for Transition&lt;/a
        &gt;
      &lt;/li&gt;
    &lt;/p&gt;

    &lt;h3&gt;Acknowledgements&lt;/h3&gt;
    &lt;p&gt;&lt;/p&gt;
    I would like to thank Amodsen Chiota and Pierre Girard for their support for
    my application. As well as my PhD supervisor Andrei Zinovyev for allowing to
    participate in the course during my PhD time. Funding was provided by “Ecole
    Doctorale Frontières du Vivant (FdV) – Programme Bettencourt.”

    &lt;!-- Go to www.addthis.com/dashboard to customize your tools --&gt;
    &lt;script
      type=&quot;text/javascript&quot;
      src=&quot;//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-584ec4ce89deed84&quot;
    &gt;&lt;/script&gt;
    &lt;!-- Go to www.addthis.com/dashboard to customize your tools --&gt;
    &lt;div class=&quot;addthis_inline_share_toolbox&quot;&gt;&lt;/div&gt;
  &lt;/section&gt;
&lt;/html&gt;
</description>
        <pubDate>Mon, 28 Aug 2017 00:00:00 +0200</pubDate>
        <link>http://urszulaczerwinska.github.io/thoughts/bigdive/</link>
        <guid isPermaLink="true">http://urszulaczerwinska.github.io/thoughts/bigdive/</guid>
        
        
        <category>thoughts</category>
        
      </item>
    
  </channel>
</rss>
