<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Urszula Czerwinska | Python, TensorFlow Expert | Data Scientist &amp; Deep Learning Engineer</title>
    <description>Explore the journey of Urszula Czerwinska from PhD to Data Science, featuring insights on Data Science projects, Machine Learning, and Deep Learning. Discover how to become a Data Scientist or Machine Learning Engineer.</description>
    <link>http://urszulaczerwinska.github.io/</link>
    <atom:link href="http://urszulaczerwinska.github.io/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Fri, 27 Sep 2024 09:49:10 +0200</pubDate>
    <lastBuildDate>Fri, 27 Sep 2024 09:49:10 +0200</lastBuildDate>
    <generator>Jekyll v4.3.3</generator>
    
      <item>
        <title></title>
        <description>&lt;head&gt;
  &lt;meta name=&quot;description&quot; content=&quot;An overview of the GenAI course and workshop I attended, focusing on the challenges and strategies for implementing RAG in production environments, optimizing performance, and evaluating the results.&quot; /&gt;
&lt;/head&gt;

&lt;h2 id=&quot;my-experience-at-the-genai-course-implementing-rag-in-production-environments&quot;&gt;My experience at the GenAI course: Implementing RAG in production environments&lt;/h2&gt;

&lt;p&gt;This past quarter, I had the opportunity to participate in a comprehensive internal Generative AI training course of &lt;a href=&quot;https://adevinta.com/&quot;&gt;Adevinta&lt;/a&gt; and &lt;a href=&quot;https://leboncoincorporate.com/&quot;&gt;Leboncoin&lt;/a&gt;, which culminated in a two-day workshop focused on hands-on applications. The training covered a wide array of topics, with the key focus on &lt;strong&gt;Retrieval-Augmented Generation (RAG)&lt;/strong&gt;, an emerging approach in leveraging large language models (LLMs) in real-world applications.&lt;/p&gt;

&lt;p&gt;Adevinta is a global classifieds specialist with market-leading positions in key European markets that aims to find perfect matches between its users and the platforms’ goods.&lt;/p&gt;

&lt;p&gt;Before this workshop, I had encountered RAG in research papers, but seeing it in action, especially when applied to real-world problems, was a valuable experience. I learned that creating a flashy demo is one thing, but scaling that demo to work in production environments and taking into account ethical considerations is not as easy. Here’s a closer look at what we experienced and the lessons learned along the way.&lt;/p&gt;

&lt;h3 id=&quot;1-overview-of-the-genai-training-structure-and-goals&quot;&gt;1. &lt;strong&gt;Overview of the GenAI Training: Structure and Goals&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;The course was split into three main segments:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Self-paced online learning modules&lt;/strong&gt; designed to introduce foundational concepts of GenAI.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Customized live sessions&lt;/strong&gt; focused on addressing specific use cases relevant to participants’ day-to-day work.&lt;/li&gt;
  &lt;li&gt;A &lt;strong&gt;two-day intensive workshop&lt;/strong&gt;, where we were tasked with building Minimum Viable Products (MVPs) using RAG and other GenAI technologies.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The course was divided into three sections. The first involved self-paced learning, where I had to work through a series of online modules on AI and machine learning.&lt;/p&gt;

&lt;p&gt;The courses suggested in the self-paced learning modules were:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;How Diffusion Models Work - free course&lt;/li&gt;
  &lt;li&gt;Generative AI with LLMs - Coursera 45 eur&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;my-feedback-on-how-diffusion-models-work-course&quot;&gt;My feedback on &lt;a href=&quot;https://www.deeplearning.ai/short-courses/how-diffusion-models-work/&quot;&gt;“How Diffusion Models Work”&lt;/a&gt; course&lt;/h4&gt;
&lt;p&gt;This a flash course covers basics of Diffusion Models, introducing simple code script explaining the magic of going from the noise to a brand new generated image, image of a sprite. It also explains the UNet architecture, the use of DDPM noise schedule and possible optimization with DDIM noise. It also gives a primer on controlling the network with classes vector. Even thought in a very short time it covers lot of topic, I personally found it quite confusing. It does covers lots of topics but the “artificial” setup of sprite generation left me we with little understanding how to make the next step towards real world application. I would recommend this course to someone who is looking for a quick overview of the topic, but not to someone who is looking for a deep dive into the topic.&lt;/p&gt;

&lt;p&gt;If you are looking for a free and more complete course on Diffusion and Stable Diffusion, I highly recommend you the &lt;a href=&quot;https://huggingface.co/learn/diffusion-course/en/unit0/1&quot;&gt;Huggingface stable diffusion course&lt;/a&gt;. It is longer and “slower” pace but it gives you a better understanding of the topic, imo.&lt;/p&gt;

&lt;h4 id=&quot;my-feedback-on-generative-ai-with-llms-course&quot;&gt;My feedback on &lt;a href=&quot;https://www.coursera.org/learn/generative-ai-with-llms&quot;&gt;“Generative AI with LLMs”&lt;/a&gt; course&lt;/h4&gt;

&lt;p&gt;I happily received a certificate of accomplishing this course. It gives a good overview of LLM architectures, explaining transformers, attention and text generation. Explanations are clear and it is a good refresher on the topic. There was also an introduction to prompting.&lt;/p&gt;

&lt;p&gt;From more advanced topics, I appreciated the explanation of PEFT with LoRA but also Soft tuning that I discoverd in that course. I understood way lass the module on RLHF and the practical use of it. Finally the use of chain of thoughts was a good primer.&lt;/p&gt;

&lt;p&gt;It uses Bedrock with AWS to run the labs. I found the labs the least interesting part of the course. The setup was easy and smooth but the use cases seemed “too easy”. Using only models and examples adapted to LoRA or PEFT out of the box. The data was already prepared and there was no need to do any data preprocessing. I would have appreciated more real world examples and more complex use cases. Running the labs was, in my opinion, not very useful and reduced to executing a few jupyter cells with not much understanding of what is happening in the background.&lt;/p&gt;

&lt;h4 id=&quot;the-practical-in-house-curriculum&quot;&gt;The practical, in house, curriculum&lt;/h4&gt;

&lt;p&gt;While overall these were informative, I found the real learning happened during the second segment, where we had live sessions tailored to the specific problems we faced in our own work environments. Here, discussions around industry-specific challenges helped me think about how the theory behind RAG could translate into everyday business problems.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Using GenAI in Adevinta (our company)&lt;/li&gt;
  &lt;li&gt;Prompt Engineering&lt;/li&gt;
  &lt;li&gt;Using Bedrock with langchain&lt;/li&gt;
  &lt;li&gt;Retrieval augmented generation&lt;/li&gt;
  &lt;li&gt;Agents&lt;/li&gt;
  &lt;li&gt;Security and risks&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Throughout the sessions, we worked with tools such as &lt;a href=&quot;https://www.langchain.com/&quot;&gt;&lt;strong&gt;LangChain&lt;/strong&gt;&lt;/a&gt;, &lt;strong&gt;LLM APIs&lt;/strong&gt; such as &lt;a href=&quot;https://aws.amazon.com/fr/bedrock/?gclid=Cj0KCQjw3bm3BhDJARIsAKnHoVUupzZaHfVWRIQi8FvbGme27pzjfIrGOiLeqJ713jiTHLw1ujZj6NEaAu8JEALw_wcB&amp;amp;trk=f1f5028e-0107-40fd-a47b-6bf2ad7d99f5&amp;amp;sc_channel=ps&amp;amp;ef_id=Cj0KCQjw3bm3BhDJARIsAKnHoVUupzZaHfVWRIQi8FvbGme27pzjfIrGOiLeqJ713jiTHLw1ujZj6NEaAu8JEALw_wcB:G:s&amp;amp;s_kwcid=AL!4422!3!692062117823!e!!g!!bedrock!21054970526!158684164545&quot;&gt;Amazon Bedrock&lt;/a&gt;, and integrated &lt;strong&gt;embeddings&lt;/strong&gt;. While the self-paced learning helped us grasp the basics, the live sessions were where we could apply these concepts to real-world scenarios.
I am really grateful to our teachers, our colleagues and the company for giving us the opportunity to learn and apply these new technologies. The content was well-prepared and delivered with passion and patience.&lt;/p&gt;

&lt;h4 id=&quot;the-two-day-workshop&quot;&gt;The two-day workshop&lt;/h4&gt;

&lt;p&gt;The final part, a two-day workshop, pushed us out of our comfort zones. We were tasked diving deep in one of the concepts. My team focused on building a RAG to retrieve data from additional sources and applying different RAG techniques. By the end of the first day, we had something functional—but it was far from production-ready and we had lots of ideas. We spent most of our time improving the solution, which taught me that even the smallest bottlenecks can make or break an AI system.&lt;/p&gt;

&lt;p&gt;Some other projects developed by my colleagues were:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;SAFA, a data analyst assistant to answer to fraud questions .&lt;/li&gt;
  &lt;li&gt;filling out a classified Ad from their photos.&lt;/li&gt;
  &lt;li&gt;scoring the match between an Ad and a CV&lt;/li&gt;
  &lt;li&gt;system computing how good a RAG system is.&lt;/li&gt;
  &lt;li&gt;a data analyst assistant&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;what-is-rag-and-why-is-it-important-for-genai&quot;&gt;&lt;strong&gt;What is RAG and Why Is It Important for GenAI?&lt;/strong&gt;&lt;/h5&gt;

&lt;p&gt;Let’s step back to clarify the concept of RAG and its significance in the GenAI landscape.&lt;/p&gt;

&lt;p&gt;Retrieval-Augmented Generation is a hybrid approach that combines the power of &lt;strong&gt;retrieval systems&lt;/strong&gt; with &lt;strong&gt;generative models&lt;/strong&gt; to enhance the accuracy and reliability of large language models. Unlike pure generative models that sometimes hallucinate or provide irrelevant information, RAG utilizes an external knowledge base to retrieve contextually relevant information, feeding it into a language model for generation.&lt;/p&gt;

&lt;p&gt;RAG is especially useful as &lt;strong&gt;question-answering systems&lt;/strong&gt; and use cases that require accurate, up-to-date, and verifiable information. In our company, my colleagues build “ADA bot” that can answer user questions in a safe environment that prevents data leakage to LLM providers. It is also connected to some of the internal databases to provide the most up-to-date information on specific internal topics.&lt;/p&gt;

&lt;p&gt;To learn more about rag you can have a look into these ressources:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.datacamp.com/blog/what-is-retrieval-augmented-generation-rag&quot;&gt;What is RAG ?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://aws.amazon.com/what-is/retrieval-augmented-generation/&quot;&gt;RAG by AWS&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://research.ibm.com/blog/retrieval-augmented-generation-RAG&quot;&gt;IBM RAG&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p align=&quot;center&quot;&gt;&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/T-D1OfcDW1M?si=vZ2HQE9-yxGDdMJL&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;sub&gt;&lt;sup&gt;What is RAG ? Video by [IBM](https://research.ibm.com/blog/retrieval-augmented-generation-RAG)&lt;/sup&gt;&lt;/sub&gt;&lt;/p&gt;

&lt;h4 id=&quot;the-two-day-workshop-rag-project&quot;&gt;The two-day workshop RAG project&lt;/h4&gt;

&lt;p&gt;In the workshop we first added selected content from three different sources: company Github, company internal Confluence, and company intranet. We used the LangChain to connect to these sources.&lt;/p&gt;

&lt;p&gt;We also generated with LLM Claude set of QA about each added document to be able to test the retrieval afterwards. Already at this step some prompt engineering was needed to make sure the QA were relevant.&lt;/p&gt;

&lt;p&gt;We experimented three approaches to RAG:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;simple RAG with basic prompt searching in all connected sources&lt;/li&gt;
  &lt;li&gt;LLM classified RAG with a classifier that would decide which source to search&lt;/li&gt;
  &lt;li&gt;ReRanker RAG that would search in all sources and then rerank the results&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We were highly inspired by &lt;a href=&quot;https://github.com/NirDiamant/RAG_Techniques/tree/main&quot;&gt;Nir Diamant repo&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;As over two days we need to focus on specific optimization axes we decided not to spend too much time on prompt engineering or testing different LLM models. We used “anthropic.claude-3-haiku-20240307-v1:0” as our LLM model and “cohere.embed-multilingual-v3” for embeddings as there were proof tested before as good for our use case.&lt;/p&gt;

&lt;p&gt;We used existing &lt;a href=&quot;https://streamlit.io/&quot;&gt;streamlit&lt;/a&gt; interface of ADA bot to display the results adding our RAG-augmented LLM version.&lt;/p&gt;

&lt;p&gt;As the outcome, we managed to index a few dozens of pages from selected sources the simple rag would work correctly searching in a common database.&lt;/p&gt;

&lt;p&gt;Then we implemented the idea of classifying the question to indicate the right source. This could have advantages of optimization and relevance improvement. We tested two ways of question classification : with an LLM model and with a BERT classifier. LLM prompt was challenging but we ended up with &amp;gt;80% precision. The classifier was easier to implement and gave similar precision with a short training time. Both options seems to be valid for our use case and could be used depending on the resources available and the overall system design. In the real world, we would need to find a solution for a question that needs several sources to be searched.&lt;/p&gt;

&lt;p&gt;Finally, we implemented the Reranker. The reranker was able to rerank the results. The reranker was able to rerank the results based on the relevance of the sources and the questions. The results were better than the simple rag and the classifier.&lt;/p&gt;

&lt;h3 id=&quot;2-challenges-in-scaling-genai-and-rag-applications&quot;&gt;2. &lt;strong&gt;Challenges in Scaling GenAI and RAG Applications&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;One of the main takeaways from the workshop was the difficulty in scaling GenAI applications, particularly RAG systems. While creating a POC was straightforward, thanks to frameworks like &lt;strong&gt;LangChain&lt;/strong&gt;, taking it to a production-ready solution would be a whole different challenge.&lt;/p&gt;

&lt;p&gt;Some of the critical challenges in scaling GenAI applications include:&lt;/p&gt;

&lt;h4 id=&quot;data-availability-and-retrieval-performance&quot;&gt;&lt;strong&gt;Data availability and retrieval performance&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;Effective RAG systems depend on robust, well-maintained knowledge bases. Scaling such systems requires integrating large and frequently updated datasets, which increases the complexity of both &lt;strong&gt;retrieval speed&lt;/strong&gt; and &lt;strong&gt;index management&lt;/strong&gt;. Throwing in not curated and not cleaned data can lead to garbage in garbage out. The way data can be cleaned is not fully solved, especially if we are talking of years of company knowledgebase or codebase. I&lt;/p&gt;

&lt;p&gt;In any production-level AI system, performance optimization is crucial. Here are some strategies that can be explored for improving the efficiency and reliability of RAG-based systems:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Using pre-computed embeddings&lt;/strong&gt;: To minimize computational overhead during inference, a solution could be to pre-computed embeddings for frequently accessed data. This allows faster lookup times when retrieving relevant documents.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Batch processing&lt;/strong&gt;: Rather than processing every query individually, batching similar queries allows to take advantage of parallelism, significantly speeding up the retrieval process.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Indexing strategies&lt;/strong&gt;: Different indexing algorithms, such as &lt;strong&gt;Faiss&lt;/strong&gt; and &lt;strong&gt;Annoy&lt;/strong&gt;, can optimize how the knowledge base is queried. The right indexing technique can drastically reduce the time required for retrieving documents from large datasets.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you are interested in the topic, I recommend you to have a look at &lt;a href=&quot;https://arxiv.org/abs/2403.09727&quot;&gt;this article&lt;/a&gt; and also &lt;a href=&quot;https://hyperight.com/6-ways-for-optimizing-rag-performance/&quot;&gt;this overview&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;mitigating-hallucinations&quot;&gt;&lt;strong&gt;Mitigating hallucinations&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;Even though RAG reduces hallucinations by leveraging external data, managing these risks becomes more challenging when scaling across different industries and datasets. Fact-checking and ensuring the relevance of retrieved information is not difficult in a sandbox environment but much more difficult with billions of documents. Here is some reading on this topic: &lt;a href=&quot;https://www.k2view.com/blog/rag-hallucination/&quot;&gt;RAG Hallucination: What is It and How to Avoid It&lt;/a&gt;, &lt;a href=&quot;https://www.wired.com/story/reduce-ai-hallucinations-with-rag/&quot;&gt;Tricks to reduce RAG hallucinations&lt;/a&gt;, &lt;a href=&quot;https://medium.com/autonomous-agents/rag-does-not-reduce-hallucinations-in-llms-math-deep-dive-900107671e10&quot;&gt;RAGs Do Not Reduce Hallucinations in LLMs — A Math Deep Dive&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&quot;adversarial-attacks&quot;&gt;&lt;strong&gt;Adversarial attacks&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;During the workshop, we touched on the importance of securing models against adversarial prompts that could manipulate the retrieval mechanism, potentially leading to incorrect or malicious outputs. This can be a real challenge in a company setup, especially if the bot is connected to the confidential information. In the course we haven several approaches to test-proof the system against adversarial attacks, but it is still a challenge to make sure the system is secure. Here is &lt;a href=&quot;https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/&quot;&gt;a great in depth article in this topic&lt;/a&gt;.&lt;/p&gt;
&lt;h4 id=&quot;scoring-and-evaluating-rag-system&quot;&gt;&lt;strong&gt;Scoring and evaluating RAG system&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;We also discussed the importance of evaluating the performance of RAG systems, particularly in real-world scenarios. This involves measuring the relevance of retrieved information, response accuracy, and latency under load. We tested a few ways to evaluate the system, mostly through another LLM designed to evaluate the system. The pitfall is that this is kind of evaluation is resource consuming and not always possible in a real world setup. Also we were not sure if we can really trust an LLM evaluation. I have explored some litterature on this topic and I found &lt;a href=&quot;https://dev.to/guybuildingai/-top-5-open-source-llm-evaluation-frameworks-in-2024-98m&quot;&gt;this article on Top 5 Open-Source LLM Evaluation Frameworks (2024)&lt;/a&gt; very interesting. Also &lt;a href=&quot;https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation&quot;&gt;this article that discuss the traditional evaluations like BLEU or ROUGE&lt;/a&gt;. Finally &lt;a href=&quot;https://eugeneyan.com/writing/llm-evaluators/?utm_source=brevo&amp;amp;utm_campaign=december_23_newsletter&amp;amp;utm_medium=email&quot;&gt;this article&lt;/a&gt; that emphasizes the importance of the human alignement.&lt;/p&gt;

&lt;h3 id=&quot;3-implementing-rag-workshop-lessons-learned&quot;&gt;3. &lt;strong&gt;Implementing RAG workshop: lessons learned&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;During the two-day project sprint, my team worked on developing a &lt;strong&gt;data retrieval assistant&lt;/strong&gt; applying different RAG strategies. We imagined several challenges when transitioning from POC to a more robust solution.&lt;/p&gt;

&lt;p&gt;Getting a POC up and running with hashtag#LangChain and LLM API is actually easier than you might think.&lt;/p&gt;

&lt;p&gt;However, taking that to the next level—building a robust, production-ready solution that avoids hallucinations and can handle adversarial attacks—is a whole new challenge.&lt;/p&gt;

&lt;p&gt;Taken into account the overall panorama of presented projects, compared to those early post-ChatGPT hacathons, today’s approaches are more mature. We’re now solving real-world problems rather than just chasing the shiny tech. More precisely, we’ve got a clearer picture of what LLMs can do—and what they can’t.&lt;/p&gt;

&lt;h3 id=&quot;4-ethical-considerations-and-ai-governance&quot;&gt;4. &lt;strong&gt;Ethical Considerations and AI Governance&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;AI ethics played a significant role in our discussions, particularly around how to manage the risks of bias and data privacy in RAG systems. For example, we had to think about how biased data in a knowledge base could influence the model’s outputs. To address this, we discussed &lt;a href=&quot;https://shelf.io/blog/10-step-rag-system-audit-to-eradicate-bias-and-toxicity/&quot;&gt;methods for detecting and mitigating bias&lt;/a&gt; during the retrieval process.&lt;/p&gt;

&lt;p&gt;We also considered the issue of data privacy. Retrieving sensitive information, especially in fields like healthcare or finance, can raise significant privacy concerns. Proper anonymization and data governance strategies are essential to ensure that personal or confidential data is protected.&lt;/p&gt;

&lt;h3 id=&quot;8-final-thoughts-moving-beyond-the-workshop&quot;&gt;8. &lt;strong&gt;Final Thoughts: Moving beyond the workshop&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;The workshop gave me a clearer understanding of the practical challenges associated with RAG and other GenAI applications. Implementing these systems in real-world production environments is far more nuanced than building a POC. The key lies in continuous &lt;strong&gt;performance optimization&lt;/strong&gt;, &lt;strong&gt;scaling&lt;/strong&gt;, and &lt;strong&gt;ethical governance&lt;/strong&gt;. As GenAI technologies evolve, I’m excited to continue exploring how these advancements can be applied to solve complex, real-world problems.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;frequently-asked-questions-faqs&quot;&gt;Frequently Asked Questions (FAQs)&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;How can you implement RAG in LLMs?&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;RAG can be implemented by combining a retrieval mechanism with a generative model, enabling the model to fetch relevant information from external databases before generating a response.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;What are the challenges in scaling GenAI applications?&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Major challenges include handling large-scale data retrieval, maintaining data consistency, avoiding hallucinations, and securing the system against adversarial attacks.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;How do you optimize RAG systems for better performance?&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Performance optimization strategies include pre-computing embeddings, batch processing, and selecting the appropriate indexing algorithms for faster data retrieval.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;How do you evaluate the performance of RAG systems?&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Evaluation focuses on response accuracy, relevance of retrieved information, latency under load, and error analysis.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Can RAG be used for question-answering systems?&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Yes, RAG is particularly effective for question-answering systems that need to pull relevant data from knowledge bases to provide accurate, up-to-date answers.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;What are the ethical concerns when implementing RAG systems?&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Ethical concerns include the risk of bias in retrieved data, privacy issues when handling sensitive information, and the need for robust AI governance.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;footer&gt;
  &lt;p&gt;Exported from &lt;a href=&quot;https://medium.com&quot;&gt;Medium&lt;/a&gt; on June 06,
    2023.&lt;/p&gt;
  &lt;p&gt;&lt;a href=&quot;https://medium.com/adevinta-tech-blog/deep-dive-in-paddleocr-inference-e86f618a0937&quot;&gt;View
      the original. This article was orignally co-authored by Cognition team members, special credits to Joaquin Cabezas&lt;/a&gt;&lt;/p&gt;
&lt;/footer&gt;
&lt;script type=&quot;text/javascript&quot; src=&quot;//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-584ec4ce89deed84&quot;&gt;&lt;/script&gt;

&lt;div class=&quot;addthis_inline_share_toolbox&quot;&gt;&lt;/div&gt;

</description>
        <pubDate>Fri, 27 Sep 2024 09:49:10 +0200</pubDate>
        <link>http://urszulaczerwinska.github.io/2024-09-22-Implementing-rag-systems.html</link>
        <guid isPermaLink="true">http://urszulaczerwinska.github.io/2024-09-22-Implementing-rag-systems.html</guid>
        
        
      </item>
    
      <item>
        <title>Dilated Diffusion from DemoFusion</title>
        <description>&lt;head&gt;
  &lt;meta name=&quot;description&quot; content=&quot;Learn how DemoFusion’s dilated diffusion revolutionizes AI image generation, making high-resolution results accessible using standard hardware. Dive into dilated sampling and progressive upscaling techniques.&quot; /&gt;
&lt;/head&gt;

&lt;p&gt;&lt;strong&gt;TDLR:&lt;/strong&gt; see demo of dilated diffusion &lt;a href=&quot;https://colab.research.google.com/drive/1gHCjibaI91a50bXjYbemUE7khRdZmyle?usp=sharing&quot;&gt;in a collab&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*ZPCdjBebme6X_8-0.png&quot; alt=&quot;Graphical representation of dilated sampling in stable diffusion Unicorn diffusion art&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;At the #CVPR24 best paper review, I came across an exciting stable diffusion paper.&lt;/p&gt;

&lt;h2 id=&quot;demofusion-unlocking-high-resolution-ai-image-generation-with-dilated-diffusion&quot;&gt;DemoFusion: Unlocking High-Resolution AI Image Generation with Dilated Diffusion&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://ruoyidu.github.io/demofusion/demofusion.html&quot; title=&quot;https://ruoyidu.github.io/demofusion/demofusion.html&quot;&gt;&lt;strong&gt;DemoFusion&lt;/strong&gt; &lt;em&gt;Democratising High-resolution Image Generation without a Sweat&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The goal of this research is to democratize high-resolution image generation while reducing costs. DemoFusion extends Latent Diffusion Models (LDMs) by introducing Progressive Upscaling, Skip Residuals, and Dilated Sampling mechanisms.&lt;/p&gt;

&lt;h3 id=&quot;key-features-of-demofusion&quot;&gt;Key Features of DemoFusion:&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Progressive Upscaling&lt;/strong&gt;: Iteratively increases image resolution using lower-resolution results as a base.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Upsample-Diffuse-Denoise Loop&lt;/strong&gt;: Utilizes noise-inverted representations for guiding higher resolution generation.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Dilated Sampling&lt;/strong&gt;: Enhances global context, resulting in more coherent image generation.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;applications-of-demofusion&quot;&gt;Applications of DemoFusion:&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Generate high-resolution images up to &lt;strong&gt;4096×4096&lt;/strong&gt; using standard hardware like an RTX 3090 GPU.&lt;/li&gt;
  &lt;li&gt;Integrate with &lt;strong&gt;ControlNet&lt;/strong&gt; for additional functionality.&lt;/li&gt;
  &lt;li&gt;Upscale existing images by encoding them into the LDM’s latent space.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Bonus:&lt;/strong&gt; Intermediate results are available during the generation process, enabling rapid iteration and previewing.&lt;/p&gt;

&lt;p&gt;Check out more demos &lt;a href=&quot;https://replicate.com/lucataco/demofusion&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;exploring-the-concept-and-benefits-of-dilated-sampling-in-ai-image-generation&quot;&gt;Exploring the Concept and Benefits of Dilated Sampling in AI Image Generation&lt;/h2&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*-1ylqkgPXbdNxeTMw9MstA.png&quot; alt=&quot;Visual concept of dilated diffusion process, showing pixel grids and sampling gaps&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;h3 id=&quot;concept-of-dilated-sampling&quot;&gt;Concept of Dilated Sampling:&lt;/h3&gt;

&lt;p&gt;Imagine an image as a grid of pixels. Instead of processing each pixel in sequence, &lt;strong&gt;dilated sampling&lt;/strong&gt; selects every second or third pixel, which creates a broader view of the image. This technique enables fewer steps, while providing a broader context for denoising and refining images.&lt;/p&gt;

&lt;h3 id=&quot;purpose-of-dilated-sampling&quot;&gt;Purpose of Dilated Sampling:&lt;/h3&gt;

&lt;p&gt;The goal is to capture global image information instead of focusing on small local details. This method helps establish a global context, leading to more cohesive and coherent image generation.&lt;/p&gt;

&lt;h3 id=&quot;implementation-of-dilated-sampling&quot;&gt;Implementation of Dilated Sampling:&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;A regular pattern is avoided; instead, dilated sampling skips pixels based on a &lt;em&gt;dilation factor&lt;/em&gt;. For example, if the dilation factor is 2, every second pixel is picked.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Shifting and Combining&lt;/strong&gt;: The sampling shifts its starting point in each round to ensure complete image coverage.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;preventing-image-graininess&quot;&gt;Preventing Image Graininess:&lt;/h3&gt;

&lt;p&gt;One drawback of dilated sampling is the potential for graininess, as the sampled pixels are spread apart. To counter this, a &lt;strong&gt;Gaussian filter&lt;/strong&gt; smooths the image before sampling, ensuring the sampled points represent the image more accurately.&lt;/p&gt;

&lt;h3 id=&quot;conclusion-how-dilated-sampling-enhances-ai-image-generation&quot;&gt;Conclusion: How Dilated Sampling Enhances AI Image Generation&lt;/h3&gt;

&lt;p&gt;Think of dilated sampling like stepping back to admire an entire painting before focusing on the details. This technique strikes a balance between global perspective and fine detail, resulting in high-quality images.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;step-by-step-code-implementation&quot;&gt;Step-by-Step Code Implementation&lt;/h2&gt;

&lt;p&gt;For those interested in the technical details, full code is available on GitHub: &lt;a href=&quot;https://github.com/PRIS-CV/DemoFusion/blob/main/pipeline_demofusion_sdxl.py&quot;&gt;DemoFusion GitHub Repository&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;code-step-bystep&quot;&gt;CODE STEP BY STEP&lt;/h3&gt;

&lt;p&gt;Full code can be found in author’s github: &lt;a href=&quot;https://github.com/PRIS-CV/DemoFusion/blob/main/pipeline_demofusion_sdxl.py&quot;&gt;https://github.com/PRIS-CV/DemoFusion/blob/main/pipeline_demofusion_sdxl.py&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;views&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;current_scale_num&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;current_scale_num&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;views_batch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;views&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;view_batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;views&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;view_batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Here, `views` and `views_batch` set up the grid for dilated sampling. `current_scale_num` determines the dilation factor, creating a sparse sampling grid.&lt;/p&gt;

&lt;p&gt;Gather more global information about the image rather than focusing on local details.&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Grid for dilated sampling
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;count_global&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;zeros_like&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;latents_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;value_global&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;zeros_like&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;latents_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;`count_global` and `value_global` are initialized to aggregate global information.&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Loop for picking pixels with gaps
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_view&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;views_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;latents_for_view&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;cat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;latents_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;current_scale_num&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;current_scale_num&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_view&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The loop iterates through `views_batch`, picking pixels with a gap determined by `current_scale_num`.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_view&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;views_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;latents_for_view_gaussian&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;cat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;latents_gaussian&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;current_scale_num&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;current_scale_num&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_view&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;`latents_for_view_gaussian` ensures the global context is gathered, then combined with local details later.&lt;/p&gt;

&lt;p&gt;Shifted dilated sampling means the starting point shifts to cover different parts of the image. The global context is combined with local details to refine the final image.&lt;/p&gt;

&lt;p&gt;Gaussian filter is applied to smooth the image before sampling&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;std_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mean_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;latents_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;latents_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;latents_gaussian&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;gaussian_filter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;latents_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;current_scale_num&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sigma&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigma&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;latents_gaussian&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;latents_gaussian&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;—&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;latents_gaussian&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;latents_gaussian&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mean_&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h3 id=&quot;smart-blending&quot;&gt;&lt;strong&gt;Smart Blending&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;Combining global and local details ensures that the image retains the broader context and finer details.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;latents_view_denoised&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;latents_denoised_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;chunk&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vb_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_view&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
 &lt;span class=&quot;n&quot;&gt;value_global&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;current_scale_num&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;current_scale_num&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;latents_view_denoised&lt;/span&gt;
 &lt;span class=&quot;n&quot;&gt;count_global&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;current_scale_num&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;current_scale_num&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Here, denoised views (`latents_view_denoised`) are added to `value_global`, blending the global and local contexts.&lt;/p&gt;

&lt;p&gt;The final latent representation is formed by blending global and local contexts.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;value_global&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value_global&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h_pad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w_pad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value_global&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c2&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;count&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;ones_like&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value_global&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c2&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;latents&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;where&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;count&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The global values are combined with local values (`value += value_global * c2`) and normalized (`latents = torch.where(count &amp;gt; 0, value / count, value)`).&lt;/p&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;CONCLUSION &lt;/h3&gt;

&lt;p&gt;This code implements dilated sampling by creating a grid with gaps (dilation), applying a Gaussian filter to smooth out graininess, gathering global context, and then blending it with local details to form the final denoised image. This process ensures a balance between capturing the big picture and refining the details.&lt;/p&gt;

&lt;p&gt;Try out a simple demo illustrating the concept of dilated sampling&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://colab.research.google.com/drive/1gHCjibaI91a50bXjYbemUE7khRdZmyle?usp=sharing&quot;&gt;Collab demo&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The demo is a simple illustration of the dilated sampling concept using simulated data. The visualizations help in understanding how dilated sampling and smoothing work together.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Original Image:
 — A simple checkboard patten image is created for demonstration.&lt;/li&gt;
  &lt;li&gt;Smoothed Image (Gaussian Filter):
 — The original image is smoothed using a Gaussian filter to reduce graininess.&lt;/li&gt;
  &lt;li&gt;Dilated Sampling after Smoothing:
 — Dilated sampling is applied to the smoothed image, resulting in a more coherent global context.&lt;/li&gt;
  &lt;li&gt;Dilated Sampling:
 — Pixels are sampled with a gap (dilation factor).&lt;/li&gt;
&lt;/ol&gt;

&lt;footer&gt;
  &lt;p&gt;Exported from &lt;a href=&quot;https://medium.com&quot;&gt;Medium&lt;/a&gt; in June 2024.&lt;/p&gt;
  &lt;p&gt;&lt;a href=&quot;https://medium.com/@ulalaparis/dilated-difusion-concept-from-demofusion-e32a7b5d09d6&quot;&gt;View
      the original&lt;/a&gt;&lt;/p&gt;
&lt;/footer&gt;
&lt;script type=&quot;text/javascript&quot; src=&quot;//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-584ec4ce89deed84&quot;&gt;&lt;/script&gt;

&lt;div class=&quot;addthis_inline_share_toolbox&quot;&gt;&lt;/div&gt;

</description>
        <pubDate>Wed, 19 Jun 2024 00:00:00 +0200</pubDate>
        <link>http://urszulaczerwinska.github.io/thoughts/dilated-difusion-concept-from-demofusion/</link>
        <guid isPermaLink="true">http://urszulaczerwinska.github.io/thoughts/dilated-difusion-concept-from-demofusion/</guid>
        
        
        <category>thoughts</category>
        
      </item>
    
      <item>
        <title>Embracing the Unknown 2/2</title>
        <description>&lt;head&gt;
  &lt;meta name=&quot;description&quot; content=&quot;Explore key takeaways from the AI Startup School seminars, including lessons on entrepreneurship, risk management, and the future of AI from industry leaders.&quot; /&gt;
&lt;/head&gt;

&lt;p&gt;I &lt;strong&gt;was granted exclusive online access to the AI Startup School seminars. Here are some awesome and very transforming things I learned from the top startup and tech speakers.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In order to read about my application to AI Startup School and my thoughts on entrepreneurship and risk, check out part I:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/thoughts/embracing-the-unknown-1-2-applying-to-ai-startup-school-reflections&quot;&gt;Embracing the Unknown 1/2: Applying to AI Startup School — Reflections&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*mnx_Q7FmNfRTzb3W.jpeg&quot; alt=&quot;A diverse group of people engaging in a seminar setting, representing the AI Startup School experience.&quot; title=&quot;Participants at AI Startup School seminar&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;As I navigated my own journey, exploring my relationship with risk and entrepreneurship, I followed the AI Startup School seminar series each week. This transition—from introspective exploration to external insights—shifted both my setting and perspective. I discovered that my personal relationship with risk and uncertainty echoed the stories and lessons shared by the speaking entrepreneurs and innovators. The difference being they took the step into the unknown, armed with resilience and a thirst for knowledge.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*chNP4occqmr87cEa&quot; alt=&quot;A speaker addressing an audience during the AI Startup School seminar series.&quot; title=&quot;Speaker at AI Startup School&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;I am not allowed to share the exact content of the seminar series, but I would like to convey the messages and thoughts that impacted me the most over the 9 lectures of this special EF talks.&lt;/p&gt;

&lt;p&gt;All lectures took the form of interviews with the speaker(s). The speakers were a unique blend of famous figures in the AI startup scene, young entrepreneurs, and VC investors.&lt;/p&gt;

&lt;h3 id=&quot;key-takeaways-from-the-ai-startup-school-seminars&quot;&gt;&lt;strong&gt;Key Takeaways from the AI Startup School Seminars&lt;/strong&gt;&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;“Being in too early is the same as being wrong.”
— &lt;a href=&quot;https://www.linkedin.com/in/eisokant/&quot;&gt;Eiso Kant&lt;/a&gt;, &lt;a href=&quot;https://www.poolside.ai/&quot;&gt;Poolside&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*Dgt-Daih3LEYuGaz&quot; alt=&quot;A thoughtful entrepreneur reflecting on timing and market fit during the seminar.&quot; title=&quot;Eiso Kant sharing insights on timing and market fit&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;If the product you develop isn’t tailored to the public, or if the technology you invest in is too immature to add value, your venture is unlikely to succeed. &lt;a href=&quot;https://www.linkedin.com/in/eisokant/&quot;&gt;Eiso Kant&lt;/a&gt;, CTO and co-founder of &lt;a href=&quot;https://www.poolside.ai/&quot;&gt;Poolside&lt;/a&gt;, shared insights from his experience in founding a startup focused on developing cognitive abilities through neural networks in 2016. Similarly, &lt;a href=&quot;https://www.linkedin.com/in/arthur-mensch/?locale=fr_FR&quot;&gt;Arthur Mensch&lt;/a&gt; from &lt;a href=&quot;https://mistral.ai/fr/&quot;&gt;Mistral AI&lt;/a&gt; highlighted the importance of capitalizing on opportunities at the opportune moment—neither too early nor too late. If the technology isn’t sufficiently mature, the business might struggle. It’s possible to choose the right technology but at an inopportune moment.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*Br-KuJWcK2glhmaz&quot; alt=&quot;A speaker from the seminar discussing the balance between effort and impact.&quot; title=&quot;Matt Clifford discussing the correlation between effort and impact&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“The faster you acknowledge the lack of correlation between effort and impact, the faster it stops you complexes.”
“Life is unfair, get over it.”
— &lt;a href=&quot;https://www.linkedin.com/in/mattcliffordef/?originalSubdomain=uk&quot;&gt;Matt Clifford&lt;/a&gt;, co-founder of &lt;a href=&quot;https://www.joinef.com/&quot;&gt;Entrepreneur First&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Matt Clifford, co-founder of Entrepreneur First, shared a wealth of knowledge about AI business models and the entrepreneurial spirit, as well as life in general. Having witnessed the rise and fall of many startups and invested in numerous ventures, he has gathered a trove of stories and insights. One of the most impactful statements was about acceptance—the acceptance of the fact that we don’t always get back what we give. Working tirelessly on a project does not guarantee success. Sometimes, factors like luck or intuition play a decisive role in a company’s fate.&lt;/p&gt;

&lt;h3 id=&quot;the-role-of-open-source-in-ai&quot;&gt;&lt;strong&gt;The Role of Open Source in AI&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;The figures in the AI field, such as &lt;a href=&quot;https://twitter.com/emostaque?lang=en&quot;&gt;Emad Mostaque&lt;/a&gt; of &lt;a href=&quot;https://stability.ai/&quot;&gt;Stability AI&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/arthur-mensch/?locale=fr_FR&quot;&gt;Arthur Mensch&lt;/a&gt; of &lt;a href=&quot;https://mistral.ai/fr/&quot;&gt;Mistral AI&lt;/a&gt;, and &lt;a href=&quot;https://www.linkedin.com/in/karim-beguir-2350161/?originalSubdomain=uk&quot;&gt;Karim Beguir&lt;/a&gt; of &lt;a href=&quot;https://www.instadeep.com/&quot;&gt;Instadeep&lt;/a&gt;, are strong supporters of open source. They see significant business opportunities in building around open-source models, with community engagement and customer acquisition being strong arguments in favor of this approach. Arthur Mensch believes in enhancing open-source business with exceptional customer service and thriving through partnerships. Karim Beguir finds great reward in community contributions to their work, which strengthens the product and unlocks business potential in scaling open source to meet client needs.&lt;/p&gt;

&lt;p&gt;Stability AI’s business model involves providing access to advanced generative AI models through a subscription membership. This membership includes all available models, similar to Amazon Prime. This approach aims to make generative AI models predictable and easy to use. Additionally, the company offers consulting services for top organizations in need of expertise in generative AI. The goal is to revolutionize the industry by providing model base and support while adapting to the market’s growth.&lt;/p&gt;

&lt;p&gt;Leading players in the AI space have varying perspectives on open vs. closed models. OpenAI initially started with open models but has become cautious due to concerns about misuse. Google is open in some areas but avoids open sourcing models. Meta embraces openness, especially in language models. Microsoft supports open AI but has a mixed approach. Amazon focuses more on infrastructure. Apple is very secretive about anything they make, with some rare exceptions. Smaller labs and Japanese companies are aggressively pursuing open models.&lt;/p&gt;

&lt;p&gt;The balance between open and closed AI models is shifting, with more players recognizing the benefits of openness.&lt;/p&gt;

&lt;h3 id=&quot;the-future-of-ai&quot;&gt;&lt;strong&gt;The Future of AI&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;Many speakers emphasized the shift towards ‘quality over quantity’ as an emerging trend. This could take the form of providing the best data to achieve better model performance, or a shift towards specialized models and swarms of models.&lt;/p&gt;

&lt;p&gt;The importance of developing robust evaluation frameworks and corrective mechanisms was also a common theme. The significance of exploring multimodality and enhancing model inference capabilities beyond current paradigms should not be neglected.&lt;/p&gt;

&lt;h3 id=&quot;insights-on-the-entrepreneurial-journey&quot;&gt;&lt;strong&gt;Insights on the Entrepreneurial Journey&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;The speakers extensively discussed their journeys with business ideas and the people they collaborated with. A speaker highlighted that a ‘&lt;strong&gt;moment of audacity&lt;/strong&gt;’ is essential for taking the leap into entrepreneurship.&lt;/p&gt;

&lt;p&gt;Another important step is &lt;strong&gt;finding a co-founder&lt;/strong&gt;. This person should be both your best friend and your challenger, offering support and constructive disagreement. It’s commonly recommended to seek a business partner with complementary skills. However, a few speakers mentioned they did not follow this advice, and it still worked out for them as they are complementary on a different level, even though they have similar hard skills.&lt;/p&gt;

&lt;p&gt;To build a successful business, the right idea involves &lt;strong&gt;identifying a niche&lt;/strong&gt;, such as industry-specific issues, novel technologies, or regulatory gaps. For instance, Mistral AI identified a gap in the AI startup landscape and aimed to expedite progress in foundational models.&lt;/p&gt;

&lt;p&gt;They also advised against becoming too lazy in a stable job. &lt;strong&gt;The quicker you immerse yourself in entrepreneurship, the better.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Another vital skill mentioned was the need to rapidly obtain feedback. The speaker encouraged aspiring entrepreneurs to begin creating, even if the idea isn’t perfect, and to engage with potential customers for feedback. Prioritizing swift iteration over waiting for the perfect final product is crucial. An environment that facilitates rapid learning and exposure is key.&lt;/p&gt;

&lt;p&gt;Additionally, the speakers underscored the importance of embracing failure and perseverance throughout the entrepreneurial journey. Cultivate and pursue your ambition, and think beyond traditional boundaries.&lt;/p&gt;

&lt;p&gt;To elevate the culture of entrepreneurship, Matt Clifford recommended carefully studying successful individuals. Understand their decisions and learn from their mistakes. He argued that reading books is more beneficial than spending time on platforms like Twitter and watching short videos.&lt;/p&gt;

&lt;p&gt;Lastly, investors can act as true partners and guides on the entrepreneurial path, with strategic partnerships and foundational funding serving as catalysts for change.&lt;/p&gt;

&lt;h3 id=&quot;the-next-wave-of-startups-what-to-expect&quot;&gt;&lt;strong&gt;The Next Wave of Startups: What to Expect&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;Currently, the media sector holds great potential for impactful innovation. Entrepreneurs are encouraged to tackle problems that deeply concern them and focus on regulated industries where they can make significant contributions. Within the AI ecosystem, opportunities for value creation span from specialized&lt;/p&gt;

&lt;footer&gt;
  &lt;p&gt;Exported from &lt;a href=&quot;https://medium.com&quot;&gt;Medium&lt;/a&gt; on December 19
    2023.&lt;/p&gt;
  &lt;p&gt;&lt;a href=&quot;https://medium.com/@ulalaparis/embracing-the-unknown-2-2-takeaways-from-ai-startup-school-seminars-9d6bfe7cf4ab&quot;&gt;View
      the original&lt;/a&gt;&lt;/p&gt;
&lt;/footer&gt;
&lt;script type=&quot;text/javascript&quot; src=&quot;//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-584ec4ce89deed84&quot;&gt;&lt;/script&gt;

&lt;div class=&quot;addthis_inline_share_toolbox&quot;&gt;&lt;/div&gt;

</description>
        <pubDate>Thu, 25 Apr 2024 00:00:00 +0200</pubDate>
        <link>http://urszulaczerwinska.github.io/thoughts/embracing-the-unknown-2-2-applying-to-ai-startup-school-reflections</link>
        <guid isPermaLink="true">http://urszulaczerwinska.github.io/thoughts/embracing-the-unknown-2-2-applying-to-ai-startup-school-reflections</guid>
        
        
        <category>thoughts</category>
        
      </item>
    
      <item>
        <title>Embracing the Unknown 1/2</title>
        <description>&lt;head&gt;
  &lt;meta name=&quot;description&quot; content=&quot;A personal reflection on the challenges and insights gained from applying to AI Startup School, exploring the themes of risk-taking, comfort zones, and the entrepreneurial spirit.&quot; /&gt;
&lt;/head&gt;

&lt;p&gt;I &lt;strong&gt;have recently applied for the AI startup school organized by Entrepreneur First.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*kZpUj5gHFo82d49h&quot; alt=&quot;Beautiful sky with coloured clouds.&quot; title=&quot;AI Startup School Application Journey&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;With the rapid pace of AI transformations and new challenges brought by Generative AI, I feel like I am always behind and never have enough time to catch up. I am motivated to learn more, accelerate, and be part of the innovation movement. I aim to contribute more to “INtrapreneurship” in my company’s Computer Vision team and be able to drive change.&lt;/p&gt;

&lt;p&gt;The “Entrepreneurs First” program in Paris is an initiative aimed at fostering entrepreneurial talent and facilitating the creation of new startups, particularly in the deep tech sector. EF invests in talented individuals based on their skills or expertise, regardless of whether they already have a business idea or team in place. Entrepreneur First’s Paris program conveys a belief that some of the best potential startup founders never embark on entrepreneurial journeys due to barriers to entry.&lt;/p&gt;

&lt;p&gt;They organized the AI Startup School, a series of evening seminars, and a great networking opportunity.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“An exclusive lecture series in Paris delivered by renowned speakers in the AI startup scene (…) EF’s AI Startup School in Paris will bring together talented individuals excited by the opportunities within AI, to share knowledge and build a wider network within the local AI ecosystem.”
 — &lt;a href=&quot;https://www.joinef.com/ai-startup-school/&quot;&gt;Entrepreneur First AI Startup School&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&quot;https://www.actuia.com/actualite/entrepreneur-first-annonce-le-lancement-dun-nouveau-programme-a-paris-et-poursuit-son-expansion-internationale/&quot; title=&quot;https://www.actuia.com/actualite/entrepreneur-first-annonce-le-lancement-dun-nouveau-programme-a-paris-et-poursuit-son-expansion-internationale/&quot;&gt;&lt;strong&gt;Entrepreneur First annonce le lancement d’un nouveau programme à Paris et poursuit son expansion…&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;SCOOP: I was not accepted.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;If you want to read directly about AI Startup School seminars, skip to &lt;a href=&quot;/thoughts/embracing-the-unknown-2-2-applying-to-ai-startup-school-reflections&quot;&gt;Embracing the Unknown 2/2: Takeaways from AI Startup School — Seminars&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Of course, I cannot know the specific reasons; however, there was one question I reckon I did not nail:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“What was the most risky thing you have ever done?”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;risk&quot;&gt;Risk?&lt;/h3&gt;

&lt;p&gt;During the interview, my mind raced with seemingly trivial or unimpressive ideas such as: “I tested my code in prod” (lol, never do that), “I drive a motorcycle” or “I changed jobs on my own initiative”.&lt;/p&gt;

&lt;p&gt;Some time later, &lt;strong&gt;&lt;em&gt;I started to think about my relationship with risk.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Growing up in Poland, I was ingrained with a cautious ethos from my parents: a conviction that regular life can surprise us with enough troubles that deliberately taking risks is pure stupidity and equals a miserable outcome.&lt;/p&gt;

&lt;p&gt;We have this saying: “do not praise the day before the sunset”, “do not say ‘hop’ before you jump”, “do not share the skin of the bear before you’ve killed the beast”, “enjoy today, tomorrow will be worse”. I was told I should not be too (or at all) optimistic and always think about the worst-case scenario.&lt;/p&gt;

&lt;p&gt;For most of my life, this cautious approach underpinned every decision I made, be it choosing a high school or committing to a relationship. Always a Plan A, followed by Plan B and C, etc. Living life being scared and putting my energy into foreseeing what can go wrong and which backup plan I should put in place.&lt;/p&gt;

&lt;p&gt;Some (including myself) might argue I’ve taken risks: moving solo from Poland to a tiny, unknown, cute town in French Brittany for bio-mathematics studies. But to be honest, I had worked out a solid Plan B; I was subscribed to a university in Poland, starting a month later than the one in France. It would only cost me a plane ticket to go back and live my life like nothing ever happened (I stayed in France, after all). Would I have chosen the adventure if there were no way back?&lt;/p&gt;

&lt;p&gt;I suppose all these plans and being “realistic” (or one could say rather “pessimistic”) about my life and career steps helped me to survive and saved me from trouble many times. But, also, maybe, stopped me from getting somewhere further or somewhere else.&lt;/p&gt;

&lt;p&gt;It’s not that I lack ambition or curiosity. I have always had tons of it. Being the best in the class, being the most performant, getting recognition for what I do, giving my time and passion to projects, trying different sports: kite surfing, skiing, biking, boxing. From some perspective now, I just realize I have never taken a real risk, going into a total unknown with faith.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;To me, entrepreneurship embodies this very essence of risk-taking: a belief so strong it borders on the edge of madness.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Once, with two colleagues, we had a start-up idea. We pitched it to founders, attended startup events, and when we faced the question of commitment, all of us chose a more secure career choice, granting a stable income.&lt;/p&gt;

&lt;p&gt;I convinced myself that those who embark on this entrepreneurial journey are invariably privileged. They are usually white men from good families, supporting them with money or a professional network so that they do not have to stress about what to eat, and therefore they can live most of their entrepreneurial adventure. Even if this is true for some, I think now, I was telling myself a story I wanted to hear.&lt;/p&gt;

&lt;p&gt;Let’s think of the controversial protagonist of “WeCrashed” Netflix series, Adam Neumann, who tells a tale of ambition, innovation, and ultimate downfall. He managed, with an idea and determination, to reach a valuation of $47 billion at its peak. Also, think about Gordon Ramsay, the multi-Michelin starred chef and star of the small screen. In his interview at Masterclass.com, he shared that in order to build his business he left his comfortable position and went to France to learn everything “from scratch”, even though he worked for a starred restaurant in the UK. Later, he also sold his house in order to put all the money in his new restaurant, working like a madman.&lt;/p&gt;

&lt;p&gt;All those people who apply to EF to learn, grow, and thrive, even if they don’t always have a comfortable backup plan, have earned my respect and admiration. They are getting out of their comfort zone because they believe there is something worth fighting for, whatever the motivation is — be it praise, money, or saving the world and curing cancer.&lt;/p&gt;

&lt;h3 id=&quot;comfort&quot;&gt;Comfort?&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Another important keyword: comfort&lt;/strong&gt;. Nowadays, in Western Europe, in France, I think, we are really used to our comfort. Comfort is a heated house, a warm meal but also a stable job, health insurance, a partner to talk to and government aids. I realize that I like my comfort too much, that I am privileged enough, but not ready to face the discomfort of the unknown.&lt;/p&gt;

&lt;p&gt;So the one remarkable question emerges: &lt;strong&gt;is there a cause, a dream, something I value so profoundly that I’m willing to step beyond my comfort zone and embrace the unknown?&lt;/strong&gt; Accept that there is no Plan B, that there is Plan A or nothing? Will I allow myself to dream?&lt;/p&gt;

&lt;p&gt;In these reflections, there is a deeper quest to understand what drives me, what scares me, and what it truly means to step into the unknown.&lt;/p&gt;

&lt;h3 id=&quot;ai-startup-school-seminars&quot;&gt;AI Startup School Seminars&lt;/h3&gt;

&lt;p&gt;As I mentioned before, I was not accepted into the AI Startup School program. However, I am grateful that &lt;strong&gt;I was granted exclusive online access to the seminars.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Read about my learnings in the next post:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/thoughts/embracing-the-unknown-2-2-applying-to-ai-startup-school-reflections&quot;&gt;Embracing the Unknown 2/2: Takeaways from AI Startup School — Seminars&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The series of nine interviews with the speaker(s) representing a unique blend of famous figures of AI startup scene, young entrepreneurs and VC investors.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Maybe the learning will change your life? At least, I hope, you will have an enjoyable read if you are interested in entrepreneurship, AI, investing, or all of it. Let’s dive into the AI startup world together.&lt;/strong&gt;&lt;/p&gt;

&lt;footer&gt;
  &lt;p&gt;Exported from &lt;a href=&quot;https://medium.com&quot;&gt;Medium&lt;/a&gt; on April 2024.&lt;/p&gt;
  &lt;p&gt;&lt;a href=&quot;https://medium.com/@ulalaparis/embracing-the-unknown-1-2-applying-to-ai-startup-school-reflections-9364c4814d0c&quot;&gt;View
      the original&lt;/a&gt;&lt;/p&gt;
&lt;/footer&gt;
&lt;script type=&quot;text/javascript&quot; src=&quot;//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-584ec4ce89deed84&quot;&gt;&lt;/script&gt;

&lt;div class=&quot;addthis_inline_share_toolbox&quot;&gt;&lt;/div&gt;

</description>
        <pubDate>Sat, 13 Apr 2024 00:00:00 +0200</pubDate>
        <link>http://urszulaczerwinska.github.io/thoughts/embracing-the-unknown-1-2-applying-to-ai-startup-school-reflections</link>
        <guid isPermaLink="true">http://urszulaczerwinska.github.io/thoughts/embracing-the-unknown-1-2-applying-to-ai-startup-school-reflections</guid>
        
        
        <category>thoughts</category>
        
      </item>
    
      <item>
        <title>The Mamba Effect</title>
        <description>&lt;head&gt;
  &lt;meta name=&quot;description&quot; content=&quot;Unlock the potential of AI with our comprehensive guide on ML Mamba models. Discover how these advanced machine learning frameworks are revolutionizing data analysis, predictive analytics, and automated decision-making. Learn about their key features, benefits, and applications across various industries. Enhance your understanding and leverage ML Mamba models to stay ahead in the rapidly evolving world of artificial intelligence.&quot; /&gt;
&lt;/head&gt;

&lt;p&gt;Already 8 papers since December 2023 !&lt;/p&gt;

&lt;p&gt;Mamba is already a new wave starting as a replacement for vanilla transformer, it has already been adapted to text, vison, video…&lt;/p&gt;

&lt;p&gt;Mamba models represent a significant breakthrough in neural network architecture.&lt;/p&gt;

&lt;p&gt;Among published papers, besides the original Mamba paper, I would like to distinguish two spin-offs: VMamba and MambaBytes. There it also a bunch of papers with specific biomedical applications that I am not expert to evaluate impact.&lt;/p&gt;

&lt;p&gt;Here I compiled a short overview of all (?) those papers adapting the template from &lt;a href=&quot;https://www.deeplearning.ai/the-batch/&quot;&gt;The Batch newsletter from deeplearning.ai&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;here is a list of mamba papers : &lt;a href=&quot;https://github.com/yyyujintang/Awesome-Mamba-Papers&quot;&gt;https://github.com/yyyujintang/Awesome-Mamba-Papers&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;arxiv-231201-mamba-linear-time-sequence-modeling-with-selective-state-spaces&quot;&gt;Arxiv 23.12.01: Mamba: Linear-Time Sequence Modeling with Selective State Spaces&lt;/h2&gt;

&lt;p&gt;#mamba&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2312.00752&quot;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/state-spaces/mamba&quot;&gt;Code&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This paper is the first to introduce the Mamba architecture. Mamba offers faster inference, linear scaling with sequence length, and strong performance.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*aPbcg2rPGh68SViRpTi21Q.png&quot; alt=&quot;Deep learing mamaba model schema&quot; /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*bAj0TMSgZetUghDij6U-zA.png&quot; alt=&quot;Mamba model illustration of architecture&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What’s New&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A new architecture, Mamba, integrating SSMs without relying on attention or MLP blocks.&lt;/li&gt;
  &lt;li&gt;Implementation of a hardware-aware parallel algorithm for efficient computation.&lt;/li&gt;
  &lt;li&gt;Long context: the quality and efficiency together yield performance improvements on real data up to sequence length 1M&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;How it Works:&lt;/strong&gt; Mamba adds recurrent and convolutional models, to &lt;strong&gt;a unique selection mechanism&lt;/strong&gt; that enables the model to prioritize or ignore inputs based on the content relevance. This approach allows for &lt;strong&gt;linear scalability&lt;/strong&gt; in sequence length. Mamba integrates selective SSMs into a simplified neural network architecture with gates. They are structured to enable the model to selectively propagate or to forget information based on the current token.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Results:&lt;/strong&gt; Mamba demonstrates state-of-the-art performance across various modalities. In language modeling, &lt;strong&gt;Mamba-3B outperforms Transformers&lt;/strong&gt; of the same size, matches Transformers twice its size in both pretraining and downstream evaluation. In terms of efficiency, it achieves &lt;strong&gt;5× higher throughput than Transformers&lt;/strong&gt; and scales linearly with sequence length.
This in various domains such as language, genomics, audio modeling. It efficiently handles &lt;strong&gt;sequences up to a million lengths&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Behind the News:&lt;/strong&gt; The development of Mamba marks a significant &lt;strong&gt;shift from the dominant Transformer-based architectures&lt;/strong&gt;. It opens new avenues in sequence modeling, especially for applications requiring efficient processing of long data sequences. Authors mentions their ambition to make Mamba alternative to Tranformers and CNN as a &lt;a href=&quot;https://medium.com/adevinta-tech-blog/foundation-models-a-new-vision-for-e-commerce-76904a3066e8&quot;&gt;Foundation Model&lt;/a&gt; backbone.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Why it Matters:&lt;/strong&gt; Mamba has the potential to revolutionize various applications in deep learning. This first paper already resulted in 7 other papers in the same month as code is under apache licene and public.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;We’re Thinking:&lt;/strong&gt; We are missing distance to see how Mamba module will perform in practice. However the fact that many publicaitons already applied Mamba, modified it and obtained pulishable resutls is promising.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;In Detail:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Selective SSMs&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/2560/1*j4g2N5BtJUvjEsXqfVPmJg.png&quot; alt=&quot;&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;The key contribution of the paper lies in the novel implementation of &lt;strong&gt;Selective&lt;/strong&gt; State Space Models. It leverages parameters that control if the model response to current inputs or it maintains its existing state. For instance, a parameter ∆ (Delta) in the model’s architecture determines the balance between focusing on the current input (larger ∆ values) and preserving the ongoing state (smaller ∆ values). The selective modulation of parameters B and C tunes how the inputs influence the state and, conversely, how the state influences the outputs. The selective approach also manages context and resets boundaries in scenarios where sequences are concatenated. This prevents the unwanted bleed of information between concatenated sequences.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Other ressources:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/ai-insights-cobet/building-mamba-from-scratch-a-comprehensive-code-walkthrough-5db040c28049&quot;&gt;building-mamba-from-scratch-a-comprehensive-code-walkthrough&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/@joeajiteshvarun/mamba-revolutionizing-sequence-modeling-with-selective-state-spaces-8a691319b34b&quot;&gt;mamba-revolutionizing-sequence-modeling-with-selective-state-spaces&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/@athekunal/mamba-and-state-space-models-explained-b1bf3cb3bb77&quot;&gt;mamba-and-state-space-models-explained&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;arxiv-240108-moe-mamba-efficient-selective-state-space-models-with-mixture-of-experts&quot;&gt;Arxiv 24.01.08: &lt;strong&gt;MoE-Mamba: Efficient Selective State Space Models with Mixture of Experts&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;#LLM&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2401.04081&quot;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The paper introduces a novel model in the field of sequential modeling.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*wCH-77Q_ThqTubu4H-Sl9w.png&quot; alt=&quot;MoE-Mamba: Efficient Selective State Space Models with Mixture of Experts&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Introduction:&lt;/strong&gt; The paper presents MoE-Mamba, a model that integrates State Space Models (SSMs) with Mixture of Experts (MoE) to enhance sequential modeling. This combination aims to leverage the strengths of both SSMs, known for their efficient performance, and MoE, a technique for scaling up models efficiently.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What’s New:&lt;/strong&gt; Integration of SSMs and MoE. MoE layers are used by Mistral, one of SOTA LLM models.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How it Works:&lt;/strong&gt; The MoE-Mamba architecture replaces every other Mamba layer with a MoE feed-forward layer.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;
- MoE-Mamba achieves better performance than both Mamba and Transformer-MoE models.
- It reaches the same performance as Mamba in significantly fewer training steps.
- The model scales well with the number of experts, with optimal results at 32 experts.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Behind the News:&lt;/strong&gt; The scalability potential of MoE-Mamba is remarkable. Although the current study focuses on smaller models, the architecture suggests a promising avenue for handling larger, more complex models efficiently.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Why it Matters:&lt;/strong&gt; Given the efficiency in training and inference, MoE-Mamba shows promise for deployment in large-scale language models.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;We’re Thinking:&lt;/strong&gt; It is also not clear what are the scalability limits of MoE-Mamba, especially in comparison to existing large-scale models like GPT-3? Also, we would like to have a look a the code…&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;In Detail&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Integration of MoE layers into the Mamba architecture&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This design choice enables MoE-Mamba to leverage the conditional processing capabilities of MoE and the context integration of Mamba. By alternating between unconditional processing by the Mamba layer and conditional processing by a MoE layer, MoE-Mamba achieves a balance between efficient state compression and selective information retention.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*hsiJ04bKp4QspHpyFmrApw.png&quot; alt=&quot;Integration of MoE layers into the Mamba architecture&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Authors also investigate a unified Mamba module containg MoE&lt;/p&gt;

&lt;h2 id=&quot;arxiv-240109-u-mamba-enhancing-long-range-dependency-for-biomedical-image-segmentation&quot;&gt;Arxiv 24.01.09: &lt;strong&gt;U-Mamba: Enhancing Long-range Dependency for Biomedical Image Segmentation&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2401.04722&quot;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/bowang-lab/U-Mamba&quot;&gt;Code&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;#biomedical #segmentation #cv&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*RM4DAO5rm5n7kE79iWtw_w.png&quot; alt=&quot;U-Mamba: Enhancing Long-range Dependency for Biomedical Image Segmentation&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;This paper presents an innovative network architecture for biomedical image segmentation.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What’s new:&lt;/strong&gt; U-Mamba is a novel network integrating Mamba blocks, into a U-Net based architecture. This hybrid CNN-SSM structure enables modeling of long-range dependencies in images.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How it works:&lt;/strong&gt; U-Mamba follows an encoder-decoder structure, where each building block comprises Residual blocks followed by a Mamba block. This design captures both local features and long-range dependencies. The network’s self-configuring mechanism allows it to adapt automatically to various datasets.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Results:&lt;/strong&gt; The experiments across biomedical segmentation tasks show that U-Mamba outperforms the state-of-the-art CNN/Transformer-based networks in terms of segmentation accuracy by a small margin.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Behind the news:&lt;/strong&gt; The integration of Mamba blocks within a U-Net architecture represents an interesting integration of two architectures, highlighting the potential of State Space Models in this domain.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Why it matters:&lt;/strong&gt; Yet one more architecture smoothly integrating Mamba with good results.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;We’re thinking:&lt;/strong&gt; The U-Mamba network’s adaptability and performance set a new benchmark in medical image segmentation. This work might stimulate further exploration of hybrid architectures in medical image analysis.&lt;/p&gt;

&lt;p&gt;While U-Mamba shows some advantages over existing methods, the improvement margin in some cases appears modest. For example, in 3D organ segmentation, U-Mamba’s DSC scores are marginally higher than nnU-Net, which is one of the closest competitors.&lt;/p&gt;

&lt;p&gt;Authors do not support with numbers efficiency of the model, model size and comparison in that matter of the two architecture variants.&lt;/p&gt;

&lt;p&gt;Should we wait for U-Mamba perfomance for standart segmentation benchmarks such as &lt;a href=&quot;https://paperswithcode.com/sota/semantic-segmentation-on-ade20k&quot;&gt;ADE20K&lt;/a&gt; or PASCAL to decide U-Mamba generic value?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;In Detail&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;U-Mamba Architecture&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The U-Mamba architecture follows the encoder-decoder pattern of U-Net, known for its effectiveness in medical imaging.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Building Blocks:&lt;/strong&gt; Each block contains two successive Residual blocks followed by a Mamba block. The Residual block includes a plain convolutional layer, Instance Normalization, and Leaky ReLU.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Mamba Block:&lt;/strong&gt; It processes image features that are flattened and transposed, followed by Layer Normalization. The Mamba block has two parallel branches:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;The first branch&lt;/strong&gt; expands the features and processes them through a linear layer, a 1D convolutional layer, SiLU activation, and the SSM layer.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;The second branch&lt;/strong&gt; also expands the features, followed by SiLU activation. Features from both branches are then merged using the Hadamard product and projected back to their original shape.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Encoder and Decoder:&lt;/strong&gt; The U-Mamba encoder, captures both local features and long-range dependencies. The decoder focuses on local information and resolution recovery, using Residual blocks, transposed convolutions, and inherits skip connections from U-Net. The output is passed through a convolutional layer and a Softmax layer for the final segmentation probability map.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Variants:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;U-Mamba_Bot: Uses the U-Mamba block only in the bottleneck.&lt;/li&gt;
  &lt;li&gt;U-Mamba_Enc: Employs the U-Mamba block in all encoder blocks.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;arxiv-240117-vision-mamba-efficient-visual-representation-learning-with-bidirectional-state-space-model&quot;&gt;Arxiv 24.01.17: &lt;strong&gt;Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2401.09417``&quot;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/hustvl/Vim&quot;&gt;Code&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;#computervision #classification #segmentation&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*D6KVLVs8MJYnd_DBCDUKuw.png&quot; alt=&quot;Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Vim utilizes bidirectional State Space Models (SSMs) to process image sequences.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What’s New&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Vim proposes a pure-SSM-based method for vision tasks, differing from self-attention-based models.&lt;/li&gt;
  &lt;li&gt;Incorporation of &lt;strong&gt;bidirectional SSMs&lt;/strong&gt; for efficient visual data processing.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;How It Works&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Vim transforms images into sequences of flattened 2-D patches, applying bidirectional SSM.&lt;/li&gt;
  &lt;li&gt;The system uses a combination of position embeddings and bidirectional state space models for visual data processing.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;On tasks like ImageNet classification Vim outperforms established models like DeiT and ViTs in terms of accuracy with smaller size.&lt;/li&gt;
  &lt;li&gt;Authors compare also segmentation UperNet framework with Vim, DeiT and ResNet on ADE20k with similar conclusion.&lt;/li&gt;
  &lt;li&gt;For object detection benchmark Vim slighly outperforms DeiT in the scope of Cascade Mask R-CNN on COCO.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Behind the News:&lt;/strong&gt; Vim challenges the dominance of self-attention in visual representation, offering an alternative that’s more efficient in handling large-scale and high-resolution datasets.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Why It Matters:&lt;/strong&gt; Vim’s efficiency in processing high-resolution images makes it a promising backbone for future vision foundation models.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;We’re Thinking:&lt;/strong&gt; Will Vim show its efficiency/accuracy across other frameworks than UperNet or Cascade Mask R-CNN ?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;In Detail&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Bidirectional State Space Model&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;It processes the visual data both forward and backward, unlike traditional unidirectional models.&lt;/li&gt;
  &lt;li&gt;This bidirectionality allows for more robust capturing of visual contexts and dependencies, particularly in dense prediction tasks.&lt;/li&gt;
  &lt;li&gt;The model effectively compresses the visual representation, leveraging position embeddings to maintain spatial awareness.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;arxiv-240118-vmamba-visual-state-space-model&quot;&gt;Arxiv 24.01.18 : &lt;strong&gt;VMamba: Visual State Space Model&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2401.10166&quot;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/MzeroMiko/VMamba&quot;&gt;Code&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;#computervision #classification #segmentation #detection&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;VMamba&lt;/strong&gt; presents new approach in visual representation learning. It merges CNNs’ and ViTs’ strengths and does not have their limitations.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What’s new:&lt;/strong&gt; VMamba has a unique architecture: integrates global receptive fields and dynamic weights within a linear computational complexity framework.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How it works:&lt;/strong&gt; The model is based on a &lt;strong&gt;Cross-Scan Module (CSM)&lt;/strong&gt;. This module processes visual data in &lt;strong&gt;four directions&lt;/strong&gt;. This ensures global information integration without increased complexity. The 2D Selective Scan combines CMS with S6 mamba block and merge output features creating 2D feature map.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Results:&lt;/strong&gt; VMamba shows significant performance improvements in different task : image classification, object detection, and semantic segmentation. For instance, it surpasses established benchmarks like ResNet and Swin in ImageNet-1K classification. In COCO object detection, VMamba models outperform their counterparts in mean Average Precision (mAP) and mean Intersection over Union (mIoU).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Behind the news:&lt;/strong&gt; VMamba maintains high performance across various input image sizes, which can indicate a robustness to changes in input conditions. This feature is crucial for practical applications where image resolutions can vary significantly.&lt;/p&gt;

&lt;p&gt;It dethrones Vim just the next day of Vim publication.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Why it matters:&lt;/strong&gt; VMamba’s approach combines the strengths of CNNs and ViTs and on the top it is computationally efficient.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*F7uFR9RPyq4bDvrO4a3Ycg.png&quot; alt=&quot;VMamba: Visual State Space Model&quot; /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*yK0hfDpKYFnGk0NupiBHXg.png&quot; alt=&quot;VMamba has a unique architecture: integrates global receptive fields and dynamic weights within a linear computational complexity framework&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;We’re thinking:&lt;/strong&gt; While VMamba’s results are promising, its practical applicability in diverse real-world scenarios require time verification. For data scientists, VMamba holds a promise of efficient processing of large-scale image datasets and a new playground.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;In Detail&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;VMamba’s 2D Selective Scan&lt;/strong&gt; operates by dynamically adjusting weights based on the importance of different areas in an image. This process involves an algorithm that assesses each pixel’s contribution to the target task. The scan prioritizes regions with higher information content, therefore it reduces computational load on less relevant areas. This method contrasts with traditional approaches where all pixels are treated equally. Which also leads to higher computational costs.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/1200/1*Ajt_aoa_ElU82EEXM63n2g.jpeg&quot; alt=&quot;VMamba’s 2D Selective Scan&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Vim vs VMamba&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;VMamba beats Vim on ImageNet-1k benchmark&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;VMamba-T with 22M params achieves 82.2 % acc while comparable VimS with 26M achieves 80.3% acc&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;VMamba beats Vim on COCO benchmark&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;the smalles model VMamba-T achieves APbox of 46.5 and APmask of 42.1 while Vim-T achives 45.7 and 39.2 respecitively&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;VMamba beats Vim on ADE20k benchmark&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;numbers are not directly comparable but Vmamaba seems to have better perfomance&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;arxiv-240124-segmamba-long-range-sequential-modeling-mamba-for-3d-medical-image-segmentation&quot;&gt;Arxiv 24.01.24 : &lt;strong&gt;SegMamba: Long-range Sequential Modeling Mamba For 3D Medical Image Segmentation&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2401.13560&quot;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/ge-xing/SegMamba&quot;&gt;Code&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;#segmentation #computervision&lt;/p&gt;

&lt;p&gt;This paper integrates the Mamba model with a U-shape structure for 3D medical image segmentation, aiming to efficiently process high-dimensional images.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What’s New&lt;/strong&gt;: SegMamba’s combines the Mamba model, known for handling long-range dependencies, with a U-shaped architecture, with application in 3D medical image segmentation.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How it Works&lt;/strong&gt;: SegMamba employs a Mamba encoder, a 3D decoder, and skip-connections. The Mamba encoder, with depth-wise convolution and flattening operations, reduces computational load while handling high-dimensional features.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;: On the BraTS2023 dataset, SegMamba achieved best performance.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Why it Matters:&lt;/strong&gt; SegMamba’s is quite niche but confims again universality of Mamba block.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;We’re Thinking:&lt;/em&gt; Questions arise about SegMamba’s applicability to various medical imaging forms and datasets, and its comparison with other state-of-the-art methods.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;In Detail&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;SegMamba vs Umamba&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The key difference is in their approach to enhancing the U-Net architecture. SegMamba emphasizes the Mamba model for 3D segmentation, while U-Mamba combines CNNs with SSMs for versatile segmentation tasks.&lt;/p&gt;

&lt;h2 id=&quot;arxiv-240124-mambabyte-token-free-selective-state-space-model&quot;&gt;Arxiv 24.01.24: &lt;strong&gt;MambaByte: Token-free Selective State Space Model&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2401.13660&quot;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/kyegomez/MambaByte&quot;&gt;Code&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;#llm #tokenfree&lt;/p&gt;

&lt;p&gt;This model is token-free, directly learning from raw bytes and bypassing the biases inherent in subword tokenization.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What’s New&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;“MambaByte” is a token-free language model, a novel approach as it directly learns from raw bytes, no tokenization.&lt;/li&gt;
  &lt;li&gt;A unique perspective on efficiency and performance compared to other byte-level models.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;How It Works:&lt;/strong&gt; “MambaByte” slightly modfies Mamba module to accept raw bytes.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Results:&lt;/strong&gt; The model surpasses the performance of state-of-the-art subword Transformers with lower computational resources. It demonstrates linear scaling in sequence length, leading to faster inference times.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Behind the News:&lt;/strong&gt; “MambaByte” proposes an alternative to autoregressive Transformers, libertaing us from tokenization.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Why It Matters:&lt;/strong&gt; This efficient byte sequence processing opens new avenues for language models in large-scale and diverse applications.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;We’re Thinking:&lt;/strong&gt; This paper shows a potential for token free llm learning. New applications outside of LLM should come soon.&lt;/p&gt;

&lt;h2 id=&quot;arxiv-240125-vivim-a-video-vision-mamba-for-medical-video-object-segmentation&quot;&gt;Arxiv 24.01.25 &lt;strong&gt;Vivim: a Video Vision Mamba for Medical Video Object Segmentation&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2401.14168&quot;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/scott-yjyang/Vivim&quot;&gt;Code&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;#video #biomedical&lt;/p&gt;

&lt;p&gt;A framework for medical video object segmentation, focusing on addressing challenges in long-sequence modeling in video analysis. It uses a Temporal Mamba Block, which allows the model to obtain excellent segmentation results with improved speed performance compared to existing methods.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*uqxFkqdGvoi3vf7opR57mQ.png&quot; alt=&quot;Vivim: a Video Vision Mamba for Medical Video Object Segmentation&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What’s New:&lt;/strong&gt; Vivim integrates the Mamba model into a multi-level transformer architecture, transforming video clips into feature sequences containing spatiotemporal information at various scales.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How It Works&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The Temporal Mamba Block employs a sequence reduction process for efficiency, integrating a spatial self-attention module and a Mix-FeedForward layer.&lt;/li&gt;
  &lt;li&gt;The Mamba module explores correlations among patches of input frames, while a Detail-specific FeedForward preserves fine-grained details.&lt;/li&gt;
  &lt;li&gt;A lightweight CNN- based decoder head integrates multi-level feature sequences to predict segmentation masks.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Results:&lt;/strong&gt; Vivim demonstrates superior performance on the breast US dataset outperforming existing video- and image-based segmentation methods.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Why It Matters:&lt;/strong&gt; Vivim’s approach represents a significant advancement in medical video analysis, particularly for tasks like lesion segmentation in ultrasound videos.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;We’re Thinking:&lt;/strong&gt; What are the potential limitations or challenges in scaling Vivim for broader clinical applications? The paper focuses solely on breast ultrasound videos, which may limit generalizability.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;In Detail:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Temporal Mamba Block&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;This block starts with a spatial self-attention module for extracting spatial features, followed by a Mix-FeedForward layer.&lt;/li&gt;
  &lt;li&gt;For temporal modeling, it transposes and flattens the spatiotemporal feature embedding into a 1D long sequence.&lt;/li&gt;
  &lt;li&gt;The Mamba module within the block tackles the correlation among patches in input frames, while the Detail-specific FeedForward focuses on preserving fine-grained details through depth-wise convolution.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;arxiv-240125-mambamorph-a-mamba-based-backbone-with-contrastive-feature-learning-for-deformable-mr-ct-registration&quot;&gt;Arxiv 24.01.25: &lt;strong&gt;MambaMorph: a Mamba-based Backbone with Contrastive Feature Learning for Deformable MR-CT Registration&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2401.13934.pdf&quot;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/guo-stone/mambamorph&quot;&gt;Code&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;#biomedical&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*QNfM9VLXeYFlpF57yU69Ag.png&quot; alt=&quot;MambaMorph: a Mamba-based Backbone with Contrastive Feature Learning for Deformable MR-CT Registration&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;A multi-modality deformable registration network designed specifically for aligning Magnetic Resonance (MR) and Computed Tomography (CT) images.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What’s New:&lt;/strong&gt; MambaMorph combines Mamba blocks with a feature extractor.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How It Works:&lt;/strong&gt; MambaMorph integrates a Mamba-based backbone with contrastive feature learning for deformable MR-CT registration.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Mamba-Based Registration Module: This module utilizes the Mamba blocks for efficient handling and processing of high-dimensional imaging data.&lt;/li&gt;
  &lt;li&gt;Contrastive Feature Learning: a feature extractor that employs supervised contrastive learning. This is designed to learn fine-grained, modality-specific features from the MR and CT images.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;: MambaMorph demonstrates superior performance over existing methods in MR-CT registration, showing improvements in accuracy and efficiency.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Behind the News:&lt;/strong&gt; The development of MambaMorph is a significant step in addressing the prevalent issues in multi-modality image registration, particularly in the context of MR and CT images.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Why It Matters:&lt;/strong&gt; The success of MambaMorph in MR-CT image registration has significant implications for medical imaging analysis.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;We’re Thinking:&lt;/strong&gt; While MambaMorph shows promising results, questions remain about its applicability to other forms of medical imaging and its performance in varied clinical scenarios.&lt;/p&gt;

&lt;footer&gt;
  &lt;p&gt;Exported from &lt;a href=&quot;https://medium.com&quot;&gt;Medium&lt;/a&gt; on Feb 03
    2024.&lt;/p&gt;
  &lt;p&gt;&lt;a href=&quot;https://medium.com/@ulalaparis/the-mamba-effect-mamba-models-gaining-ground-f2d2c9b9245c&quot;&gt;View
      the original&lt;/a&gt;&lt;/p&gt;
&lt;/footer&gt;
&lt;script type=&quot;text/javascript&quot; src=&quot;//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-584ec4ce89deed84&quot;&gt;&lt;/script&gt;

&lt;div class=&quot;addthis_inline_share_toolbox&quot;&gt;&lt;/div&gt;

</description>
        <pubDate>Sat, 03 Feb 2024 00:00:00 +0100</pubDate>
        <link>http://urszulaczerwinska.github.io/thoughts/the-mamba-effect-mamba-models-gaining-ground/</link>
        <guid isPermaLink="true">http://urszulaczerwinska.github.io/thoughts/the-mamba-effect-mamba-models-gaining-ground/</guid>
        
        
        <category>thoughts</category>
        
      </item>
    
      <item>
        <title>From PCA to SSL - A personal odyssey in Data Science</title>
        <description>&lt;head&gt;
  &lt;meta name=&quot;description&quot; content=&quot;A reflection on my personal journey through the data science field, tracing the evolution from fundamental techniques like Principal Component Analysis (PCA) and Independent Component Analysis (ICA) to modern Self-Supervised Learning (SSL). This post explores how foundational concepts have influenced my career and the field&apos;s progress.&quot; /&gt;
&lt;/head&gt;

&lt;p&gt;Welcome to a journey through the evolving landscape of Data Science, a journey that parallels my own academic and professional path. In this article, I will share how fundamental concepts like Principal Component Analysis (PCA) and Independent Component Analysis (ICA) have been instrumental in leading me to the field of Self-Supervised Learning (SSL).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;So, why share this story?&lt;/strong&gt; It’s not merely a chronicle of progress in Data Science; it’s a testament to the profound synergy between the foundational theories I mastered during my doctoral studies and the avant-garde techniques I am navigating now. This path hasn’t just shaped my career; it has fundamentally altered my perspective on the potential and direction of Data Science.&lt;/p&gt;

&lt;p&gt;I hope it can inspire you, dear reader, that fundamental concepts are reborn under a different form.&lt;/p&gt;

&lt;p&gt;We will start by looking at the basics of PCA and ICA, the building blocks of my entry into data analysis. From there, we’ll see how these methods have evolved into today’s more dynamic and autonomous SSL approaches.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What lessons did I learn in this transition? How do these experiences mirror the broader changes in the field of Data Science?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This exploration aims to provide a deeper understanding of how the field is evolving and what it means to us as practitioners and enthusiasts.&lt;/p&gt;

&lt;h2 id=&quot;my-roots-in-unsupervised-learning&quot;&gt;My roots in Unsupervised Learning&lt;/h2&gt;

&lt;h3 id=&quot;discovering-pca-and-ica-during-myphd&quot;&gt;Discovering PCA and ICA during my PhD&lt;/h3&gt;

&lt;h4 id=&quot;beginning-my-journey-in-unsupervised-learning&quot;&gt;&lt;strong&gt;Beginning my journey in Unsupervised Learning&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;My initial encounter with unsupervised learning occurred during my undergraduate studies, deepening significantly through my P&lt;a href=&quot;https://urszulaczerwinska.github.io/UCzPhDThesis/&quot;&gt;h.D. research at the Curie Institute.&lt;/a&gt; This experience wasn’t just academic; it represented my first foray into applying complex data science concepts to real-world biological data. It was here that I delved into Principal Component Analysis (PCA) and Independent Component Analysis (ICA), exploring their capabilities in a practical, research-driven environment.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*zuJ046QrOnct_PPF.jpg&quot; alt=&quot;Graphical representation of Principal Component Analysis (PCA) and Independent Component Analysis (ICA) in the context of data science, illustrating their application to complex biological data.&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;h4 id=&quot;applying-pca-and-ica-to-transcriptomic-analysis&quot;&gt;&lt;strong&gt;Applying PCA and ICA to transcriptomic analysis&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;In my early systems bilolgy research days, PCA was a fundamental tool for analyzing gene expression data. Its primary function was to reduce the dimensionality of large datasets, allowing me to identify and focus on the most significant variables in the data. This method was crucial for managing the complexity of transcriptomes and extracting meaningful insights from vast amounts of data.&lt;/p&gt;

&lt;p&gt;ICA, in contrast, served a different but equally vital role. It was instrumental in separating mixed signals in the data, enabling the identification of independent sources of variation. This technique was particularly useful in dissecting a complex gene expression pattern in tissue representing a mix of normal, cancer and immune cells, allowing for a clearer understanding of the underlying biological processes of an immune response to immunotherapy.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*kTN_M6iax9U6zrqK.png&quot; alt=&quot;Visualization of Principal Component Analysis (PCA) and Independent Component Analysis (ICA) applied to gene expression data, highlighting their roles in reducing dimensionality and identifying independent sources of variation in transcriptomic analysis.&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;h4 id=&quot;navigating-the-maze-challenges-and-limitations&quot;&gt;&lt;strong&gt;Navigating the maze: challenges and limitations&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*WZpscmnhCoIiTCHOaggWbA.png&quot; alt=&quot;Diagram illustrating the challenges and limitations of PCA and ICA, including issues with linear assumptions and component stability in the analysis of biological data.&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Despite their utility, both PCA and ICA have limitations, especially when applied to biological data. One major limitation of PCA is its inherent assumption of linear relationships and multivariate normal distribution of data. However, biological processes are often intricate and non-linear, and microarray gene expression measurements, for instance, tend to follow a super-Gaussian distribution, not accurately &lt;a href=&quot;https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-13-24&quot;&gt;captured by PCA&lt;/a&gt;​​. This limitation becomes particularly evident when the biological questions at hand are not directly related to the highest variance in the data, a fundamental aspect of PCA’s data decomposition approach.&lt;/p&gt;

&lt;p&gt;ICA, while effective in separating mixed signals and identifying non-Gaussian components, also presents challenges. Its results can be unstable and dependent on the number of components extracted. This is compounded by the fact that ICA does not inherently order its components by relevance, necessitating multiple runs and averaging of results to obtain robust outcomes. The high dimensionality of biological datasets often requires PCA to be used as a pre-processing step before applying ICA, which can further &lt;a href=&quot;https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-13-24&quot;&gt;complicate the analysis&lt;/a&gt;​​.&lt;/p&gt;

&lt;p&gt;Also, both approaches, belongs to the realm of statistical analysis. In a world of fast-paced innovation and deep learning, they seem covered with dust and old-fashioned.&lt;/p&gt;

&lt;p&gt;The challenges and insights gained from working extensively with ICA during my Ph.D. significantly contributed to my expertise in machine learning. Post-Ph.D., I ventured into the field of natural language processing, applying deep learning techniques to tackle complex linguistic data. My career then led me to Adevinta, where I expanded my focus to include computer vision, leveraging deep learning in new and innovative ways. This diverse experience in ML and DL paved the way for my current exploration into Self-Supervised Learning (SSL), marking a continuous journey through the evolving landscape of artificial intelligence.&lt;/p&gt;

&lt;h4 id=&quot;transitioning-to-self-supervised-learning&quot;&gt;&lt;strong&gt;Transitioning to Self-Supervised Learning&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;These experiences with PCA and ICA were more than just academic exercises; they were foundational in shaping my approach to data analysis. They prepared me for the another phase of my career in Self-Supervised Learning (SSL), where I would apply the principles of unsupervised learning to even more complex and dynamic datasets, such as image datasets. &lt;strong&gt;The core idea of finding robust abstraction of a data object (image, dataset, text) remains common to PCA and SSL.&lt;/strong&gt; This non-linear transition marked a significant shift from analyzing biological data to exploring the frontiers of artificial intelligence, but at the same time it is very connected in an intriguing way.&lt;/p&gt;

&lt;h2 id=&quot;the-evolution-of-unsupervised-learning&quot;&gt;&lt;strong&gt;The Evolution of Unsupervised Learning&lt;/strong&gt;&lt;/h2&gt;

&lt;h3 id=&quot;from-pca-and-ica-to-deep-learning&quot;&gt;&lt;strong&gt;From PCA and ICA to Deep Learning&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;The evolution from traditional unsupervised methods like PCA and ICA to Self-Supervised Learning (SSL) in Deep Learning is a significant development in the field of machine learning. While PCA and ICA were pivotal in their time for dimensionality reduction and signal separation, they have limitations with non-linear, high-dimensional data structures common in modern datasets.&lt;/p&gt;

&lt;p&gt;The advent of SSL in Deep Learning has revolutionized our approach to data. SSL, unlike its predecessors, leverages deep neural networks to learn from unlabeled data, extracting complex patterns and features without the need for explicit annotations. This advancement not only overcomes the constraints of labeled data but also opens new possibilities in diverse domains, ranging from natural language processing to computer vision.&lt;/p&gt;

&lt;p&gt;SSL represents a paradigm shift, offering a more nuanced and comprehensive understanding of data. It signifies the ongoing evolution and sophistication of machine learning techniques, marking a new era in the exploration and utilization of data.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;“If intelligence was a cake, unsupervised learning would be the cake, supervised learning would be the icing on the cake, and reinforcement learning would be the cherry on the cake.”&lt;/em&gt; — Yann LeCun&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;in-2019-yann-lecun-updated-the-above-quote-changing-unsupervised-learning-to-self-supervised-learning-and-in-2020-he-declared-that-self-supervised-learning-ssl-is-the-future-of-machine-learning-source&quot;&gt;&lt;strong&gt;In 2019, Yann LeCun updated the above quote, changing &lt;em&gt;“unsupervised learning”&lt;/em&gt; to “&lt;/strong&gt;&lt;a href=&quot;https://www.eyerys.com/articles/people/1560388243/opinions/the-next-ai-revolution-will-not-be-supervised?ref=blog.salesforceairesearch.com&quot;&gt;&lt;strong&gt;&lt;em&gt;self-supervised learning&lt;/em&gt;&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;&lt;em&gt;,”&lt;/em&gt; and in 2020 he declared that self-supervised learning (SSL) is &lt;em&gt;the future of machine learning (&lt;/em&gt;&lt;/strong&gt;&lt;a href=&quot;https://blog.salesforceairesearch.com/learning-vision-without-labels/&quot;&gt;&lt;strong&gt;&lt;em&gt;source&lt;/em&gt;&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;&lt;em&gt;)&lt;/em&gt;.&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*4QwOfU5uUYZrliLz.png&quot; alt=&quot;Image featuring a quote by Yann LeCun about the future of machine learning, emphasizing the importance of Self-Supervised Learning (SSL) in the field.&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;h2 id=&quot;introduction-to-self-supervised-learningssl&quot;&gt;Introduction to Self-Supervised Learning (SSL)&lt;/h2&gt;

&lt;h4 id=&quot;what-is-ssl&quot;&gt;&lt;strong&gt;What is SSL?&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;Self-Supervised Learning (SSL) represents the latest frontier in this evolutionary journey. SSL, in a sense, is a clever workaround to the challenge of labeled data scarcity. It leverages the data itself to generate its own supervision. Think of it as a student who, instead of relying solely on a teacher’s guidance, learns by exploring and questioning the world around them.&lt;/p&gt;

&lt;p&gt;The system learns to understand and work with data by creating its own labels from the inherent structure of the data. This is a significant departure from traditional supervised learning, where models are trained on a dataset explicitly labeled by humans.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*W6QT1xPyNisB2ptcmN8i6w.png&quot; alt=&quot;Diagram or visualization explaining the concept of Self-Supervised Learning (SSL), illustrating how the model generates its own supervision from the data.&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;For instance, in image processing, an SSL algorithm might learn to predict missing parts of an image, or in text processing, to predict the next word in a sentence. Through these tasks, the model gains an intrinsic understanding of the structure and context of the data.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*Gz_gg4DZBQzd72sKpiphmA.png&quot; alt=&quot;Illustration of an SSL algorithm in action, such as predicting missing parts of an image or the next word in a sentence, demonstrating how Self-Supervised Learning models understand data structure.&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Self-Supervised Learning, a subset of unsupervised learning, has evolved dramatically, introducing various families of models, each with its unique approach to learning from unlabeled data. Here are some of the primary &lt;a href=&quot;https://arxiv.org/abs/2301.05712&quot;&gt;families of SSL algorithms&lt;/a&gt;:&lt;/p&gt;

&lt;h4 id=&quot;1-contrastive-learning-models&quot;&gt;&lt;strong&gt;1. Contrastive Learning Models:&lt;/strong&gt;&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Principle: These models learn representations by contrasting positive pairs (similar or related data points) against negative pairs (dissimilar or unrelated data points).&lt;/li&gt;
  &lt;li&gt;SimCLR (Simple Framework for Contrastive Learning of Visual Representations): Utilizes a simple contrastive learning framework for visual representations.&lt;/li&gt;
  &lt;li&gt;MoCo (Momentum Contrast for Unsupervised Visual Representation Learning): Focuses on building dynamic dictionaries for contrastive learning in vision.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;2-predictive-learning-models&quot;&gt;&lt;strong&gt;2. Predictive Learning Models:&lt;/strong&gt;&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Principle: These models predict some parts of the data using other parts, thereby learning useful representations.&lt;/li&gt;
  &lt;li&gt;BERT (Bidirectional Encoder Representations from Transformers): Predicts missing words in a sentence, gaining contextual understanding in NLP.&lt;/li&gt;
  &lt;li&gt;GPT (Generative Pretrained Transformer): Predicts the next word in a sequence, learning sequential and contextual patterns in text.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;3-generative-models&quot;&gt;&lt;strong&gt;3. Generative Models:&lt;/strong&gt;&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Principle: These models learn to generate or reconstruct data, thereby understanding the distribution and structure of the dataset.&lt;/li&gt;
  &lt;li&gt;VAE (Variational Autoencoders): Learns to reconstruct input data, capturing the probabilistic distribution.&lt;/li&gt;
  &lt;li&gt;GANs (Generative Adversarial Networks): Involves a generator and a discriminator learning in a competitive manner to produce realistic data.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;4-clustering-based-models&quot;&gt;&lt;strong&gt;4. Clustering-Based Models:&lt;/strong&gt;&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Principle: This approach involves clustering data points and learning representations that respect these cluster assignments.&lt;/li&gt;
  &lt;li&gt;DeepCluster: Utilizes clustering of features and subsequent representation learning.&lt;/li&gt;
  &lt;li&gt;SwAV (Swapping Assignments between Views): Employs a unique approach where it clusters data points and enforces consistency between cluster assignments of different augmented views of the same image.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;5-transformation-recognition-models&quot;&gt;&lt;strong&gt;5. Transformation Recognition Models:&lt;/strong&gt;&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Principle: These models learn by recognizing transformations applied to the input data.&lt;/li&gt;
  &lt;li&gt;Jigsaw Puzzles as a task: The model learns by solving jigsaw puzzles, essentially recognizing spatial relations and transformations.&lt;/li&gt;
  &lt;li&gt;RotNet: Involves learning by recognizing the rotation applied to images.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Each of these families represents a different angle of approaching the challenge of learning from unlabeled data. Self-Supervised Learning (SSL) marks a significant evolution in machine learning, especially in the realms of deep learning. It transforms the challenge of learning from unlabeled data into an opportunity. In practice, SSL has been revolutionary, particularly in fields like natural language processing and computer vision, where it has expanded the boundaries of machine learning applications.&lt;/p&gt;

&lt;h4 id=&quot;limitations&quot;&gt;&lt;strong&gt;Limitations&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;SSL’s transformative potential is undeniable, but its journey to widespread industry adoption is hindered by several key challenges. The most notable is the high computational cost. Training SSL models demands significant resources, posing a barrier for smaller entities, as highlighted in industry analyses. Additionally, the technical complexity of SSL algorithms is a daunting hurdle, requiring deep expertise for effective implementation and task-specific fine-tuning.&lt;/p&gt;

&lt;p&gt;Data quality and variety are crucial for SSL effectiveness. In data-limited or sensitive industries, SSL models face difficulties in learning efficiently. Moreover, the industry lags in developing readily usable SSL frameworks, slowing down practical application despite rapid academic progress.&lt;/p&gt;

&lt;p&gt;Another critical aspect is the ethical and privacy implications of using large datasets essential for SSL. The industry must navigate this delicate balance to ensure ethical data utilization.&lt;/p&gt;

&lt;h4 id=&quot;bridge-between-pca-and-ssl&quot;&gt;&lt;strong&gt;Bridge between PCA and SSL&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;The intriguing aspect of SSL, especially in the context of contrastive learning (CL) and its relation to PCA, is highlighted in studies such as “&lt;a href=&quot;https://arxiv.org/pdf/2201.12680v2.pdf&quot;&gt;Deep Contrastive Learning is Provably (almost) Principal Component Analysis&lt;/a&gt;.” This research provides a novel perspective on CL, showing that with deep linear networks, the representation learning aspect of CL aligns closely with PCA’s principles​​. This connection underlines the evolutionary link from traditional statistical methods like PCA to advanced SSL techniques.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*_5M3NTt8CSpFtazdwIY_QQ.png&quot; alt=&quot;Diagram or illustration from research showing the relationship between Deep Contrastive Learning (CL) and Principal Component Analysis (PCA), highlighting the theoretical connection between these methods.&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;We provide a novel game-theoretical perspective of con- trastive learning (CL) over loss functions (e.g., InfoNCE) and prove that with deep linear network, the representation learning part is equivalent to Principal Component Analysis (PCA).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;By leveraging deep neural networks, CL in SSL transcends PCA’s linear constraints, enabling the extraction of complex, nonlinear relationships within data. This progression from PCA to SSL illustrates how foundational data science concepts continue to shape contemporary technologies. Understanding this link allows us to appreciate SSL, particularly in deep learning, as a modern interpretation of long-standing principles in data analysis.&lt;/p&gt;

&lt;p&gt;The transition from PCA and ICA to SSL represents a leap forward in our capacity to not just recognize but deeply comprehend patterns in data, opening new horizons in data science and beyond.&lt;/p&gt;

&lt;h2 id=&quot;linking-the-past-with-the-present-personal-perspective&quot;&gt;Linking the past with the present: personal perspective&lt;/h2&gt;

&lt;h3 id=&quot;my-encounter-withssl&quot;&gt;My encounter with SSL&lt;/h3&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*WKF4iR5aQVEX9Mv73dsNPQ.png&quot; alt=&quot;Image representing the personal journey into Self-Supervised Learning (SSL), reflecting the transition from traditional methods like PCA and ICA to advanced SSL techniques in data science&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;My journey into the realm of Self-Supervised Learning (SSL) was like stepping into a new world, yet one that felt strangely familiar. I first encountered SSL while expanding my horizons in the ever-evolving landscape of data science. Coming from a background heavily influenced by PCA and ICA in computational biology, the leap to SSL was both intriguing and formidable.&lt;/p&gt;

&lt;p&gt;Initially, SSL seemed like a puzzle. It promised a more nuanced understanding of data without the explicit need for labels, a concept that was both challenging and exciting and above all very familiar. This resonated with my earlier work where we often grappled with unlabeled biological datasets. SSL’s approach of learning from the data itself, finding patterns, and using them to build more robust models, was a game changer. It was like watching the evolution of my previous work in PCA and ICA but on a more intricate and expansive scale.&lt;/p&gt;

&lt;h3 id=&quot;insights-from-the-recent-sotaarticles&quot;&gt;&lt;strong&gt;Insights from the recent SOTA articles&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;The insights from ICCV 2023, particularly the &lt;a href=&quot;https://bigmac-vision.github.io/&quot;&gt;BigMAC workshop,&lt;/a&gt; illuminated the recent strides in SSL. The workshop’s focus on large model adaptation for computer vision highlighted the challenges and opportunities arising from the increasing size and complexity of neural networks, especially in adapting them to novel tasks and domains​​.&lt;/p&gt;

&lt;p&gt;Key talks such as Neil Houlsby’s on “&lt;a href=&quot;https://www.youtube.com/watch?v=ZwtMEF0u5cM&quot;&gt;Advances in Visual Pretraining for LLMs&lt;/a&gt;” emphasized the scalability of Vision Transformers and their growing visual capabilities, marking significant progress in visual pre-training​​. I&lt;a href=&quot;https://arxiv.org/abs/2111.06377&quot;&gt;shan Misra’s discussion&lt;/a&gt; on leveraging SSL for scaling multimodal models demonstrated how SSL can enhance model efficiency and improve foundational multimodal models, even in the context of scarce paired data​​.&lt;/p&gt;

&lt;p&gt;The session on fine-tuning pretrained models revealed crucial insights into mitigating feature distortion, a challenge often overlooked but vital for preserving the robustness and accuracy of models​​. Additionally, talks on controlling large-scale text-to-image diffusion models like DALL-E 2 and Imagen offered perspectives on enhancing user control in the generation process, blending training-time and inference-time techniques​​.&lt;/p&gt;

&lt;p&gt;These developments at ICCV 2023 underscored SSL’s dynamic evolution and its transition from traditional PCA and ICA methodologies to more sophisticated, nuanced models capable of deeper and more intuitive data understanding.&lt;/p&gt;

&lt;h3 id=&quot;the-future-of-multimodal-data-alignment-inbiology&quot;&gt;The future of multimodal data alignment in Biology&lt;/h3&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*SodBq0lnsOvDWBNvSGjCVQ.png&quot; alt=&quot;Diagram or illustration related to the alignment of multimodal biological data using advanced AI techniques, showcasing concepts discussed at ICCV 2023 and models like I-JEPA.&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;One of the most exciting prospects brought to light at &lt;a href=&quot;https://openaccess.thecvf.com/ICCV2023?day=2023-10-04&quot;&gt;ICCV 2023&lt;/a&gt;, a conference rich in innovative ideas as evident in its proceedings, was the potential of AI in aligning multimodal biological data. The discussions led by pioneers like Yann LeCun, whose publications and &lt;a href=&quot;https://www.youtube.com/playlist?list=PL80I41oVxglK--is17UhoHVosOLFEJzKQ&quot;&gt;talks&lt;/a&gt; have been a personal beacon for me, particularly on AI models like &lt;a href=&quot;https://ai.meta.com/blog/yann-lecun-ai-model-i-jepa/&quot;&gt;I-JEPA&lt;/a&gt; for different modalities, opened up a new realm of possibilities. This approach could revolutionize how we handle complex biological datasets, like transcriptomes, microscopy images, immunoscores, single-cell data, and proteomics.&lt;/p&gt;

&lt;p&gt;The idea of integrating these diverse data types using advanced SSL techniques is not just a technological leap; it’s a new way of thinking about biological research. Reflecting on my journey, where resources like “&lt;a href=&quot;https://www.nature.com/articles/d41586-018-02174-z&quot;&gt;Deep Learning for Biology&lt;/a&gt;” have been instrumental in understanding the application of deep learning in biology, I see how we’re transitioning from isolated data types to a holistic, interconnected understanding of life’s complexities. Likewise, the survey ‘&lt;a href=&quot;https://arxiv.org/abs/1705.09406&quot;&gt;Multimodal Machine Learning: A Survey and Taxonomy&lt;/a&gt;’ by Morency et al. has been enlightening in understanding the methodologies of multimodal data integration. These readings have not only informed my professional growth but have also mirrored the evolution of data science itself — from focusing on singular data types to embracing an integrated, multi-dimensional approach that resonates with my own professional evolution and reaffirms my belief in the transformative power of data science.&lt;/p&gt;

&lt;h2 id=&quot;practical-implications-and-future-directions&quot;&gt;Practical implications and future directions&lt;/h2&gt;

&lt;h3 id=&quot;applying-ssl-in-my-currentprojects&quot;&gt;Applying SSL in my current projects&lt;/h3&gt;

&lt;p&gt;Integrating Self-Supervised Learning (SSL) into my work at Adevinta has been akin to embarking on an exhilarating expedition into uncharted territories of computer vision. Reding about models like &lt;a href=&quot;https://betterprogramming.pub/dinov2-the-new-frontier-in-self-supervised-learning-b3a939f6d533&quot;&gt;DINO v2&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/2111.06377&quot;&gt;masked autoencoders&lt;/a&gt;, the versatility and transformative power of these tools continually astound me. In my role, which orbits primarily around vision classifiers and deep learning frameworks like TensorFlow and PyTorch, SSL has emerged not just as a tool but as a concept, enabling me to push for an operational paradigm shift for data users and ML/DS teams.&lt;/p&gt;

&lt;h3 id=&quot;the-boundless-horizon-ofssl&quot;&gt;The Boundless Horizon of SSL&lt;/h3&gt;

&lt;p&gt;Peering into the future, I envision SSL as the an important advancement. Its prowess in harnessing the untapped potential of unlabeled data heralds a new era of possibilities. Imagine a world where SSL is the norm in data science, especially in data-rich yet label-poor realms like healthcare and finance. The implications are monumental.&lt;/p&gt;

&lt;p&gt;Inspired by visionaries like Yann LeCun, the prospect of SSL in aligning disparate biological data types — from the intricate patterns of transcriptomics to the dynamic landscapes of proteomics — is not just exciting; it’s revolutionary. It’s akin to finding a Rosetta Stone in a sea of biological data, offering us a more integrated and nuanced understanding of complex systems.&lt;/p&gt;

&lt;h3 id=&quot;the-vanguard-of-computer-vision-ssls-transformative-role&quot;&gt;The vanguard of Computer Vision: SSL’s transformative role&lt;/h3&gt;

&lt;p&gt;The realm of computer vision (CV) is undergoing a metamorphosis, thanks to SSL. The ICCV 2023 conference in Paris was a revelatory experience for me, showcasing the boundless potential of SSL in CV. It’s not just about big models and fine-tuning; it’s a paradigm shift in how we interact with visual data. Think of SSL combined with weak supervision as the new alchemists in the world of visual understanding.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://medium.com/adevinta-tech-blog/foundation-models-a-new-vision-for-e-commerce-76904a3066e8&quot;&gt;Foundation models&lt;/a&gt; in SSL, these behemoths of pre-trained knowledge, are poised to redefine tasks from image classification to the more intricate challenges of visual reasoning. Their true magic lies in their chameleon-like ability to adapt to specific tasks, offering unparalleled versatility and efficiency.&lt;/p&gt;

&lt;h3 id=&quot;ssl-the-dawn-of-a-newera&quot;&gt;SSL, the dawn of a new era&lt;/h3&gt;

&lt;p&gt;The journey of SSL is just beginning. Its evolution promises to be more than just incremental; it’s set to be revolutionary, reshaping how we approach, analyze, and derive meaning from data. The future of SSL, particularly in computer vision and the broader landscape of data science, is not just brimming with promise — it’s poised to be a transformative force.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;As I reflect on this odyssey from the structured realms of PCA and ICA during my Ph.D. at the Curie Institute, to the explorative and innovative universe of Self-Supervised Learning (SSL), it feels like an enlightening journey, threading together various phases of my life. This transition symbolizes more than a shift in methodologies; it represents a personal evolution in understanding and harnessing the power of data.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*HMFH9LJvEuar1UqaLl8kww.png&quot; alt=&quot;Image symbolizing the journey from traditional PCA and ICA to the modern landscape of Self-Supervised Learning (SSL), reflecting the personal and professional evolution in understanding and applying data science methodologies.&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Throughout this journey, several &lt;strong&gt;key learnings&lt;/strong&gt; stand out. Firstly, the importance of foundational understanding — the principles of PCA and ICA have been crucial in grasping the nuances of SSL. Secondly, adaptability and continuous learning are not just beneficial but essential in the ever-evolving field of data science. Lastly, interdisciplinary collaboration, as I experienced in Paris, has been invaluable, teaching me that diverse perspectives often lead to groundbreaking innovations.&lt;/p&gt;

&lt;p&gt;Looking ahead, I foresee SSL playing a pivotal role in areas beyond its current applications. The integration of SSL with emerging technologies like quantum computing or augmented reality could open new frontiers in data analysis and interpretation. Additionally, as artificial intelligence becomes increasingly autonomous, SSL may become central in developing more intuitive, self-improving AI systems.&lt;/p&gt;

&lt;p&gt;For the data science industry, the implications are vast. SSL’s ability to leverage unlabeled data will become increasingly crucial as data volumes grow exponentially. This could lead to more efficient data processing and a deeper understanding of complex patterns, significantly benefiting sectors like healthcare, finance, and environmental science. Moreover, as SSL continues to evolve, it will likely drive a shift towards more sophisticated, nuanced data analysis methods across the industry.&lt;/p&gt;

&lt;p&gt;In my current role, exploring different facets of data science, including my recent involvement in computer vision at &lt;a href=&quot;https://adevinta.com/&quot;&gt;Adevinta&lt;/a&gt;, I’ve seen firsthand the transformative impact of SSL. It’s a vivid reminder that our expertise is always evolving, built upon the foundations of our past experiences.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;To my readers, I share this journey as an encouragement to embrace the vast and varied landscape of data science. Let your experiences guide and inspire you to explore new territories. The application of SSL across different fields, including my own explorations in computer vision, demonstrates the exciting potential of integrating past knowledge with cutting-edge innovations.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In the ever-changing world of data science, our greatest strength lies in our willingness to learn and adapt. The path of learning is endless, and I am eager to see where our collective curiosity and innovation will lead us next.&lt;/p&gt;

&lt;h3 id=&quot;recommended-reading&quot;&gt;Recommended reading:&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2301.05712&quot;&gt;https://arxiv.org/abs/2301.05712&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/2201.12680v2.pdf&quot;&gt;https://arxiv.org/pdf/2201.12680v2.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2305.13689&quot;&gt;https://arxiv.org/abs/2305.13689&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2305.00729&quot;&gt;https://arxiv.org/abs/2305.00729&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2111.06377&quot;&gt;https://arxiv.org/abs/2111.06377&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2301.08243&quot;&gt;https://arxiv.org/abs/2301.08243&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2304.07193&quot;&gt;https://arxiv.org/abs/2304.07193&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;my-relatedworks&quot;&gt;My related works&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;PCA : &lt;a href=&quot;http://bioinfo-out.curie.fr/projects/dedal/&quot;&gt;DeDaL&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;ICA: &lt;a href=&quot;https://urszulaczerwinska.github.io/DeconICA/&quot;&gt;DeconICA&lt;/a&gt; &amp;amp; &lt;a href=&quot;https://urszulaczerwinska.github.io/UCzPhDThesis/&quot;&gt;PhD Thesis&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/adevinta-tech-blog/foundation-models-a-new-vision-for-e-commerce-76904a3066e8&quot;&gt;Foundation models blog post&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://urszulaczerwinska.github.io/about/&quot;&gt;about me&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;footer&gt;
  &lt;p&gt;Exported from &lt;a href=&quot;https://medium.com&quot;&gt;Medium&lt;/a&gt; on Jan 03
    2024.&lt;/p&gt;
  &lt;p&gt;&lt;a href=&quot;https://medium.com/@ulalaparis/from-pca-to-ssl-a-personal-odyssey-in-data-science-ba41ef311c5b&quot;&gt;View
      the original&lt;/a&gt;&lt;/p&gt;
&lt;/footer&gt;
&lt;script type=&quot;text/javascript&quot; src=&quot;//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-584ec4ce89deed84&quot;&gt;&lt;/script&gt;

&lt;div class=&quot;addthis_inline_share_toolbox&quot;&gt;&lt;/div&gt;

</description>
        <pubDate>Wed, 03 Jan 2024 00:00:00 +0100</pubDate>
        <link>http://urszulaczerwinska.github.io/thoughts/from-pca-to-ssl-a-personal-odyssey-in-data-science</link>
        <guid isPermaLink="true">http://urszulaczerwinska.github.io/thoughts/from-pca-to-ssl-a-personal-odyssey-in-data-science</guid>
        
        
        <category>thoughts</category>
        
      </item>
    
      <item>
        <title>AI Foundation Models</title>
        <description>&lt;head&gt;
  &lt;meta name=&quot;description&quot; content=&quot;Discover how AI foundation models are revolutionizing e-commerce by enhancing product development, driving sustainability, and fostering collaboration. Explore the challenges and strategies for successful implementation.&quot; /&gt;
&lt;/head&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;&lt;em&gt;“Foundation models are models that are trained on broad data and can be adapted to a wide range of downstream tasks. (…) In choosing this term, we take ‘foundation’ to designate the function of these models: a foundation is built first and it alone is fundamentally unfinished, requiring (possibly substantial) subsequent building to be useful. ‘Foundation’ also conveys the gravity of building durable, robust, and reliable bedrock through deliberate and judicious action.”&lt;/em&gt;&lt;/strong&gt;
 — the Stanford Institute for Human-Centred AI founded the &lt;a href=&quot;https://arxiv.org/pdf/2108.07258.pdf&quot;&gt;Center for Research on Foundation Models&lt;/a&gt; (CRFM), 2021&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*PDC2WWQyZPpIcKZU&quot; alt=&quot;Members of the Cognition team at Adevinta discussing the latest trends in AI and computer vision.&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;In the competitive realm of digital commerce, embracing technological advancements is not a luxury but a necessity for maintaining success. Among ML tools, foundation models are emerging as a formidable force. But what are foundation models, and why have they become a focal point among technologists and business leaders?&lt;/p&gt;

&lt;p&gt;As members of ‘Cognition’, a team dedicated to computer vision at &lt;a href=&quot;https://adevinta.com/&quot;&gt;Adevinta&lt;/a&gt;, we are eager to share the insights we have gathered through our technology trends watch and recent attendance of the &lt;a href=&quot;https://iccv2023.thecvf.com/&quot;&gt;ICCV 2023&lt;/a&gt; conference. This practice not only ensures our internal services remain up to date but also improves the standards experienced by our users, enhancing the services provided to customers across &lt;a href=&quot;https://adevinta.com/our-brands&quot;&gt;Adevinta’s brands&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;what-are-foundation-models&quot;&gt;What are foundation models ?&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Foundation models&lt;/strong&gt; are a breed of artificial intelligence (AI) models pre-trained on a vast amount of data, laying a robust groundwork for further customisation on specific tasks.&lt;/p&gt;

&lt;p&gt;Unlike traditional machine learning models, which require from-scratch training or fine tuning for every new task, &lt;strong&gt;foundation models offer a substantial head start&lt;/strong&gt;. They have already learned a good deal from the data they were initially trained on, which includes recognising patterns, objects, and in the domain of computer vision, even understanding the semantics of a scene.&lt;/p&gt;

&lt;p&gt;Foundation models can be leveraged in various ways, each with its own balance of resource consumption and performance enhancement. The most resource-efficient method involves extracting features from an image, “freezing” them, and then using them directly as a zero-shot retrieval, classifier or detector. This zero-shot approach requires no further learning, allowing for immediate application.&lt;/p&gt;

&lt;p&gt;Alternatively, these embeddings can serve as inputs to other models, such as an MLP or an XGBoost classifier, through transfer learning. This strategy necessitates a minimal training dataset, yet it remains swift and cost-effective. &lt;a href=&quot;https://arxiv.org/abs/2209.07932&quot;&gt;Pastore et al&lt;/a&gt; reported that there can be &lt;strong&gt;10x to 100x speed increase&lt;/strong&gt; coupled with limited accuracy decrease (1–5% on average), depending on the dataset, when using a kernel classifier on top of frozen features. For a well-known CIFAR100 dataset, the authors observed 10x to 12x speed increase and −3.70% accuracy decrease. From our preliminary experiments, preparing for deploying image embedding services for Adevinta marketplaces, we noted a 5x to 10x speed increase with less than 3% accuracy drop for ImageNet1K dataset with Dinov2 frozen features compared to fine tuning a CNN backbone.&lt;/p&gt;

&lt;p&gt;For those seeking even greater performance enhancements, fine-tuning either the last layers or the entire network is an option. This process may demand a deeper understanding of machine learning and a larger dataset for model refinement, but can lead to substantial improvements. A key challenge in this approach is maintaining the model’s generalisability and preventing it from “forgetting” previously learned datasets.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*yvtVGxc_UkJgw2q6&quot; alt=&quot;A visual representation of the AI model development process, highlighting the efficiency gained by using foundation models.&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;To further enrich the model with bespoke data, one can explore self-supervised learning techniques for pre-training or distillation on domain-specific data. Moreover, to ensure the model remains current with new data, continuous learning methodologies can be employed. These advanced techniques not only enhance the model’s performance, but also tailor it more closely to specific business needs and data environments.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*2ccVFc0mWHKRw6L0&quot; alt=&quot;An illustration of the fine-tuning process in AI model development, emphasizing the balance between performance and resource use&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Beyond the singular applications of foundation models lies the potential for a transformative synergy. By harnessing models trained on diverse datasets with various loss functions, we can unlock new heights of performance. This approach was masterfully demonstrated by &lt;a href=&quot;https://arxiv.org/abs/2306.00984?utm_campaign=The%20Batch&amp;amp;utm_medium=email&amp;amp;_hsmi=281785463&amp;amp;utm_content=281787502&amp;amp;utm_source=hs_email&quot;&gt;Krishnan et al&lt;/a&gt;, who capitalised on images synthesised by Stable Diffusion. They adeptly trained another model (StableRep) using a contrastive loss approach and ViT backbone to achieve remarkable success in a classification benchmark. This strategy showcases the innovative fusion of generative and discriminative model capabilities, setting a new standard for adaptive and robust AI applications.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;&lt;em&gt;“Different foundation models understand different aspects of the world. It’s exciting that a large diffusion model, which is good at generating images, can be used to train a large vision transformer, which is good at analysing images!”&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;&lt;em&gt;—&lt;/em&gt;&lt;/strong&gt; &lt;a href=&quot;https://info.deeplearning.ai/openai-empowers-developers-ai-risk-in-the-spotlight-decoding-schizophrenic-language-synthetic-data-helps-image-classification-1?ecid=ACsprvum6jLSdI3_MtO8GVvBlJrsfj1iNuU7d7wJk3k6DmAu6jDwlmqxh0ZqOYQpd5P6T9SkgLDq&amp;amp;utm_campaign=The%20Batch&amp;amp;utm_medium=email&amp;amp;_hsmi=281785463&amp;amp;_hsenc=p2ANqtz-_AfPCrj7xxHY4m3H4td4jKSdynMLio8p3y-HqpQE0KbMIn5qoGh6dicKnKqf-6eVEcThLfdSR4_uMpwahHLcZqGQKfVg&amp;amp;utm_content=281787502&amp;amp;utm_source=hs_email&quot;&gt;The Batch @Deeplearning.ai newsletter&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;the-rise-of-foundation-models&quot;&gt;The rise of foundation models&lt;/h2&gt;

&lt;p&gt;The history of foundation models is closely tied to the rise of deep learning, particularly with the advent of large-scale models like GPT (Generative Pre-trained Transformer) by OpenAI and BERT (Bidirectional Encoder Representations from Transformers) by Google, which demonstrated the feasibility and effectiveness of pre-training models on vast datasets and then fine-tuning them for specific tasks.&lt;/p&gt;

&lt;p&gt;As technology advanced, so did the scale and capabilities of these models, with models like GPT-3, GPT-4 and T5 showcasing unprecedented levels of generalisation and adaptability across numerous domains including natural language processing, computer vision, and even multimodal tasks combining both vision and text. The success of these models started &lt;strong&gt;a new era where the focus shifted from training task-specific models from scratch to developing robust, versatile foundation models.&lt;/strong&gt; This new type of model could be fine-tuned or used in transfer-learning to excel at a broad spectrum of tasks. This shift not only catalysed significant advancements in AI research but also broadened adoption of AI across various industries, paving the way for more sophisticated and capable foundation models that continue to push the boundaries of what’s achievable with Artificial Intelligence.&lt;/p&gt;

&lt;p&gt;Notable examples of foundation models abound in the tech landscape. For instance, DINOv2 and MAE (Masked Autoencoder) by Meta AI for image understanding. On the other hand, models like CLIP and BLIP from OpenAI have shown the potential of bridging the gap between vision and language. These models, pre-trained on diverse and voluminous datasets, encapsulate a broad spectrum of knowledge that can be adapted for more specialised tasks, making them particularly advantageous for industries with data-rich environments like e-commerce.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*EHZAOKUjrq7Sz6uP&quot; alt=&quot;An illustration demonstrating the innovative use of foundation models to achieve advanced AI capabilities.&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Here is a short description of a few of those models:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;DINOv2:&lt;/strong&gt; Developed by Meta, &lt;a href=&quot;https://ai.meta.com/blog/dino-v2-computer-vision-self-supervised-learning/&quot;&gt;DINOv2&lt;/a&gt; is recognised for its self-supervised learning approach in training computer vision models, achieving significant results.&lt;/p&gt;

&lt;p&gt;The model underscores the potency of self-supervised learning in advancing computer vision capabilities​​.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Masked Autoencoders (&lt;/strong&gt;&lt;a href=&quot;https://arxiv.org/abs/2111.06377#:~:text=This%20paper%20shows%20that%20masked,based%20on%20two%20core%20designs&quot;&gt;&lt;strong&gt;MAE&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;):&lt;/strong&gt; MAE is a scalable self-supervised learning approach for computer vision that involves masking random patches of the input image and reconstructing the missing pixels.&lt;/p&gt;

&lt;p&gt;Meta AI demonstrated the effectiveness of MAE pre-pre training for billion-scale pretraining, combining self-supervised (1st stage) and weakly-supervised learning (2nd stage) for improved performance​.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2103.00020&quot;&gt;&lt;strong&gt;CLIP&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;(Contrastive Language-Image Pre-Training):&lt;/strong&gt; Developed by OpenAI, CLIP is a groundbreaking model that bridges computer vision and natural language processing, leveraging an abundantly available source of supervision: the text paired with images found across the internet.&lt;/p&gt;

&lt;p&gt;CLIP is the first multimodal model tackling computer vision, trained on a variety of (image, text) pairs, achieving competitive zero-shot performance on a variety of image classification datasets. It brings many of the recent developments from the realm of natural language processing into the mainstream of computer vision, including unsupervised learning, transformers, and multimodality​&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Segment Anything Model (&lt;/strong&gt;&lt;a href=&quot;https://segment-anything.com/&quot;&gt;&lt;strong&gt;SAM&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;):&lt;/strong&gt; Developed by Meta’s FAIR lab, SAM is a state-of-the-art image segmentation model that aims to revolutionise the field of computer vision by identifying which pixels in an image belong to which object, producing detailed object masks from input prompts.&lt;/p&gt;

&lt;p&gt;SAM is built on foundation models that have significantly impacted natural language processing (NLP), and focuses on promptable segmentation tasks, adapting to diverse downstream segmentation problems using prompt engineering​.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2211.06220&quot;&gt;&lt;strong&gt;OneFormer&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;/&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2105.15203&quot;&gt;&lt;strong&gt;SegFormer&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;:&lt;/strong&gt; A state-of-the-art multi-task image segmentation framework implemented using transformers. Parameters: 219 million. Architecture: ViT&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.microsoft.com/en-us/research/publication/florence-a-new-foundation-model-for-computer-vision/#:~:text=While%20existing%20vision%20foundation%20models,videos&quot;&gt;&lt;strong&gt;Florence&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;:&lt;/strong&gt; Introduced by Microsoft, this foundation model has set new benchmarks on several leaderboards such as TextCaps Challenge 2021, nocaps, Kinetics-400/Kinetics-600 action classification, and OK-VQA Leaderboard. Florence aims to expand representations from coarse (scene) to fine (object), and from static (images) to dynamic (videos)​.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2112.10752&quot;&gt;&lt;strong&gt;Stable Diffusion&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;:&lt;/strong&gt; A generative model utilising AI and deep learning to generate images, functioning as a diffusion model with a sequential application of denoising autoencoders​.&lt;/p&gt;

&lt;p&gt;It employs a U-Net model, specifically a Residual Neural Network (ResNet), originally developed for image segmentation in biomedicine, to denoise images and control the image generation process without retraining​​.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://openai.com/dall-e-3&quot;&gt;&lt;strong&gt;DALL-E&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;:&lt;/strong&gt; Developed by OpenAI, DALL-E is a generative model capable of creating images from textual descriptions, showcasing a unique blend of natural language understanding and image generation. It employs a version of the GPT-3 architecture to generate images, demonstrating the potential of transformer models in tasks beyond natural language processing​&lt;/p&gt;

&lt;p&gt;The tech titans, often bundled as GAFA (Google, Amazon, Facebook and Apple), alongside several other companies such as Hugging Face, Anthropic, AI21 Labs, Cohere, Aleph Alpha, Open AI and Salesforce have been instrumental in developing, utilising and advancing foundation models. Substantial investments in these models underscore their potential, as these corporations harness foundation models to augment various facets of their operations, setting a benchmark for &lt;a href=&quot;https://crfm.stanford.edu/2021/10/18/reflections.html#:~:text=Simultaneously%2C%20in%20industry%2C%20several%20startups,that%20impact%20billions%20of%20people&quot;&gt;other sectors&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Insights from industry leaders at &lt;a href=&quot;https://venturebeat.com/ai/foundation-models-2022s-ai-paradigm-shift/#:~:text=Foundation%20models%20like%20DALL,computer%20science%20%20department%20at&quot;&gt;Google&lt;/a&gt;, &lt;a href=&quot;https://www.microsoft.com/en-us/research/academic-program/accelerate-foundation-models-research-fall-2023/#:~:text=About%20the%20program,society%20while%20mitigating%20risks&quot;&gt;Microsoft&lt;/a&gt; and &lt;a href=&quot;https://research.ibm.com/topics/foundation-models&quot;&gt;IBM&lt;/a&gt;, alongside academic institutions, provide a rich tapestry of knowledge and perspectives​.&lt;/p&gt;

&lt;p&gt;Percy Liang, a director of the Center for Research on Foundation Models, emphasised in &lt;a href=&quot;https://www.protocol.com/enterprise/foundation-models-ai-standards-stanford&quot;&gt;this article&lt;/a&gt; that foundation models like DALL-E and GPT-3 herald new creative opportunities and novel interaction mechanisms with systems, showcasing the innovation that these models can bring to the table. He also mentions potential risks of such powerful models​.&lt;/p&gt;

&lt;p&gt;At the &lt;a href=&quot;https://iccv2023.thecvf.com/&quot;&gt;ICCV 2023 conference&lt;/a&gt;, held this year in Paris, foundation models were a very present topic. William T. Freeman, Professor of Computer Science, MIT, talked about the foundation models in his talk in &lt;a href=&quot;https://gkioxari.github.io/Tutorials/iccv2023/&quot;&gt;QUO VADIS Computer Vision&lt;/a&gt; workshop. He cited reasons why he &lt;a href=&quot;https://drive.google.com/file/d/1HfSrxSMS54c6-rYQNKBZnqgk_eRYqwOx/view&quot;&gt;does not like foundation models&lt;/a&gt; as an academic:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;em&gt;They don’t tell us how vision works.&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;They’re not fundamental (and therefore not stable)&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;They separate academia from industry&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This highlights the importance of foundation models for the future of computer vision and their established position and pragmatic aspect of those models focusing on performance.&lt;/p&gt;

&lt;p&gt;IBM Research posits that &lt;a href=&quot;https://research.ibm.com/topics/foundation-models&quot;&gt;foundation models will significantly expedite AI adoption&lt;/a&gt; in business settings. The general applicability of these models, enabled through self-supervised learning and fine-tuning, allows for a wide range of AI applications, thereby accelerating AI deployment across various business domains​.&lt;/p&gt;

&lt;p&gt;Microsoft Research highlights that foundation models are instigating &lt;a href=&quot;https://research.ibm.com/topics/foundation-models&quot;&gt;a fundamental shift in computing research&lt;/a&gt; and across various scientific domains. This shift is underpinned by the models’ ability to fuel industry-led advances in AI, thereby contributing to a vibrant and diverse research ecosystem that’s poised to unlock the promise of AI for societal benefit while addressing associated risks.&lt;/p&gt;

&lt;p&gt;Experts also underscore the critical role of computer vision foundation models in solving real-world applications, emphasising their &lt;a href=&quot;https://crfm.stanford.edu/2021/10/18/reflections.html#:~:text=Simultaneously%2C%20in%20industry%2C%20several%20startups,that%20impact%20billions%20of%20people&quot;&gt;adaptability to a myriad of downstream&lt;/a&gt; tasks due to training on diverse, large-scale datasets​. Moreover, foundation models like CLIP &lt;a href=&quot;https://arxiv.org/abs/2103.00020&quot;&gt;enable zero-shot learning&lt;/a&gt;, allowing for versatile applications like classifying video frames, identifying scene changes and building semantic image search engines without necessitating prior training.&lt;/p&gt;

&lt;p&gt;In another workshop of ICCV 2023, &lt;a href=&quot;https://bigmac-vision.github.io/&quot;&gt;BigMAC&lt;/a&gt;: Big Model Adaptation for Computer Vision, the &lt;a href=&quot;https://bigmac-vision.github.io/pdfs/ludwig.pdf&quot;&gt;robustness of the CLIP model&lt;/a&gt; on the popular ImageNet benchmark was discussed. In conclusion, thanks to training on a large, versatile dataset means that zero-shot predictions of the CLIP model are less vulnerable to data drift than popular CNN models trained and fine tuned on imageNet. In this &lt;a href=&quot;https://www.youtube.com/watch?v=XiouM3MEOKs&amp;amp;t=4546s&quot;&gt;recording of Ludwig’s presentation&lt;/a&gt; different ways to preserve CLIP robustness while fine-tuned are discussed.&lt;/p&gt;

&lt;p&gt;On a side note, the ICCV conference was quite an event. With five days of workshops, talks and demos! Big tech companies such as Meta marked their presence with impressive hubs, answering attendees’ questions. Numerous poster sessions gave us a chance to interact with authors and select some ideas we would like to contribute to the tech stack at Adevinta.&lt;/p&gt;

&lt;p&gt;In the subsequent sections, we will dig into real-world instances, underscoring their impact on e-commerce and elaborate how investing in this technology can galvanise collaboration and innovation across various teams within a company.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*ZtACVLp3aedfQYAy&quot; alt=&quot;An overview of popular foundation models and their applications in various AI domains.&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;h2 id=&quot;real-world-adoption-of-foundation-models&quot;&gt;Real-World Adoption of foundation models&lt;/h2&gt;

&lt;p&gt;Major tech companies have paved the way in producing and distributing ready-to-use foundation models, which are now being utilised by various businesses to &lt;a href=&quot;https://www.forbes.com/sites/moorinsights/2023/07/21/the-extraordinary-ubiquity-of-generative-ai-and-how-major-companies-are-using-it/&quot;&gt;enhance or create new products&lt;/a&gt; for tech-savvy consumers&lt;a href=&quot;https://www.forbes.com/sites/moorinsights/2023/07/21/the-extraordinary-ubiquity-of-generative-ai-and-how-major-companies-are-using-it/&quot;&gt;​&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;e-commerce-andretail&quot;&gt;E-commerce and retail&lt;/h2&gt;

&lt;p&gt;In the sphere of e-commerce, companies like Pinterest and eBay, have &lt;a href=&quot;https://developer.nvidia.com/blog/pinterest-uses-ai-to-enhance-its-recommendations-system/#:~:text=Developers%20from%20Pinterest%2C%20along%20with,objects%20saved%20has%20crossed&quot;&gt;invested in deep learning&lt;/a&gt; and machine learning technologies to enhance user experiences. Pinterest has developed PinSage for advertising and shopping recommendations and a multi-task deep metric &lt;a href=&quot;https://blog.acolyer.org/2019/10/11/learning-a-unified-embedding-for-visual-search-at-pinterest/#:~:text=The%20foundation%20of%20Pinterest%E2%80%99s%20approach,task%20learning&quot;&gt;learning system for unified image embedding&lt;/a&gt; to aid in &lt;a href=&quot;https://arxiv.org/abs/1908.01707&quot;&gt;visual search&lt;/a&gt; and recommendation systems​&lt;a href=&quot;https://blog.acolyer.org/2019/10/11/learning-a-unified-embedding-for-visual-search-at-pinterest/#:~:text=The%20foundation%20of%20Pinterest%E2%80%99s%20approach,task%20learning&quot;&gt;​&lt;/a&gt;. eBay, on the other hand, utilises a convolutional neural network for its &lt;a href=&quot;https://www.ebayinc.com/stories/news/an-easier-way-to-search-ebay-computer-vision-with-find-it-on-ebay-and-image-search-is-now-live/#:~:text=When%20you%20upload%20images%20to,the%20live%20listings%20on%20eBay&quot;&gt;image search feature&lt;/a&gt;, “Find It On eBay.”​&lt;/p&gt;

&lt;p&gt;Computer vision applications are transforming e-commerce, aiding in creating seamless omnichannel shopping experiences​. When it comes to the importance of visuals in shopping experiences, a study by PowerReviews found that &lt;strong&gt;88% of consumers specifically&lt;/strong&gt; &lt;a href=&quot;https://www.ebayinc.com/stories/news/an-easier-way-to-search-ebay-computer-vision-with-find-it-on-ebay-and-image-search-is-now-live/#:~:text=When%20you%20upload%20images%20to,the%20live%20listings%20on%20eBay&quot;&gt;&lt;strong&gt;look for visuals&lt;/strong&gt;&lt;/a&gt; submitted by other consumers prior to making a purchase​.&lt;/p&gt;

&lt;h3 id=&quot;broader-techindustry&quot;&gt;Broader tech industry&lt;/h3&gt;

&lt;p&gt;In the broader tech industry, Microsoft has introduced &lt;a href=&quot;https://www.ebayinc.com/stories/news/an-easier-way-to-search-ebay-computer-vision-with-find-it-on-ebay-and-image-search-is-now-live/#:~:text=When%20you%20upload%20images%20to,the%20live%20listings%20on%20eBay&quot;&gt;Florence&lt;/a&gt;, a novel foundation model for computer vision. The underlying technology of foundation models is designed to provide a solid base that can be fine-tuned for various specific tasks, an advantage that has been recognised and harnessed by industry giants.&lt;/p&gt;

&lt;p&gt;Take Copenhagen-based startup Modl.ai for instance, which relies on foundation models, self-supervised training and computer vision for &lt;a href=&quot;https://the-decoder.com/ai-startup-wants-to-bring-foundation-models-to-game-development/#:~:text=Copenhagen,with%20and%20against%20human%20players&quot;&gt;developing AI bots&lt;/a&gt; to test video games for bugs and performance. Such applications demonstrate the versatility and potential of foundation models in different sectors​.&lt;/p&gt;

&lt;p&gt;The practical implementations of foundation models in these different sectors underscores their potential to drive innovation, enhance user experiences and foster cross-functional collaboration within and beyond the e-commerce spectrum. The flexibility and adaptability of foundation models, as demonstrated by these real-world examples, make them a valuable asset for companies striving to stay ahead in the competitive e-commerce landscape.&lt;/p&gt;

&lt;h2 id=&quot;investing-in-foundation-models-cost-benefit-analysis&quot;&gt;Investing in foundation models: Cost-benefit analysis&lt;/h2&gt;

&lt;p&gt;The investment in foundation models for computer vision transcends the mere financial outlay. It encapsulates a strategic foresight to harness advanced AI technologies for bolstering e-commerce operations.&lt;/p&gt;

&lt;p&gt;Investing in foundation models for computer vision in e-commerce does entail upfront costs such as acquiring computational resources and the requisite expertise. OpenAI’s GPT-3 model, for example, reportedly cost $4.6M to train. According to another OpenAI report, the cost of training a large AI model is &lt;a href=&quot;https://encord.com/blog/visual-foundation-models-vfms-explained/#:~:text=OpenAI%E2%80%99s%20GPT%2D3%20model%2C%20for%20example%2C%20reportedly%20cost%20%244.6MM%20to%20train.%20According%20to%20another%20OpenAI%20report%2C%20the%20cost%20of%20training%20a%20large%20AI%20model%20is%20projected%20to%20rise%20from%20%24100MM%20to%20%24500MM%20by%202030.&quot;&gt;projected to rise&lt;/a&gt; from $100M to $500M by 2030.&lt;/p&gt;

&lt;p&gt;However, the potential benefits could justify the investment. For instance, the &lt;strong&gt;global visual search market,&lt;/strong&gt; which is significantly powered by computer vision technology, is projected to reach &lt;strong&gt;$15 billion by 2023&lt;/strong&gt;. Early adopters who incorporate visual search on their platforms could see &lt;a href=&quot;https://blog.taskmonk.ai/what-role-will-computer-vision-play-in-the-future-of-ecommerce/#:~:text=Early%20adopters%20who%20incorporate%20visual,of%20their%20online%20shopping%20experience&quot;&gt;&lt;strong&gt;revenues increase by 30%&lt;/strong&gt;&lt;/a&gt;. The computer vision market itself is soaring with an expected &lt;strong&gt;annual growth rate of 19.5%&lt;/strong&gt;, predicted to reach a value of $100.4 billion by 2023​&lt;a href=&quot;https://encord.com/blog/visual-foundation-models-vfms-explained/#:~:text=April%2024%2C%202023%20%E2%80%A2%205,9Bn%20in%202022&quot;&gt;​&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;These figures suggest that the integration of computer vision, particularly through foundation models, can be a lucrative venture in the long-term. Consumers are increasingly leaning towards platforms that offer visual search and other AI-driven features. Therefore, the cost of investment could be offset by the subsequent increase in revenue, enhanced user engagement and improved operational efficiency brought about by the advanced capabilities of foundation models in computer vision.​&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;&lt;em&gt;“Foundation models cut down on data labelling requirements anywhere from a factor of like 10 times, 200 times, depending on the use case”&lt;/em&gt;&lt;/strong&gt;— &lt;a href=&quot;https://venturebeat.com/ai/foundation-models-2022s-ai-paradigm-shift/#:~:text=%E2%80%9CFoundation%20models%20cut%20down%20on%20data%20labeling%20requirements%20anywhere%20from%20a%20factor%20of%20like%2010%20times%2C%20200%20times%2C%20depending%20on%20the%20use%20case%2C%E2%80%9D%20Dakshi%20Agrawal%2C%20IBM%20fellow%20and%20CTO%20of%20IBM%20AI%2C&quot;&gt;Dakshi Agrawal&lt;/a&gt;, IBM fellow and CTO of IBM AI&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Moreover, the global computer vision market, which encompasses technologies enabling such visual experiences, is expected to &lt;a href=&quot;https://www.syte.ai/blog/visual-ai/how-visual-ai-is-changing-omnichannel-retail/&quot;&gt;grow substantially&lt;/a&gt;, indicating the increasing importance of investment in visual technologies for retail and e-commerce​. The role of visual AI, which includes &lt;a href=&quot;https://research.aimultiple.com/computer-vision-retail/#:~:text=The%20global%20computer%20vision%20market,improve%20efficiency%20in%20omnichannel&quot;&gt;computer vision&lt;/a&gt;, is also highlighted in how it’s changing omnichannel retail, showcasing the intertwined relationship between visual technology and &lt;a href=&quot;https://losspreventionmedia.com/computer-vision-future-of-retail/&quot;&gt;enhanced shopping experiences&lt;/a&gt; across channels​.&lt;/p&gt;

&lt;h2 id=&quot;examples-of-application-of-foundation-models-in-e-commerce&quot;&gt;Examples of application of foundation models in e-commerce&lt;/h2&gt;

&lt;p&gt;Because of their pre-training on expansive datasets, foundation models in computer vision bring a treasure trove of capabilities to the table. &lt;strong&gt;The pre-trained nature of foundation models significantly accelerates the deployment of computer vision applications in e-commerce, as they require less data and resources for fine-tuning compared to training models from scratch.&lt;/strong&gt; Let’s illustrate this through real-world examples within the e-commerce sector.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Product Categorisation&lt;/strong&gt;: Leveraging a foundation model for automated product categorisation can be a time and resource-saver.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Visual Search&lt;/strong&gt;: Implementing visual search features can be expedited with foundation models. Their pre-trained knowledge can be leveraged to recognise fashion or product trends, making visual search more intuitive.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Counterfeit Detection&lt;/strong&gt;: Counterfeit detection is a complex task; however, with a foundation model, the pre-existing knowledge about different objects can be fine-tuned to identify subtle discrepancies between genuine and counterfeit products&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Moderation&lt;/strong&gt;: Detection of unwanted or harmful content can be done through a classification head added on top of image embeddings generated by a foundation model.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*xDEvavmYRc4MF2xE&quot; alt=&quot;A diagram showcasing the various applications of foundation models in e-commerce, from product categorization to counterfeit detection.&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Beyond these examples, foundation models also hold promise in enhancing user experiences in recommendation systems and augmented reality (AR) shopping.&lt;/p&gt;

&lt;p&gt;Most of this use-case could be applied to Adevinta marketplaces or replace existing services based on more traditional models.&lt;/p&gt;

&lt;h2 id=&quot;empowering-teams-across-the-e-commerce-spectrum&quot;&gt;Empowering teams across the e-commerce spectrum&lt;/h2&gt;

&lt;p&gt;Foundation models in computer vision open up avenues for fostering cross-functional collaboration, expediting product development, and making data-driven decision-making a norm across an e-commerce enterprise. Let’s delve into how these models can act as catalysts in harmonising the efforts of various teams and speeding up the journey from conception to market-ready solutions.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*3hnJ3nPcTBr68D70&quot; alt=&quot;Members of the Cognition team at Adevinta discussing the latest trends in AI and computer vision.&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;h2 id=&quot;accelerating-the-product-development-cycle&quot;&gt;Accelerating the product development cycle&lt;/h2&gt;

&lt;p&gt;The pre-trained nature of foundation models significantly &lt;strong&gt;cuts down the time traditionally required to develop, train and deploy machine learning models&lt;/strong&gt;. This acceleration in the product development cycle is invaluable in the fiercely competitive e-commerce market, where being the first to introduce innovative features can provide a substantial competitive edge. Moreover, the resource efficiency of foundation models ensures that &lt;strong&gt;teams can iterate and improve upon models swiftly&lt;/strong&gt;, aligning with dynamic market trends and customer expectations.&lt;/p&gt;

&lt;h2 id=&quot;stepping-stone-to-broader-business-objectives&quot;&gt;Stepping stone to broader business objectives&lt;/h2&gt;

&lt;p&gt;Foundation models can act as a springboard towards achieving broader business goals such as sustainability and promoting the second-hand goods trade. By enabling smarter product listings and verifications through image recognition and visual search capabilities, these models can streamline the process of listing and verifying second-hand goods. This, in turn, &lt;strong&gt;promotes a circular economy, encouraging the reuse and recycling of products&lt;/strong&gt;, which aligns with the sustainability goals of many modern e-commerce platforms.&lt;/p&gt;

&lt;h2 id=&quot;challenges-and-overcoming-strategies&quot;&gt;Challenges and overcoming strategies&lt;/h2&gt;

&lt;p&gt;Incorporating foundation models for computer vision within an e-commerce setting comes with a range of challenges, but with the right strategies, these hurdles can be navigated to unlock the models’ full potential.&lt;/p&gt;

&lt;h2 id=&quot;computational-requirements&quot;&gt;Computational requirements&lt;/h2&gt;

&lt;p&gt;Foundation models are computationally intensive due to their large-scale nature, which necessitates &lt;a href=&quot;https://snorkel.ai/foundation-models/#:~:text=Cost,for%20their%20end%20use%20caes&quot;&gt;significant computational resources&lt;/a&gt; for training and fine-tuning. The good news is that, once the substantial work of domain-learning or fine tuning is done, numerous teams and projects can benefit from the foundation model with minimal additional effort and cost.&lt;/p&gt;

&lt;h2 id=&quot;bias-andfairness&quot;&gt;Bias and fairness&lt;/h2&gt;

&lt;p&gt;Foundation models may inherit biases present in the training data, which can lead to unfair or discriminatory behaviour. For instance, &lt;a href=&quot;https://datagen.tech/blog/the-opportunities-and-risks-of-foundation-models/&quot;&gt;DALL-E and CLIP have shown biases&lt;/a&gt; regarding gender and race when generating images or interpreting text and images​. Implementing robust data preprocessing and bias mitigation strategies will help to address potential biases in training data.&lt;/p&gt;

&lt;h2 id=&quot;interpretability-andcontrol&quot;&gt;Interpretability and control&lt;/h2&gt;

&lt;p&gt;Understanding and controlling the behaviour of foundation models like CLIP remains a challenge due to their black-box nature. This makes it difficult to interpret the models’ predictions, which is a hurdle in applications where &lt;a href=&quot;https://arxiv.org/abs/2103.00020&quot;&gt;explainability is crucial&lt;/a&gt;​​. CRFM released recently a &lt;a href=&quot;https://crfm.stanford.edu/fmti/?utm_campaign=The%20Batch&amp;amp;utm_medium=email&amp;amp;_hsmi=280825441&amp;amp;utm_content=280827829&amp;amp;utm_source=hs_email&quot;&gt;Foundation Model Transparency Index&lt;/a&gt; “scoring 10 popular models on how well their makers disclosed details of their training, characteristics and use.”&lt;/p&gt;

&lt;p&gt;Foundation models, if widely adopted, could introduce &lt;strong&gt;single points of failure&lt;/strong&gt; in machine learning systems. If adversaries find vulnerabilities in a foundation model, they could &lt;a href=&quot;https://arxiv.org/abs/2103.11251&quot;&gt;exploit these weaknesses&lt;/a&gt; across multiple systems utilising the same model​.&lt;/p&gt;

&lt;p&gt;Foundation models are &lt;strong&gt;not the answer to all&lt;/strong&gt; machine learning problems.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;&lt;em&gt;“Foundation models are neither ‘foundational’ nor the foundations of AI. We deliberately chose ‘foundation’ rather than ‘foundational,’ because we found that ‘foundational’ implied that these models provide fundamental principles in a way that ‘foundation’ does not. (…) Further, ‘foundation’ describes the (role of) model and not AI; we neither claim nor believe that foundation models alone are the foundation of AI, but instead note they are ‘only one component (though an increasingly important component) of an AI system.’”&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;— the Stanford Institute for Human-Centred AI founded the Center for &lt;a href=&quot;https://arxiv.org/pdf/2108.07258.pdf&quot;&gt;Research on Foundation Models&lt;/a&gt; (CRFM), 2021&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;The transformative potential of foundation models in computer vision is unmistakable and pivotal for advancing the e-commerce domain. They encapsulate a significant stride towards creating smarter, more intuitive and user-centric online shopping experiences. The notable successes of early adopters, alongside the burgeoning global visual search market, exhibit the financial promise inherent in embracing these models​.&lt;/p&gt;

&lt;p&gt;The real-world implications extend beyond just improved product discovery and categorisation, to fostering a sustainable trading ecosystem for second-hand goods. &lt;strong&gt;The expertise and investment in these models can expedite the product development cycle, encourage data-driven decision-making and stimulate cross-functional collaboration across various company departments.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;However, it’s crucial to acknowledge the technical and ethical challenges that come with the deployment of foundation models. The computational costs, potential biases and the necessity for robust infrastructures demand a well-thought-out strategic approach. Yet, with the right investment in computational infrastructure, continuous learning and a commitment to ethical AI practices, these hurdles can be navigated successfully.&lt;/p&gt;

&lt;p&gt;While the promise of foundation models in computer vision is evident, discerning which model will perform optimally with your specific data remains a complex challenge.&lt;/p&gt;

&lt;p&gt;This uncertainty underscores the vital &lt;strong&gt;need for comprehensive benchmarks&lt;/strong&gt; that can guide businesses in selecting the most appropriate model. Investing time in testing and evaluation is crucial, as it enables a more informed decision-making process. A recent study highlighted in the article “&lt;a href=&quot;https://arxiv.org/pdf/2310.19909.pdf&quot;&gt;A Comprehensive Study on Backbone Architectures for Regular and Vision Transformers&lt;/a&gt;” delves into this subject by testing different model backbones across a range of downstream tasks and datasets. Such research is invaluable for businesses looking to capitalise on foundation models, as it provides critical insights into model performance and applicability, ensuring that their investment in AI is both strategic and effective.&lt;/p&gt;

&lt;p&gt;In Adevinta, as an e-commerce leader, we are evaluating the pros and cons of foundation models to best leverage their potential within our company. In Cognition, we are also working on internal benchmarks that will help to chose right foundation model for the task, estimate ressources needed and showcase its potential performance on the marketplace data.&lt;/p&gt;

&lt;p&gt;With industry behemoths and experts leading the era of foundation models, the call to action for e-commerce directors is clear: &lt;strong&gt;Embrace the paradigm shift that foundation models represent, and consider them as a long-term strategic asset for maintaining a competitive edge in the rapidly evolving e-commerce landscape.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Check out this mine of knowledge about foundation models: &lt;a href=&quot;https://cs.uwaterloo.ca/~wenhuche/teaching/cs886/&quot;&gt;https://cs.uwaterloo.ca/~wenhuche/teaching/cs886/&lt;/a&gt;&lt;/p&gt;

&lt;footer&gt;
  &lt;p&gt;Exported from &lt;a href=&quot;https://medium.com&quot;&gt;Medium&lt;/a&gt; on December 19
    2023.&lt;/p&gt;
  &lt;p&gt;&lt;a href=&quot;https://medium.com/adevinta-tech-blog/foundation-models-a-new-vision-for-e-commerce-76904a3066e8&quot;&gt;View
      the original&lt;/a&gt;&lt;/p&gt;
&lt;/footer&gt;
&lt;script type=&quot;text/javascript&quot; src=&quot;//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-584ec4ce89deed84&quot;&gt;&lt;/script&gt;

&lt;div class=&quot;addthis_inline_share_toolbox&quot;&gt;&lt;/div&gt;

</description>
        <pubDate>Tue, 19 Dec 2023 00:00:00 +0100</pubDate>
        <link>http://urszulaczerwinska.github.io/thoughts/foundation-models-a-new-vision-for-e-commerce</link>
        <guid isPermaLink="true">http://urszulaczerwinska.github.io/thoughts/foundation-models-a-new-vision-for-e-commerce</guid>
        
        
        <category>thoughts</category>
        
        <category>featured</category>
        
      </item>
    
      <item>
        <title>Deep Dive in PaddleOCR inference</title>
        <description>&lt;head&gt;
  &lt;meta name=&quot;description&quot; content=&quot;A deep dive into the complexities of using PaddleOCR for text extraction from images and how the Cognition team improved the service. Learn about the challenges and solutions that enhanced user experience in OCR services.&quot; /&gt;
&lt;/head&gt;

&lt;p&gt;This article is a deep dive into part of our work as described in &lt;a href=&quot;/works/text-in-image-2-0-improving-ocr-service-with-paddleocr&quot;&gt;&lt;strong&gt;Article 1: Text in Image 2.0: improving OCR service with PaddleOCR&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We are Cognition, an &lt;a href=&quot;https://www.adevinta.com/&quot;&gt;Adevinta&lt;/a&gt; Computer Vision Machine Learning (ML) team working on solutions for our marketplaces. Adevinta is a global classifieds specialist with market-leading positions in key European markets that aims to find perfect matches between its users and the platforms’ goods. As a Global Team, our team, Cognition, provides image processing APIs to all of our marketplaces.&lt;/p&gt;

&lt;p&gt;In the process of improving our OCR API for text extraction from images, we updated our existing Text in Image service to the &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR&quot;&gt;PaddleOCR&lt;/a&gt; framework, which was the winner of our benchmarks. In order to test if this framework was the most suitable solution, we carried out a deeper analysis of their code base. This article shares the challenges we encountered and how we overcame them.&lt;/p&gt;

&lt;p&gt;We believe our code version is easier to work with, given the use case of text extraction from images. The different steps and pre-processing and post-processing parts are clearly separated so they can be called independently, which should make further community extensions easier to add. It also makes putting into production easier, as the simplified, modular code combines well with the structure of &lt;em&gt;inference.py&lt;/em&gt; for serving SageMaker endpoints. Our proposed code version does not alter predictions (compared to the 2.6 release) for images.&lt;/p&gt;

&lt;h2 id=&quot;understanding-the-paddleocr-framework&quot;&gt;Understanding the PaddleOCR framework&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/PaddlePaddle/Paddle&quot;&gt;PaddlePaddle&lt;/a&gt; (short for Parallel Distributed Deep Learning) is an open source deep learning platform developed by Baidu Research. It is written in C++ and Python, and is designed to be easy to use and efficient for large-scale machine learning tasks.&lt;/p&gt;

&lt;p&gt;PaddlePaddle provides a range of tools and libraries for building and training deep learning models, including support for convolutional neural networks (CNNs), recurrent neural networks (RNNs) and long short-term memory (LSTM) networks.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR&quot;&gt;PaddleOCR&lt;/a&gt; builds on PaddlePaddle, an unfamiliar framework that our team had not used before. To make things even more challenging, PaddleOCR is not just one algorithm, it includes a range of pre-trained models and tools for recognising text in images and documents, as well as for training custom OCR models.&lt;/p&gt;

&lt;p&gt;PaddleOCR is divided into two main sections:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;PP-OCR&lt;/strong&gt;, an OCR system used for text extraction from images&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;PP-Structure&lt;/strong&gt;, a document analysis system which aims to perform layout analysis and table recognition&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;PP-OCR exists in three different versions (V1, V2 and V3). In these different releases, major improvements were brought to the models’ architecture.&lt;/p&gt;

&lt;p&gt;For our Text in Image service update, we focused on the most recent and most performant PP-OCRv3 release.&lt;/p&gt;

&lt;h3 id=&quot;the-paddleocrv3-models-architecture&quot;&gt;The PaddleOCRv3 models architecture&lt;/h3&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*1mI3YTIjAut_QMrl&quot; alt=&quot;PaddleOCRv3 Architecture&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;PP-OCRv3 is composed of three parts: detection, classification and recognition, all of which can be used independently. Each part has its own model trained with the PaddlePaddle framework. For those interested, model details can be found in this dedicated research article PP-OCRv3: &lt;a href=&quot;https://arxiv.org/abs/2206.03001v2&quot;&gt;More Attempts for the Improvement of Ultra Lightweight OCR System (Yanjun et al., 2022)&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;PP-OCRv3 text detection is made with the Differentiable Binarization algorithm (&lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.6/doc/doc_en/algorithm_det_db_en.md&quot;&gt;DB&lt;/a&gt;) trained using distillation strategy. The PP-OCRv3 recogniser is optimised based on the text recognition algorithm, Scene Text Recognition with a Single Visual Model (&lt;a href=&quot;https://arxiv.org/abs/2205.00159&quot;&gt;SVTR, Du et al. 2022)&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;PP-OCRv3 adopts the text recognition network SVTR_LCNet, and uses &lt;a href=&quot;https://arxiv.org/abs/2002.01276&quot;&gt;the guided training of Connectionist Temporal Classification (CTC&lt;/a&gt;, Z&lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Lin%2C+Z&quot;&gt;hiping&lt;/a&gt; et al., 2020) by the attention, data augmentation strategy, TextConAug, Unified Deep Mutual Learning and Unlabelled Images Mining (first introduced in &lt;a href=&quot;https://arxiv.org/abs/2109.03144&quot;&gt;PaddleOCRv2, Yanjun et al. 2021&lt;/a&gt;). The Text classifier is a simple binary classifier with classes 0 and 180°.&lt;/p&gt;

&lt;h3 id=&quot;paddleocr-inference-in-practice&quot;&gt;PaddleOCR inference in practice&lt;/h3&gt;

&lt;p&gt;While testing on our benchmarks, we used the PaddleOCR code for inference with default parameters and “latin” as a language (see their &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.6/doc/doc_en/quickstart_en.md&quot;&gt;QuickStart page&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Reading the documentation and looking into the class parameters, we saw lots of model combinations to test and therefore more opportunities to potentially improve our score.&lt;/p&gt;

&lt;p&gt;For instance, the &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.6/doc/doc_en/detection_en.md&quot;&gt;documentation&lt;/a&gt; suggests there is a choice between “DB” and “EAST” algorithms for detection, but it’s only the main inference &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.6/paddleocr.py&quot;&gt;script&lt;/a&gt; where the algorithm has to be “DB” — the &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/blob/23e034c40ecd5755af48d7b14dcc1bf6c5cf1128/tools/infer/predict_det.py#L62&quot;&gt;script&lt;/a&gt; of detection inference goes through a long list of algorithms. A similar situation occurs with text recognition where the pre-trained algorithm for Latin is “SVTR_LCNet”, but in &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/blob/23e034c40ecd5755af48d7b14dcc1bf6c5cf1128/paddleocr.py#L51&quot;&gt;theory&lt;/a&gt;, the accepted values are “‘CRNN’ and ‘SVTR_LCNet’ with the general &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/blob/18ddb6d5f9bdc2c1b0aa7f6e399ec0f76119dc87/doc/doc_en/inference_args_en.md&quot;&gt;documentation&lt;/a&gt; mentioning a plethora of models.&lt;/p&gt;

&lt;p&gt;Pre-trained English models are available in “‘CRNN’ and ‘SVTR_LCNet’ architectures. However, to find the information, the user would need to look into the pretrained model &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/blob/18ddb6d5f9bdc2c1b0aa7f6e399ec0f76119dc87/configs/rec/PP-OCRv3/en_PP-OCRv3_rec.yml#L39&quot;&gt;config&lt;/a&gt;. If the user does not specify the “rec_algorithm”, the default value, “SVTR_LCNet”, would be used, even if it isn’t correct. This doesn’t actually make any difference to the inference &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.6/tools/infer/predict_rec.py&quot;&gt;code&lt;/a&gt; as none of the “if” applies to ‘CRNN’ or ‘SVTR_LCNet’.&lt;/p&gt;

&lt;p&gt;In order to test a different architecture, we would need to train it ourselves and chain dedicated scripts.&lt;/p&gt;

&lt;h2 id=&quot;clarifying-paddleocr-inference&quot;&gt;Clarifying PaddleOCR inference&lt;/h2&gt;

&lt;p&gt;From digging into the code, we discovered several complexities, unnecessary for our use case. Firstly, the code seemed to grow organically, where the inference version is a limited choice entry to the multi-option code. This leaves us with numerous “factory patterns” and “if .. elses”, where the user has no choice at all. The English documentation was confusing and referenced different usage cases. We struggled to follow the logic as it neither explained parameters, nor clearly defined the limitations of the inference code.&lt;/p&gt;

&lt;p&gt;Despite these complexities, we managed to clarify the general way of working, calling the PaddleOCR.ocr() method from the ‘master’ file, &lt;em&gt;paddleocr.py&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*zwImfJ-4pOxDvrEI&quot; alt=&quot;PaddleOCR.ocr() Method&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;The input image and parameters are entered into the PaddleOCR.ocr() method which calls TextSystem class in order: TextDetector, TextClassifier and TextRecogniser, with a selection of helper functions, including one that formats the outputs of TextDetector into a list of cropped images being input to TextClassifier and TextRecogniser.&lt;/p&gt;

&lt;p&gt;The PaddleOCR.ocr() method is parsing params, including the language, version, type of OCR (or structure), downloads inference models and imports actual image (with check_image).&lt;/p&gt;

&lt;p&gt;If we want our image to go through a full OCR process, the TextSystem class will sequentially call classes responsible for detection, classification and recognition.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*B-7pY0A4Xv7eNTcr&quot; alt=&quot;TextSystem Class Flow&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Each of the main classes has an &lt;em&gt;__init__&lt;/em&gt; method that initialises pre- &amp;amp; post- processing classes and loads the model (create_predictor), and &lt;em&gt;__call__&lt;/em&gt; method that executes (pre- &amp;amp;) post-processing on the image and performs the model inference for the input image(s).&lt;/p&gt;

&lt;p&gt;Most of the scripts used for inference can be found under ‘tools/infer/’. The pre-processing scripts are under “ppocr/data/imaug/operators.py”. The post-processing classes are under ‘ppocr/postprocess/’.&lt;/p&gt;

&lt;p&gt;This schema enables us to reduce the essential inference code to just a couple of files and better understand exactly how the code works. To make it easier to maintain, we decided to reformat the code, keeping only the essential parts for our use case.&lt;/p&gt;

&lt;h2 id=&quot;paddleocr-inference-code-caveats-andfixes&quot;&gt;PaddleOCR inference code caveats and fixes&lt;/h2&gt;

&lt;p&gt;Let’s walk you through the PaddleOCR features we didn’t like and suggestions on how they could be improved.&lt;/p&gt;

&lt;h3 id=&quot;spaghetti-code&quot;&gt;Spaghetti code&lt;/h3&gt;

&lt;p&gt;Overall, most of the code is in object oriented programming style where classes are not modular and most things happen in very long &lt;em&gt;__init__&lt;/em&gt; and &lt;em&gt;__call__&lt;/em&gt; methods. We have noticed (fig. 2 and fig. 3) that generally, three parts can be extracted: pre-processing, inference and post-processing. We have removed ‘create_operators’ and ‘build_post_process’ intermediate functions and called directly the class performing the task such as “DBPostprocess” and “NormalizeImage”. To make things more straightforward, we transformed them into simple functions, performing what their &lt;em&gt;__call__&lt;/em&gt; method was doing before. This leaves us with more modular code and direct logic that fits our needs.&lt;/p&gt;

&lt;h3 id=&quot;parameter-parsing&quot;&gt;&lt;em&gt;Parameter parsing&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;We found it problematic that the inference class requires 105 parameters, of which more than 70 were ignored.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*jPMJx-wOF-R5DsmqJFs5BA.png&quot; alt=&quot;PaddleOCR inference parameters are not all used&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/blob/18ddb6d5f9bdc2c1b0aa7f6e399ec0f76119dc87/doc/doc_en/inference_args_en.md&quot;&gt;English documentation&lt;/a&gt; lists the parameters and gives a succinct definition of them. In the code, they are defined in at least three different places: &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/blob/23e034c40ecd5755af48d7b14dcc1bf6c5cf1128/paddleocr.py#L307&quot;&gt;paddleocr.py&lt;/a&gt;, &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/blob/23e034c40ecd5755af48d7b14dcc1bf6c5cf1128/ppstructure/utility.py#L21&quot;&gt;utility.py&lt;/a&gt; and different &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/blob/34b9569800a38af41a27ed893b12567757ef6c89/tools/infer/utility.py#L34&quot;&gt;utility.py&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;However while executing the code, we found that only 20 parameters were useful in our refactored code:&lt;/p&gt;

&lt;p&gt;When rewriting the code, we cleaned the parameter list, leaving only the relevant parameters.&lt;/p&gt;

&lt;h3 id=&quot;parameter-impact-on-prediction&quot;&gt;&lt;em&gt;Parameter impact on prediction&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;Some of the parameter definitions and effect they would have when changed from default, were not clear to us. We built a &lt;a href=&quot;https://streamlit.io/&quot;&gt;Streamlit app&lt;/a&gt; to visualise the changes in params on the predictions. For instance, “unclip ratio” would impact the size of the box, and “threshold” would detect two bounding boxes instead of one. We advise you to play with your own data and model to see how different parameters affect the detection. Overall, we were not able to see a major improvement from changing defaults.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*B4uqn-7vcxfu5aPz&quot; alt=&quot;The illustration of PaddleOCR parameters impact on the machine learning model prediction&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;h3 id=&quot;language-choice&quot;&gt;&lt;em&gt;Language choice&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;Normally in our role, we work with “‘PP-OCRv3”, the most recent version of the framework. As we are dealing with European languages, we would choose “fr”, “en”, “es” as the “lang” param, thinking that this means different models are being called. However, while looking into the paddleocr.py, we saw how the languages are interpreted:&lt;/p&gt;

&lt;p&gt;The first definition serves to define the recognition model name/path. But if we typed “fr” or “es”, it becomes lang = “latin”, yet “en” remains “en”. Then another simplification happens for the detection model.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;if lang in [“en”, “latin”]:&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;det_lang = “en”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;We are left with an English detection model and a Latin recognition model for any European language written with Latin characters except English, which has its own recognition model.&lt;/p&gt;

&lt;h3 id=&quot;downloading-models&quot;&gt;&lt;em&gt;Downloading models&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;Based on the language parameter and framework version, the first time we call the PaddleOCR class with those parameters, the model will be downloaded from the url encoded in paddleocr.py.&lt;/p&gt;

&lt;p&gt;Firstly, this could cause some issues when running the code in secure or offline environments.&lt;/p&gt;

&lt;p&gt;Secondly, we found inconsistencies between the model urls in the paddleocr.py and the models provided in the dedicated &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/blob/18ddb6d5f9bdc2c1b0aa7f6e399ec0f76119dc87/doc/doc_en/models_list_en.md&quot;&gt;documentation page&lt;/a&gt;. For instance, “en_PP-OCRv3_det_slim” is not an option when models are downloaded by the paddleocr.py script. In order to use some of the models from Model Zoo, a database of pre-trained models and code, you would need to download the model and provide the path to it manually.&lt;/p&gt;

&lt;p&gt;In order to remove this ambiguity and use the specific model we needed, we decided to pre-download the chosen model, then provide the path directly. In the original code, it is possible to provide det_model_dir, cls_model_dir and rec_model_dir. The language param will then be ignored and any pre-trained model with the accepted backbones can be used. After this process, we removed the model download functionality from our code.&lt;/p&gt;

&lt;h3 id=&quot;using-onnxmodels&quot;&gt;&lt;em&gt;Using ONNX models&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;PaddleOCR provides a &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/blob/18ddb6d5f9bdc2c1b0aa7f6e399ec0f76119dc87/deploy/paddle2onnx/readme.md&quot;&gt;handy way&lt;/a&gt; to export models to the &lt;a href=&quot;https://onnx.ai/&quot;&gt;ONNX framework&lt;/a&gt; that can serve or integrate in different pipelines. We exported the pre-trained models using PaddleOCR instructions. In the PaddleOCR class, there is a parameter “use_onnx”. If one sets “use_onnx” and provides a direct path to the ONNX models to PaddleOCR(), the model would use the ONNX model for prediction. However, there is a small bug that occurs while running ONNX with GPUs, described further in this &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/issues/8688&quot;&gt;issue&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We applied the modification suggested and tested the code with ONNX models, obtaining satisfactory results on both CPU and GPU (even though we noticed small numerical differences between the Paddle and ONNX model versions).&lt;/p&gt;

&lt;h3 id=&quot;documentation&quot;&gt;&lt;em&gt;Documentation&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;If you look at the &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/tree/18ddb6d5f9bdc2c1b0aa7f6e399ec0f76119dc87/doc&quot;&gt;documentation pages&lt;/a&gt;, you will find a lot of resources in both English and Chinese. However, when looking at &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/issues&quot;&gt;Issues&lt;/a&gt;, you will find most of them are in Chinese, Japanese or Korean. The same applies to blog posts and community resources online. We also found that some documentation is only partially translated to English and the Chinese version contains much more detail.&lt;/p&gt;

&lt;p&gt;We did not find a solution for this. We made sure to always check both the English and Chinese documentation (translated to English by an automatic translator) to ensure that we have all the possible information.&lt;/p&gt;

&lt;h3 id=&quot;tests--pylint-typing&quot;&gt;&lt;em&gt;Tests &amp;amp;&lt;/em&gt; &lt;a href=&quot;https://pylint.pycqa.org/en/latest/&quot;&gt;&lt;em&gt;pylint&lt;/em&gt;&lt;/a&gt; &lt;em&gt;&amp;amp;&lt;/em&gt; &lt;a href=&quot;https://docs.python.org/3/library/typing.html&quot;&gt;&lt;em&gt;typing&lt;/em&gt;&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;In general, as the original code is not modular, it was not tested according to the standards of our team. Once we cleaned and simplified the code, we worked on linting and variable typing. Our next step will be to write meaningful unit tests to secure the code base.&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;PaddleOCR is a powerful and optimised library for the extraction of text from images. However, we found that the code doesn’t fit the standards of our team as it is too complex to maintain and understand. In this article, we pointed out some of the pain points for us that other PaddleOCR users may experience when working with this framework. The fixes we proposed made our lives easier and the code more transparent for any team member and the wider community, without compromising the speed or the original model accuracy.&lt;/p&gt;

&lt;footer&gt;
  &lt;p&gt;Exported from &lt;a href=&quot;https://medium.com&quot;&gt;Medium&lt;/a&gt; on June 06,
    2023.&lt;/p&gt;
  &lt;p&gt;&lt;a href=&quot;https://medium.com/adevinta-tech-blog/deep-dive-in-paddleocr-inference-e86f618a0937&quot;&gt;View
      the original. This article was orignally co-authored by Cognition team members, special credits to Joaquin Cabezas&lt;/a&gt;&lt;/p&gt;
&lt;/footer&gt;
&lt;script type=&quot;text/javascript&quot; src=&quot;//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-584ec4ce89deed84&quot;&gt;&lt;/script&gt;

&lt;div class=&quot;addthis_inline_share_toolbox&quot;&gt;&lt;/div&gt;

</description>
        <pubDate>Mon, 06 Mar 2023 00:00:00 +0100</pubDate>
        <link>http://urszulaczerwinska.github.io/works/deep-dive-in-paddleocr-inference</link>
        <guid isPermaLink="true">http://urszulaczerwinska.github.io/works/deep-dive-in-paddleocr-inference</guid>
        
        <category>data science</category>
        
        <category>OCR</category>
        
        <category>deep learning</category>
        
        <category>computer vision</category>
        
        <category>machine learning</category>
        
        <category>featured</category>
        
        
        <category>works</category>
        
      </item>
    
      <item>
        <title>Text in Image 2.0 - improving OCR service with PaddleOCR</title>
        <description>&lt;head&gt;
  &lt;meta name=&quot;description&quot; content=&quot;Discover how the Cognition team at Adevinta enhanced the Text in Image service using PaddleOCR, leading to significant improvements in OCR accuracy and performance.&quot; /&gt;
&lt;/head&gt;

&lt;h2 id=&quot;understanding-ocr-what-is-optical-character-recognition&quot;&gt;Understanding OCR: What is Optical Character Recognition?&lt;/h2&gt;

&lt;p&gt;Optical Character Recognition (OCR) is a popular topic for both industry and personal use. In this article, we share how we tested and used an existing open source library, PaddleOCR, to extract text from an image. This read is for anyone who would like to find out more about OCR, the needs of our customers at &lt;a href=&quot;https://www.adevinta.com/&quot;&gt;Adevinta&lt;/a&gt;, and the challenges we face in attending to them. You’ll find out how we upgraded an existing service, benchmarked different solutions and delivered the selected one to satisfy our customers.&lt;/p&gt;

&lt;h2 id=&quot;key-ocr-applications-how-ocr-transforms-business-and-daily-operations&quot;&gt;Key OCR applications: How OCR transforms business and daily operations&lt;/h2&gt;

&lt;p&gt;OCR stands for “Optical Character Recognition” and is a technology that allows computers to recognise and extract text from images and scanned documents. OCR software uses optical recognition algorithms to interpret the text in images and convert it into machine-readable text that can be edited, searched and stored electronically.&lt;/p&gt;

&lt;p&gt;There are numerous use-cases where OCR can be used:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Digitising paper documents&lt;/strong&gt;: to convert scanned images of text into digital text. This is useful for organisations that want to reduce their reliance on paper and improve their document management processes.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Extracting data from images&lt;/strong&gt;: eg from documents such as invoices, receipts and forms. This can be useful for automating data entry tasks and reducing the need for manual data entry.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Translating documents&lt;/strong&gt;: to extract text from images of documents written in foreign languages and translate them into a different language.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Archiving&lt;/strong&gt;: to create digital copies of important documents that need to be preserved for long periods of time.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Improving accessibility&lt;/strong&gt;: to make scanned documents more accessible to people with disabilities by converting the text into a format that can be read by assistive technologies such as screen readers.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Searching documents&lt;/strong&gt;: to make scanned documents searchable, allowing users to easily find specific information within a large collection of documents.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;the-adevinta-context-why-ocr-matters-in-global-marketplace&quot;&gt;The Adevinta context: Why OCR matters in global marketplace&lt;/h2&gt;

&lt;p&gt;Within &lt;a href=&quot;https://www.adevinta.com/&quot;&gt;Adevinta&lt;/a&gt;, a global classifieds specialist with market-leading positions in key European markets, there is space for all of the cited use cases. However, for this article, we focus specifically on “extracting data from images.”&lt;/p&gt;

&lt;p&gt;Applying deep learning to images is the main expertise of our team, Cognition. We are Data Scientists and Machine Learning (ML) Engineers that work together to develop image-based ML solutions at scale, helping Adevinta’s marketplaces build better products and experiences for their customers. Adevinta’s mission is to connect buyers and sellers, enabling people to find jobs, homes, cars, consumer goods and more. By making an accessible ML API with features tailored to our different marketplaces’ needs, Adevinta’s marketplaces are empowered with ML tools at a reasonable cost.&lt;/p&gt;

&lt;h2 id=&quot;text-extraction-in-images-why-its-crucial-for-adevintas-services&quot;&gt;Text Extraction in Images: Why It’s Crucial for Adevinta’s Services&lt;/h2&gt;
&lt;p&gt;Text extraction from images enables us to:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Detect unwanted content in ads (e.g., insults, hidden messages).&lt;/li&gt;
  &lt;li&gt;Better understand image content to improve search capabilities.&lt;/li&gt;
  &lt;li&gt;Support more efficient searches using visible text on items.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With over 100 million requests per month and growing, our existing Text in Image service was ripe for enhancement. We aimed to improve accuracy and performance, leading to the development of Text in Image 2.0.&lt;/p&gt;

&lt;h2 id=&quot;why-we-chose-paddleocr-benchmarking-the-best-ocr-solution&quot;&gt;Why we chose PaddleOCR: Benchmarking the best OCR solution&lt;/h2&gt;

&lt;p&gt;The existing service was based on &lt;a href=&quot;https://arxiv.org/abs/1801.01671&quot;&gt;Fast Oriented Text Spotting with a Unified Network (Yan et al., 2018)&lt;/a&gt;. Despite being state of the art in 2018, the algorithm achieved 0.4 accuracy on our internal benchmark of 200 marketplace images. Nevertheless, accuracy was not the sole criteria of choice for the Text in Image 2.0, so we compiled a list of edge cases where our partner marketplaces require high-performing algorithms.&lt;/p&gt;

&lt;p&gt;After reviewing different open source OCR frameworks (including &lt;a href=&quot;https://github.com/open-mmlab/mmocr&quot;&gt;MMOCR&lt;/a&gt;, &lt;a href=&quot;https://github.com/JaidedAI/EasyOCR&quot;&gt;EASY OCR&lt;/a&gt;, &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR&quot;&gt;PaddleOCR&lt;/a&gt; and &lt;a href=&quot;https://thehive.ai/apis/ocr&quot;&gt;HiveOCR&lt;/a&gt;) and different combinations of proposed models on our internal benchmark and on the edge cases, a indisputable winner was PaddleOCR with an average accuracy of 0.8 and an acceptable performance on our edge cases. This result competes with the paid &lt;a href=&quot;https://cloud.google.com/vision/docs/ocr&quot;&gt;Google Cloud Vision OCR API&lt;/a&gt; on the best accuracy we measured.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*UUEf-TKs1Lfn7_wx&quot; alt=&quot;Graph showing benchmark results for various OCR frameworks&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;h2 id=&quot;how-we-validated-paddleocr-building-a-comprehensive-benchmark&quot;&gt;How We Validated PaddleOCR: Building a Comprehensive Benchmark&lt;/h2&gt;

&lt;p&gt;In order to construct our independent benchmark and validate the choice of PaddleOCR at scale, we built a “Text in Image generator” that uses open source images from &lt;a href=&quot;https://unsplash.com/license&quot;&gt;Unsplash&lt;/a&gt; and &lt;a href=&quot;https://pikwizard.com/free-license&quot;&gt;Pikwizard&lt;/a&gt; and adds randomly generated text on top of them. The created tool is highly customisable in order to simulate a wide variety of cases that combine factors such as font type, rotation, text length, background type, image resolution etc. Using a simulated benchmark of 20k images with a distribution of cases matching business needs, we obtained an improvement factor of x1.4.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*sWpBlrJtdxsRlqj4&quot; alt=&quot;Sample of Text in Image generator output showing simulated text scenarios&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;h2 id=&quot;challenges-with-paddleocr-identifying-and-mitigating-issues&quot;&gt;Challenges with PaddleOCR: Identifying and mitigating issues&lt;/h2&gt;

&lt;p&gt;We identified several cases where PaddleOCR fails. This is mostly when there are different angles of rotated text, some alternative fonts and differing colour/contrast. We also observed that in some cases, the correct words are detected but the spaces between them are not placed correctly. This may or may not be an issue depending on the way the extracted text is used further.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image fit&quot;&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*3CO2dWUYPpVPPBZJDpx4EA.png&quot; alt=&quot;Example of OCR results with incorrectly spaced text&quot; /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;h2 id=&quot;deep-dive-how-we-optimized-paddleocr-for-production&quot;&gt;Deep Dive: How We Optimized PaddleOCR for Production&lt;/h2&gt;

&lt;p&gt;In order to evaluate the potential for improvement and mitigation of these errors, in addition to defining the serving strategy, we had to deep dive into the PaddleOCR framework.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR&quot;&gt;PaddleOCR&lt;/a&gt; builds on &lt;a href=&quot;https://github.com/PaddlePaddle/Paddle&quot;&gt;PaddlePaddle.&lt;/a&gt; Our team had no previous experience with this and it’s less popular in our community than other frameworks such as Tensorflow, Keras or Pytorch.&lt;/p&gt;

&lt;p&gt;From a technical point of view, PaddleOCR is composed of three distinct models:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Detection&lt;/strong&gt;, for detecting a bounding box where possible text is&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Classification&lt;/strong&gt;, rotating the text 180° if necessary&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Recognition&lt;/strong&gt;, translating the detected image frame to raw text&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Pre-trained models in different languages are &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/blob/18ddb6d5f9bdc2c1b0aa7f6e399ec0f76119dc87/doc/doc_en/models_list_en.md&quot;&gt;provided by authors&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;refactoring-paddleocr-creating-a-clean-production-ready-codebase&quot;&gt;Refactoring PaddleOCR: Creating a Clean, Production-Ready Codebase&lt;/h3&gt;

&lt;p&gt;Whilst exploring the code base of PaddleOCR for inference, we were faced with convoluted code, which was difficult to read and understand. As we wanted to use the PaddleOCR solution in production, we decided to refactor the code, keeping in mind to preserve the performance and the speed of the original code. You can read about the details of that process and the PaddleOCR model in the complementary article of this series. After refactoring the code, we had created a clean and readable code base.&lt;/p&gt;

&lt;p&gt;We believe our code version is easier to work with, given the use case of text extraction from images, and are working on making the code available open source. The different steps and pre-processing and post-processing parts are clearly separated, so they can be called independently, which should make further community extensions easier to add. It also makes putting into production easier, as the simplified, modular code combines well with the structure of inference.py for serving SageMaker endpoints. Our proposed code version does not alter predictions (compared to the 2.6 release) for images.&lt;/p&gt;

&lt;h2 id=&quot;deploying-text-in-image-20-achieving-superior-performance-with-paddleocr&quot;&gt;Deploying Text in Image 2.0: Achieving Superior Performance with PaddleOCR&lt;/h2&gt;

&lt;p&gt;Using the refactored code, we made the model available as an API. To help our customers’ transition, we maintained the same API contract used in the previous service.&lt;/p&gt;

&lt;p&gt;Serving PaddleOCR can be done in multiple ways. The straightforward approach is calling its own Python API (provided by the &lt;a href=&quot;https://pypi.org/project/paddleocr/&quot;&gt;PaddleOCR&lt;/a&gt; package) from within a well-known framework. We selected Multi Model Server, Flask and FastAPI to conduct our benchmark. All our proposed solutions are served by AWS SageMaker Endpoint, building our own container (BYOC) from the same Docker base image.&lt;/p&gt;

&lt;p&gt;MultiModel Server uses its own JAVA ModelServer, while for Flask and FastAPI, we use nginx+gunicorn (combined with &lt;a href=&quot;https://fastapi.tiangolo.com/deployment/server-workers/&quot;&gt;uvicorn workers for the ASGI FastAPI&lt;/a&gt;). The frontend for our customers is served by an API Gateway, which is out of the scope of this article.&lt;/p&gt;

&lt;h2 id=&quot;benchmarking-deployment-options-multi-model-server-flask-and-fastapi&quot;&gt;Benchmarking Deployment Options: Multi-Model Server, Flask, and FastAPI&lt;/h2&gt;

&lt;p&gt;For the performance testing, we recreated a number of requests with a controlled amount of text and different image sizes, mimicking the expected distribution from our customers. We used &lt;a href=&quot;https://locust.io/&quot;&gt;Locust&lt;/a&gt; as the testing framework, and stimulated heavy bursts in the &lt;a href=&quot;https://docs.locust.io/en/stable/writing-a-locustfile.html#wait-time-attribute&quot;&gt;waiting time&lt;/a&gt; as a stress test.&lt;/p&gt;

&lt;p&gt;With the data gathered from the performance tests, we were able to define our infrastructure (type of instance and autoscaling policy) in relation to the Service Level Agreement (SLA) terms, while balancing the risk of a sudden shift from the observed distribution (the service is sensitive to the amount of text per image).&lt;/p&gt;

&lt;p&gt;Currently, we deal with 330 million requests per month, and we have estimated that next year, more Adevinta marketplaces will onboard a Text in Image service, resulting in a 400% growth.&lt;/p&gt;

&lt;h2 id=&quot;results-and-impact-transforming-text-in-image-service-with-paddleocr&quot;&gt;Results and impact: Transforming Text in Image service with PaddleOCR&lt;/h2&gt;

&lt;p&gt;The new API resulted in an improved latency 7.5x compared to the FOTS-based solution, while providing a 7% cost reduction in serving. Also, since the new API being 12x cheaper than a typical external solution, such as GCP OCR, we received positive feedback from our users about both the speed and the accuracy of the Text in Image 2.0.&lt;/p&gt;

&lt;h2 id=&quot;key-takeaways-enhancing-ocr-with-paddleocr&quot;&gt;Key Takeaways: Enhancing OCR with PaddleOCR&lt;/h2&gt;

&lt;p&gt;As a computer vision team working for an international company serving millions of people every day, we aimed to improve our OCR API for text extraction from classified ads. After testing numerous frameworks, we built an image simulator in order to find the algorithm matching the needs of our users. The selected framework, PaddleOCR, went through our internal review and revamp. (There were challenges along the way and you can read more about them in &lt;a href=&quot;/works/deep-dive-in-paddleocr-inference&quot;&gt;&lt;strong&gt;Article 2: Deep Dive in PaddleOCR inference&lt;/strong&gt;&lt;/a&gt;). Now, we’re pleased to say we’re providing a more accurate, faster and cheaper API using the PaddleOCR framework.&lt;/p&gt;

&lt;footer&gt;
  &lt;p&gt;Exported from &lt;a href=&quot;https://medium.com&quot;&gt;Medium&lt;/a&gt; on June 06,
    2023.&lt;/p&gt;
  &lt;p&gt;&lt;a href=&quot;https://medium.com/adevinta-tech-blog/text-in-image-2-0-improving-ocr-service-with-paddleocr-61614c886f93&quot;&gt;This article was orignally co-authored by Cognition team members, special credits to Joaquin Cabezas&lt;/a&gt;&lt;/p&gt;
&lt;/footer&gt;
&lt;script type=&quot;text/javascript&quot; src=&quot;//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-584ec4ce89deed84&quot;&gt;&lt;/script&gt;

&lt;div class=&quot;addthis_inline_share_toolbox&quot;&gt;&lt;/div&gt;

</description>
        <pubDate>Mon, 06 Mar 2023 00:00:00 +0100</pubDate>
        <link>http://urszulaczerwinska.github.io/works/text-in-image-2-0-improving-ocr-service-with-paddleocr</link>
        <guid isPermaLink="true">http://urszulaczerwinska.github.io/works/text-in-image-2-0-improving-ocr-service-with-paddleocr</guid>
        
        <category>data science</category>
        
        <category>OCR</category>
        
        <category>deep learning</category>
        
        <category>computer vision</category>
        
        <category>machine learning</category>
        
        <category>API</category>
        
        
        <category>works</category>
        
      </item>
    
      <item>
        <title>Named Entity Recognition Tool by Cour de Cassation</title>
        <description>&lt;!DOCTYPE HTML&gt;
&lt;html lang=&quot;en-US&quot;&gt;
    &lt;head&gt;
        &lt;meta charset=&quot;UTF-8&quot;&gt;
        &lt;meta http-equiv=&quot;refresh&quot; content=&quot;0; url=https://github.com/Cour-de-cassation/moteurNER&quot;&gt;
        &lt;meta name=&quot;description&quot; content=&quot;Redirecting to the Named Entity Recognition (NER) tool repository by Cour de Cassation. It is a page about building deep learning NLP application for French justice&quot;&gt;
        &lt;script type=&quot;text/javascript&quot;&gt;
            window.location.href = &quot;https://github.com/Cour-de-cassation/moteurNER&quot;
        &lt;/script&gt;
        &lt;title&gt;Redirecting to Named Entity Recognition Tool by Cour de Cassation, building NER, NLP deep learning applications for French Supreme Court.&lt;/title&gt;
    &lt;/head&gt;
    &lt;body&gt;
        &lt;p&gt;You are being redirected to the Named Entity Recognition tool repository by Cour de Cassation. If you are not redirected automatically, &lt;a rel=&quot;canonical&quot; href=&apos;https://github.com/Cour-de-cassation/moteurNER&apos;&gt;click here to proceed to the repository.&lt;/a&gt;&lt;/p&gt;
    &lt;/body&gt;

      &lt;!-- Go to www.addthis.com/dashboard to customize your tools --&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-584ec4ce89deed84&quot;&gt;&lt;/script&gt;
&lt;!-- Go to www.addthis.com/dashboard to customize your tools --&gt; &lt;div class=&quot;addthis_inline_share_toolbox&quot;&gt;&lt;/div&gt;

&lt;/html&gt;</description>
        <pubDate>Wed, 01 Sep 2021 00:00:00 +0200</pubDate>
        <link>http://urszulaczerwinska.github.io/works/ner_cc</link>
        <guid isPermaLink="true">http://urszulaczerwinska.github.io/works/ner_cc</guid>
        
        <category>Machine Learning</category>
        
        <category>Python</category>
        
        <category>Deep Learning</category>
        
        <category>NER</category>
        
        <category>NLP</category>
        
        <category>Justice</category>
        
        <category>Flair</category>
        
        <category>featured</category>
        
        
        <category>works</category>
        
      </item>
    
  </channel>
</rss>
